{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"DataScienceTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting started"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "import RDatasets: dataset\n",
    "using PrettyPrinting\n",
    "import DataFrames: DataFrame, select, Not\n",
    "@load DecisionTreeClassifier pkg=DecisionTree\n",
    "\n",
    "carseats = dataset(\"ISLR\", \"Carseats\")\n",
    "\n",
    "first(carseats, 3) |> pretty"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We encode a new variable `High` based on whether the sales are higher or lower than 8 and add that column to the dataframe:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "High = ifelse.(carseats.Sales .<= 8, \"No\", \"Yes\") |> categorical;\n",
    "carseats[!, :High] = High;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now train a basic decision tree classifier for `High` given the other features after one-hot-encoding the categorical features:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X = select(carseats, Not([:Sales, :High]))\n",
    "y = carseats.High;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Tree Classifier"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "HotTreeClf = @pipeline(OneHotEncoder(),\n",
    "                       DecisionTreeClassifier())\n",
    "\n",
    "mdl = HotTreeClf\n",
    "mach = machine(mdl, X, y)\n",
    "fit!(mach);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that this is trained on the whole data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ypred = predict_mode(mach, X)\n",
    "misclassification_rate(ypred, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's right... it gets it perfectly; this tends to be classic behaviour for a DTC to overfit the data it's trained on.\n",
    "Let's see if it generalises:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.5, shuffle=true, rng=333)\n",
    "fit!(mach, rows=train)\n",
    "ypred = predict_mode(mach, rows=test)\n",
    "misclassification_rate(ypred, y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not really...\n",
    "\n",
    "### Tuning a DTC\n",
    "\n",
    "Let's try to do a bit of tuning"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r_mpi = range(mdl, :(decision_tree_classifier.max_depth), lower=1, upper=10)\n",
    "r_msl = range(mdl, :(decision_tree_classifier.min_samples_leaf), lower=1, upper=50)\n",
    "\n",
    "tm = TunedModel(model=mdl, ranges=[r_mpi, r_msl], tuning=Grid(resolution=8),\n",
    "                resampling=CV(nfolds=5, rng=112),\n",
    "                operation=predict_mode, measure=misclassification_rate)\n",
    "mtm = machine(tm, X, y)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "ypred = predict_mode(mtm, rows=test)\n",
    "misclassification_rate(ypred, y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can inspect the parameters of the best model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fitted_params(mtm).best_model.decision_tree_classifier"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Tree Regressor"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load DecisionTreeRegressor pkg=DecisionTree\n",
    "\n",
    "boston = dataset(\"MASS\", \"Boston\")\n",
    "\n",
    "y, X = unpack(boston, ==(:MedV), col -> true)\n",
    "\n",
    "train, test = partition(eachindex(y), 0.5, shuffle=true, rng=551);\n",
    "\n",
    "scitype(X)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's recode the Count as Continuous and then fit a DTR"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X = coerce(X, autotype(X, rules=(:discrete_to_continuous,)))\n",
    "\n",
    "dtr_model = DecisionTreeRegressor()\n",
    "dtr = machine(dtr_model, X, y)\n",
    "\n",
    "fit!(dtr, rows=train)\n",
    "\n",
    "ypred = predict(dtr, rows=test)\n",
    "round(rms(ypred, y[test]), sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again we can try tuning this a bit, since it's the same idea as before, let's just try to adjust the depth of the tree:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r_depth = range(dtr_model, :max_depth, lower=2, upper=20)\n",
    "\n",
    "tm = TunedModel(model=dtr_model, ranges=[r_depth], tuning=Grid(resolution=10),\n",
    "                resampling=CV(nfolds=5, rng=1254), measure=rms)\n",
    "mtm = machine(tm, X, y)\n",
    "\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "ypred = predict(mtm, rows=test)\n",
    "round(rms(ypred, y[test]), sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest\n",
    "\n",
    "**Note**: the package [`DecisionTree.jl`](https://github.com/bensadeghi/DecisionTree.jl) also has a RandomForest model but it is not yet interfaced with in MLJ."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load RandomForestRegressor pkg=ScikitLearn\n",
    "\n",
    "rf_mdl = RandomForestRegressor()\n",
    "rf = machine(rf_mdl, X, y)\n",
    "fit!(rf, rows=train)\n",
    "\n",
    "ypred = predict(rf, rows=test)\n",
    "round(rms(ypred, y[test]), sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Boosting Machine"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load XGBoostRegressor\n",
    "\n",
    "xgb_mdl = XGBoostRegressor(num_round=10, max_depth=10)\n",
    "xgb = machine(xgb_mdl, X, y)\n",
    "fit!(xgb, rows=train)\n",
    "\n",
    "ypred = predict(xgb, rows=test)\n",
    "round(rms(ypred, y[test]), sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again we could do some tuning for this."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "kernelspec": {
   "name": "julia-1.4",
   "display_name": "Julia 1.4.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
