{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"MLJTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Getting started"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ, RDatasets, PrettyPrinting\n",
    "MLJ.color_off() # hide\n",
    "import Distributions\n",
    "const D = Distributions\n",
    "\n",
    "@load LinearRegressor pkg=MLJLinearModels\n",
    "@load RidgeRegressor pkg=MLJLinearModels\n",
    "@load LassoRegressor pkg=MLJLinearModels\n",
    "\n",
    "hitters = dataset(\"ISLR\", \"Hitters\")\n",
    "@show size(hitters)\n",
    "names(hitters) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The target is `Salary`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y, X = unpack(hitters, ==(:Salary), col->true);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "It has missing values which we will just ignore:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "no_miss = .!ismissing.(y)\n",
    "y = collect(skipmissing(y))\n",
    "X = X[no_miss, :]\n",
    "train, test = partition(eachindex(y), 0.5, shuffle=true, rng=424);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using PyPlot\n",
    "\n",
    "figure(figsize=(8,6))\n",
    "plot(y, ls=\"none\", marker=\"o\")\n",
    "\n",
    "xticks(fontsize=12); yticks(fontsize=12)\n",
    "xlabel(\"Index\", fontsize=14), ylabel(\"Salary\", fontsize=14)\n",
    "\n",
    "savefig(joinpath(@OUTPUT, \"ISL-lab-6-g1.svg\")) # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "\\figalt{Salary}{ISL-lab-6-g1.svg}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "That looks quite skewed, let's have a look at a histogram:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "hist(y, bins=50, density=true)\n",
    "\n",
    "xticks(fontsize=12); yticks(fontsize=12)\n",
    "xlabel(\"Salary\", fontsize=14); ylabel(\"Density\", fontsize=14)\n",
    "\n",
    "edfit = D.fit_mle(D.Exponential, y)\n",
    "xx = range(minimum(y), 2500, length=100)\n",
    "yy = pdf.(edfit, xx)\n",
    "plot(xx, yy, lw=3, label=\"Exponential distribution fit\")\n",
    "\n",
    "legend(fontsize=12)\n",
    "\n",
    "savefig(joinpath(@OUTPUT, \"ISL-lab-6-g2.svg\")) # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "\\figalt{Distribution of salary}{ISL-lab-6-g2.svg}\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "Most features are currently encoded as integers but we will consider them as continuous"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xc = coerce(X, autotype(X, rules=(:discrete_to_continuous,)))\n",
    "scitype(Xc)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "There're a few features that are categorical which we'll one-hot-encode."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Ridge pipeline\n",
    "### Baseline\n",
    "\n",
    "Let's first fit a simple pipeline with a standardizer, a one-hot-encoder and a basic linear regression:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@pipeline RegPipe(std = Standardizer(),\n",
    "                  hot = OneHotEncoder(),\n",
    "                  reg = LinearRegressor())\n",
    "\n",
    "model = RegPipe()\n",
    "pipe  = machine(model, Xc, y)\n",
    "fit!(pipe, rows=train)\n",
    "ŷ = predict(pipe, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's get a feel for how we're doing"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "\n",
    "res = ŷ .- y[test]\n",
    "stem(res)\n",
    "\n",
    "xticks(fontsize=12); yticks(fontsize=12)\n",
    "xlabel(\"Index\", fontsize=14); ylabel(\"Residual (ŷ - y)\", fontsize=14)\n",
    "\n",
    "ylim([-1300, 1000])\n",
    "\n",
    "savefig(joinpath(@OUTPUT, \"ISL-lab-6-g3.svg\")) # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "\\figalt{Residuals}{ISL-lab-6-g3.svg}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "hist(res, bins=30, density=true, color=\"green\")\n",
    "\n",
    "xx = range(-1100, 1100, length=100)\n",
    "ndfit = D.fit_mle(D.Normal, res)\n",
    "lfit  = D.fit_mle(D.Laplace, res)\n",
    "\n",
    "plot(xx, pdf.(ndfit, xx), lw=3, color=\"orange\", label=\"Normal fit\")\n",
    "plot(xx, pdf.(lfit, xx), lw=3, color=\"magenta\", label=\"Laplace fit\")\n",
    "\n",
    "legend(fontsize=12)\n",
    "\n",
    "xticks(fontsize=12); yticks(fontsize=12)\n",
    "xlabel(\"Residual (ŷ - y)\", fontsize=14); ylabel(\"Density\", fontsize=14)\n",
    "xlim([-1100, 1100])\n",
    "\n",
    "savefig(joinpath(@OUTPUT, \"ISL-lab-6-g4.svg\")) # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "\\figalt{Distribution of residuals}{ISL-lab-6-g4.svg}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Basic Ridge\n",
    "\n",
    "Let's now swap the linear regressor for a Ridge one without specifying the penalty (`1` by default):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pipe.model.reg = RidgeRegressor()\n",
    "fit!(pipe, rows=train)\n",
    "ŷ = predict(pipe, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Ok that's a bit better but surely we can do better with an appropriate selection of the hyperparameter."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Cross validating"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "What penalty should you use? Let's do a simple CV to try  to find out:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r  = range(model, :(reg.lambda), lower=1e-2, upper=100_000, scale=:log10)\n",
    "tm = TunedModel(model=model, ranges=r, tuning=Grid(resolution=50),\n",
    "                resampling=CV(nfolds=3, rng=4141), measure=rms)\n",
    "mtm = machine(tm, Xc, y)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "round(best_mdl.reg.lambda, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "right, and  with that we get:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(mtm, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's see:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "\n",
    "res = ŷ .- y[test]\n",
    "stem(res)\n",
    "\n",
    "xticks(fontsize=12); yticks(fontsize=12)\n",
    "xlabel(\"Index\", fontsize=14); ylabel(\"Residual (ŷ - y)\", fontsize=14)\n",
    "\n",
    "ylim([-1300, 1000])\n",
    "\n",
    "savefig(joinpath(@OUTPUT, \"ISL-lab-6-g5.svg\")) # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "\\figalt{Ridge residuals}{ISL-lab-6-g5.svg}\n",
    "\n",
    "You can compare that with the residuals obtained earlier."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Lasso pipeline\n",
    "\n",
    "Let's do the same as above but using a Lasso model and adjusting the range a bit:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mtm.model.model.reg = LassoRegressor()\n",
    "mtm.model.range = range(model, :(reg.lambda), lower=500, upper=100_000, scale=:log10)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "round(best_mdl.reg.lambda, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Ok and let's see how that does:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(mtm, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Pretty good! and the parameters are reasonably sparse as expected:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "coefs, intercept = fitted_params(mtm.fitresult.fitresult.machine)\n",
    "round.(coefs, sigdigits=2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "with around 50% sparsity:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "sum(coefs .≈ 0) / length(coefs)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's visualise this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "stem(coefs)\n",
    "\n",
    "# name of the features including one-hot-encoded ones\n",
    "all_names = [:AtBat, :Hits, :HmRun, :Runs, :RBI, :Walks, :Years,\n",
    "             :CAtBat, :CHits, :CHmRun, :CRuns, :CRBI, :CWalks,\n",
    "             :League__A, :League__N, :Div_E, :Div_W,\n",
    "             :PutOuts, :Assists, :Errors, :NewLeague_A, :NewLeague_N]\n",
    "\n",
    "idxshow = collect(1:length(coefs))[abs.(coefs) .> 10]\n",
    "xticks(idxshow .- 1, all_names[idxshow], rotation=45, fontsize=12)\n",
    "yticks(fontsize=12)\n",
    "ylabel(\"Amplitude\", fontsize=14)\n",
    "\n",
    "savefig(joinpath(@OUTPUT, \"ISL-lab-6-g6.svg\")) # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "\\figalt{Lasso coefficients}{ISL-lab-6-g6.svg}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Elastic net pipeline"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load ElasticNetRegressor pkg=MLJLinearModels\n",
    "\n",
    "mtm.model.model.reg = ElasticNetRegressor()\n",
    "mtm.model.range = [range(model, :(reg.lambda), lower=0.1, upper=100, scale=:log10),\n",
    "                    range(model, :(reg.gamma),  lower=500, upper=10_000, scale=:log10)]\n",
    "mtm.model.tuning = Grid(resolution=10)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "@show round(best_mdl.reg.lambda, sigdigits=4)\n",
    "@show round(best_mdl.reg.gamma, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "And it's not too bad in terms of accuracy either"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(mtm, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "But the simple ridge regression seems to work best here."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "PyPlot.close_figs() # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0-DEV.350"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.0-DEV.350",
   "language": "julia"
  }
 },
 "nbformat": 4
}
