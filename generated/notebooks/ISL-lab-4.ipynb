{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"DataScienceTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stock market data\n",
    "\n",
    "Let's load the usual packages and the data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "import RDatasets: dataset\n",
    "import DataFrames: DataFrame, describe, select, Not\n",
    "import StatsBase: countmap, cor, var\n",
    "using PrettyPrinting\n",
    "\n",
    "smarket = dataset(\"ISLR\", \"Smarket\")\n",
    "@show size(smarket)\n",
    "@show names(smarket)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we often  want  to only show a few significant digits for the metrics etc, let's introduce a very simple function  that does that:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r3 = x -> round(x, sigdigits=3)\n",
    "r3(pi)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's get a description too"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "describe(smarket, :mean, :std, :eltype)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The target variable is `:Direction`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y = smarket.Direction\n",
    "X = select(smarket, Not(:Direction));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compute all the pairwise correlations; we use `Matrix` so that the dataframe entries are considered as one matrix of numbers with the same type (otherwise `cor` won't work):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cm = X |> Matrix |> cor\n",
    "round.(cm, sigdigits=1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see what the `:Volume` feature looks like:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using PyPlot\n",
    "figure(figsize=(8,6))\n",
    "plot(X.Volume)\n",
    "xlabel(\"Tick number\", fontsize=14)\n",
    "ylabel(\"Volume\", fontsize=14)\n",
    "xticks(fontsize=12)\n",
    "yticks(fontsize=12)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{volume}{ISL-lab-4-volume.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We will now try to train models; the target `:Direction` has two classes: `Up` and `Down`; it needs to be interpreted as a categorical object, and we will mark it as a _ordered factor_ to specify that 'Up' is positive and 'Down' negative (for the confusion matrix later):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y = coerce(y, OrderedFactor)\n",
    "classes(y[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that in this case the default order comes from the lexicographic order which happens  to map  to  our intuition since `D`  comes before `U`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "cm = countmap(y)\n",
    "bar([1, 2], [cm[\"Down\"], cm[\"Up\"]])\n",
    "xticks([1, 2], [\"Down\", \"Up\"], fontsize=12)\n",
    "yticks(fontsize=12)\n",
    "ylabel(\"Number of occurences\", fontsize=14)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\fig{ISL-lab-4-bal.svg}\n",
    "\n",
    "Seems pretty balanced."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now try fitting a simple logistic classifier (aka logistic regression) not using `:Year` and `:Today`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load LogisticClassifier pkg=MLJLinearModels\n",
    "X2 = select(X, Not([:Year, :Today]))\n",
    "clf = machine(LogisticClassifier(), X2, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's fit it to the data and try to reproduce the output:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(clf)\n",
    "ŷ = MLJ.predict(clf, X2)\n",
    "ŷ[1:3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that here the `ŷ` are _scores_.\n",
    "We can recover the average cross-entropy loss:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cross_entropy(ŷ, y) |> mean |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "in order to recover the class, we could use the mode and compare the misclassification rate:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict_mode(clf, X2)\n",
    "misclassification_rate(ŷ, y) |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well that's not fantastic...\n",
    "\n",
    "Let's visualise how we're doing building a confusion matrix,\n",
    "first is predicted, second is truth:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cm = confusion_matrix(ŷ, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then compute the accuracy or precision, etc. easily for instance:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show false_positive(cm)\n",
    "@show accuracy(ŷ, y)  |> r3\n",
    "@show accuracy(cm)    |> r3  # same thing\n",
    "@show precision(ŷ, y) |> r3\n",
    "@show recall(ŷ, y)    |> r3\n",
    "@show f1score(ŷ, y)   |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now train on the data before 2005 and use it to predict on the rest.\n",
    "Let's find the row indices for which the condition holds"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train = 1:findlast(X.Year .< 2005)\n",
    "test = last(train)+1:length(y);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now just re-fit the machine that we've already defined just on those rows and predict on the test:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(clf, rows=train)\n",
    "ŷ = predict_mode(clf, rows=test)\n",
    "accuracy(ŷ, y[test]) |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, that's not very good...\n",
    "Let's retrain a machine using only `:Lag1` and `:Lag2`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X3 = select(X2, [:Lag1, :Lag2])\n",
    "clf = machine(LogisticClassifier(), X3, y)\n",
    "fit!(clf, rows=train)\n",
    "ŷ = predict_mode(clf, rows=test)\n",
    "accuracy(ŷ, y[test]) |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interesting... it has higher accuracy than the model with more features! This could be investigated further by increasing the regularisation parameter but we'll leave that aside for now.\n",
    "\n",
    "We can use a trained machine to predict on new data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xnew = (Lag1 = [1.2, 1.5], Lag2 = [1.1, -0.8])\n",
    "ŷ = MLJ.predict(clf, Xnew)\n",
    "ŷ |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: when specifying data, we used a simple `NamedTuple`; we could also have defined a dataframe or any other compatible tabular container.\n",
    "Note also that we retrieved the raw predictions here i.e.: a score for each class; we could have used `predict_mode` or indeed"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mode.(ŷ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LDA\n",
    "\n",
    "Let's do a similar thing but with a LDA model this time:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load BayesianLDA pkg=MultivariateStats\n",
    "\n",
    "clf = machine(BayesianLDA(), X3, y)\n",
    "fit!(clf, rows=train)\n",
    "ŷ = predict_mode(clf, rows=test)\n",
    "\n",
    "accuracy(ŷ, y[test]) |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: `BayesianLDA` is LDA using a multivariate normal model for each class with a default prior inferred from the proportions for each class in the training data.\n",
    "You can also use the bare `LDA` model which does not make these assumptions and allows using a different metric in the transformed space, see the docs for details."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load LDA pkg=MultivariateStats\n",
    "using Distances\n",
    "\n",
    "clf = machine(LDA(dist=CosineDist()), X3, y)\n",
    "fit!(clf, rows=train)\n",
    "ŷ = predict_mode(clf, rows=test)\n",
    "\n",
    "accuracy(ŷ, y[test]) |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QDA\n",
    "\n",
    "Bayesian QDA is available via ScikitLearn:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load BayesianQDA pkg=ScikitLearn"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using it is done in much the same way as before:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "clf = machine(BayesianQDA(), X3, y)\n",
    "fit!(clf, rows=train)\n",
    "ŷ = predict_mode(clf, rows=test)\n",
    "\n",
    "accuracy(ŷ, y[test]) |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KNN\n",
    "\n",
    "We can use K-Nearest Neighbors models via the [`NearestNeighbors`](https://github.com/KristofferC/NearestNeighbors.jl) package:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load KNNClassifier pkg=NearestNeighbors\n",
    "\n",
    "knnc = KNNClassifier(K=1)\n",
    "clf = machine(knnc, X3, y)\n",
    "fit!(clf, rows=train)\n",
    "ŷ = predict_mode(clf, rows=test)\n",
    "accuracy(ŷ, y[test]) |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pretty bad... let's try with three neighbors"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "knnc.K = 3\n",
    "fit!(clf, rows=train)\n",
    "ŷ = predict_mode(clf, rows=test)\n",
    "accuracy(ŷ, y[test]) |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "A bit better but not hugely so."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Caravan insurance data\n",
    "\n",
    "The caravan dataset is part of ISLR as well:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "caravan  = dataset(\"ISLR\", \"Caravan\")\n",
    "size(caravan)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The target variable is `Purchase`, effectively  a categorical"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "purchase = caravan.Purchase\n",
    "vals     = unique(purchase)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how many of each we have"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nl1 = sum(purchase .== vals[1])\n",
    "nl2 = sum(purchase .== vals[2])\n",
    "println(\"#$(vals[1]) \", nl1)\n",
    "println(\"#$(vals[2]) \", nl2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "we can also visualise this as was done before:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "cm = countmap(purchase)\n",
    "bar([1, 2], [cm[\"No\"], cm[\"Yes\"]])\n",
    "xticks([1, 2], [\"No\", \"Yes\"], fontsize=12)\n",
    "yticks(fontsize=12)\n",
    "ylabel(\"Number of occurences\", fontsize=14)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\fig{ISL-lab-4-bal2.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "that's quite unbalanced.\n",
    "\n",
    "Apart from the target, all other variables are numbers; we can standardize the data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y, X = unpack(caravan, ==(:Purchase), col->true)\n",
    "\n",
    "mstd = machine(Standardizer(), X)\n",
    "fit!(mstd)\n",
    "Xs = transform(mstd, X)\n",
    "\n",
    "var(Xs[:,1]) |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: in MLJ, it is recommended to work with pipelines / networks when possible and not do \"step-by-step\" transformation and fitting of the data as this is more error prone. We do it here to stick to the ISL tutorial.\n",
    "\n",
    "We split the data in the first 1000 rows for testing and the rest for training:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "test = 1:1000\n",
    "train = last(test)+1:nrows(Xs);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now fit a KNN model and check the misclassification rate"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "clf = machine(KNNClassifier(K=3), Xs, y)\n",
    "fit!(clf, rows=train)\n",
    "ŷ = predict_mode(clf, rows=test)\n",
    "\n",
    "accuracy(ŷ, y[test]) |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "that looks good but recall the problem is very unbalanced"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mean(y[test] .!= \"No\") |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's fit a logistic classifier to this problem"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "clf = machine(LogisticClassifier(), Xs, y)\n",
    "fit!(clf, rows=train)\n",
    "ŷ = predict_mode(clf, rows=test)\n",
    "\n",
    "accuracy(ŷ, y[test]) |> r3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ROC and AUC\n",
    "\n",
    "Since we have a probabilistic classifier, we can also check metrics that take _scores_ into account such as the area under the ROC curve (AUC):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = MLJ.predict(clf, rows=test)\n",
    "\n",
    "auc(ŷ, y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also display the curve itself"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fprs, tprs, thresholds = roc(ŷ, y[test])\n",
    "\n",
    "figure(figsize=(8,6))\n",
    "plot(fprs, tprs)\n",
    "\n",
    "xlabel(\"False Positive Rate\", fontsize=14)\n",
    "ylabel(\"True Positive Rate\", fontsize=14)\n",
    "xticks(fontsize=12)\n",
    "yticks(fontsize=12)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{ROC}{ISL-lab-4-roc.svg}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "kernelspec": {
   "name": "julia-1.4",
   "display_name": "Julia 1.4.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
