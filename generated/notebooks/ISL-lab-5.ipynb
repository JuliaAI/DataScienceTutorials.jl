{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"DataScienceTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting started"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "import RDatasets: dataset\n",
    "import DataFrames: DataFrame, select\n",
    "auto = dataset(\"ISLR\", \"Auto\")\n",
    "y, X = unpack(auto, ==(:MPG), col->true)\n",
    "train, test = partition(eachindex(y), 0.5, shuffle=true, rng=444);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note the use of `rng=` to seed the shuffling of indices so that the results are reproducible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Polynomial regression"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load LinearRegressor pkg=MLJLinearModels"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this part we only build models with the `Horsepower` feature."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using PyPlot\n",
    "\n",
    "figure(figsize=(8,6))\n",
    "plot(X.Horsepower, y, ls=\"none\", marker=\"o\")\n",
    "\n",
    "xlabel(\"Horsepower\", fontsize=14)\n",
    "xticks(50:50:250, fontsize=12)\n",
    "yticks(10:10:50, fontsize=12)\n",
    "ylabel(\"MPG\", fontsize=14)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{MPG v Horsepower}{ISL-lab-5-g1.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's get a baseline:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lm = LinearRegressor()\n",
    "mlm = machine(lm, select(X, :Horsepower), y)\n",
    "fit!(mlm, rows=train)\n",
    "rms(predict(mlm, rows=test), y[test])^2"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we square the measure to  match the results obtained in the ISL labs where the mean squared error (here we use the `rms` which is the square root of that)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xx = (Horsepower=range(50, 225, length=100) |> collect, )\n",
    "yy = predict(mlm, xx)\n",
    "\n",
    "figure(figsize=(8,6))\n",
    "plot(X.Horsepower, y, ls=\"none\", marker=\"o\")\n",
    "plot(xx.Horsepower, yy, lw=3)\n",
    "\n",
    "xlabel(\"Horsepower\", fontsize=14)\n",
    "xticks(50:50:250, fontsize=12)\n",
    "yticks(10:10:50, fontsize=12)\n",
    "ylabel(\"MPG\", fontsize=14)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{1st order baseline}{ISL-lab-5-g2.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now want to build three polynomial models of degree 1, 2 and 3 respectively; we start by forming the corresponding feature matrix:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "hp = X.Horsepower\n",
    "Xhp = DataFrame(hp1=hp, hp2=hp.^2, hp3=hp.^3);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we  can write a simple pipeline where the first step selects the features we want (and with it the degree of the polynomial) and the second is the linear regressor:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "LinMod = @pipeline(FeatureSelector(features=[:hp1]),\n",
    "                   LinearRegressor());"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can  instantiate and fit 3 models where we specify the features each time:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lr1 = machine(LinMod, Xhp, y) # poly of degree 1 (line)\n",
    "fit!(lr1, rows=train)\n",
    "\n",
    "LinMod.feature_selector.features = [:hp1, :hp2] # poly of degree 2\n",
    "lr2 = machine(LinMod, Xhp, y)\n",
    "fit!(lr2, rows=train)\n",
    "\n",
    "LinMod.feature_selector.features = [:hp1, :hp2, :hp3] # poly of degree 3\n",
    "lr3 = machine(LinMod, Xhp, y)\n",
    "fit!(lr3, rows=train)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the performances on the test set"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "get_mse(lr) = rms(predict(lr, rows=test), y[test])^2\n",
    "\n",
    "@show get_mse(lr1)\n",
    "@show get_mse(lr2)\n",
    "@show get_mse(lr3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualise the models"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "hpn  = xx.Horsepower\n",
    "Xnew = DataFrame(hp1=hpn, hp2=hpn.^2, hp3=hpn.^3)\n",
    "\n",
    "yy1 = predict(lr1, Xnew)\n",
    "yy2 = predict(lr2, Xnew)\n",
    "yy3 = predict(lr3, Xnew)\n",
    "\n",
    "figure(figsize=(8,6))\n",
    "plot(X.Horsepower, y, ls=\"none\", marker=\"o\")\n",
    "plot(xx.Horsepower, yy1, lw=3, label=\"Order 1\")\n",
    "plot(xx.Horsepower, yy2, lw=3, label=\"Order 2\")\n",
    "plot(xx.Horsepower, yy3, lw=3, label=\"Order 3\")\n",
    "\n",
    "legend(fontsize=14)\n",
    "\n",
    "xlabel(\"Horsepower\", fontsize=14)\n",
    "xticks(50:50:250, fontsize=12)\n",
    "yticks(10:10:50, fontsize=12)\n",
    "ylabel(\"MPG\", fontsize=14)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{1st, 2nd and 3d order fit}{ISL-lab-5-g3.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Folds Cross Validation\n",
    "\n",
    "Let's crossvalidate over the degree of the  polynomial.\n",
    "\n",
    "**Note**: there's a  bit of gymnastics here because MLJ doesn't directly support a polynomial regression; see our tutorial on [tuning models](/getting-started/model-tuning/) for a gentler introduction to model tuning.\n",
    "The gist of the following code is to create a dataframe where each column is a power of the `Horsepower` feature from 1 to 10 and we build a series of regression models using incrementally more of those features (higher degree):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xhp = DataFrame([hp.^i for i in 1:10])\n",
    "\n",
    "cases = [[Symbol(\"x$j\") for j in 1:i] for i in 1:10]\n",
    "r = range(LinMod, :(feature_selector.features), values=cases)\n",
    "\n",
    "tm = TunedModel(model=LinMod, ranges=r, resampling=CV(nfolds=10), measure=rms)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we're left with fitting the tuned model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mtm = machine(tm, Xhp, y)\n",
    "fit!(mtm)\n",
    "rep = report(mtm)\n",
    "\n",
    "res = rep.plotting\n",
    "\n",
    "@show round.(res.measurements.^2, digits=2)\n",
    "@show argmin(res.measurements)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "So the conclusion here is that the 5th order polynomial does quite well.\n",
    "\n",
    "In ISL they use a different seed so the results are a bit different but comparable."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xnew = DataFrame([hpn.^i for i in 1:10])\n",
    "yy5 = predict(mtm, Xnew)\n",
    "\n",
    "figure(figsize=(8,6))\n",
    "plot(X.Horsepower, y, ls=\"none\", marker=\"o\")\n",
    "plot(xx.Horsepower, yy5, lw=3)\n",
    "\n",
    "xlabel(\"Horsepower\", fontsize=14)\n",
    "xticks(50:50:250, fontsize=12)\n",
    "yticks(10:10:50, fontsize=12)\n",
    "ylabel(\"MPG\", fontsize=14)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{5th order fit}{ISL-lab-5-g4.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Bootstrap\n",
    "\n",
    "_Bootstrapping is not currently supported in MLJ._"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "kernelspec": {
   "name": "julia-1.4",
   "display_name": "Julia 1.4.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
