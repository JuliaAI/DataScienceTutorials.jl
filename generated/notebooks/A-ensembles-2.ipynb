{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"MLJTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Prelims\n",
    "\n",
    "This tutorial builds upon the previous ensemble tutorial with a home-made Random Forest regressor on the \"boston\" dataset."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ, PyPlot, PrettyPrinting, Random,\n",
    "      DataFrames\n",
    "MLJ.color_off() # hide\n",
    "X, y = @load_boston\n",
    "sch = schema(X)\n",
    "p = length(sch.names)\n",
    "n = sch.nrows\n",
    "@show (n, p)\n",
    "describe(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's load the decision tree regressor"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load DecisionTreeRegressor"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's first check the performances of just a single Decision Tree Regressor (DTR for short):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tree = machine(DecisionTreeRegressor(), X, y)\n",
    "e = evaluate!(tree, resampling=Holdout(fraction_train=0.8),\n",
    "              measure=[rms, rmslp1])\n",
    "e |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note that multiple measures can be reported simultaneously."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Random forest\n",
    "\n",
    "Let's create an ensemble of DTR and fix the number of subfeatures to 3 for now."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "forest = EnsembleModel(atom=DecisionTreeRegressor())\n",
    "forest.atom.n_subfeatures = 3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "(**NB**: we could have fixed `n_subfeatures` in the DTR constructor too).\n",
    "\n",
    "To get an idea of how many trees are needed, we can follow the evaluation of the error (say the `rms`) for an increasing number of tree over several sampling round."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(5) # for reproducibility\n",
    "m = machine(forest, X, y)\n",
    "r = range(forest, :n, lower=10, upper=1000)\n",
    "curves = learning_curve!(m, resampling=Holdout(fraction_train=0.8),\n",
    "                         range=r, measure=rms);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "let's plot the curves"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "plot(curves.parameter_values, curves.measurements)\n",
    "xlabel(\"Number of trees\", fontsize=14)\n",
    "xticks([10, 250, 500, 750, 1000])\n",
    "ylim([4, 5])\n",
    "\n",
    "savefig(joinpath(@OUTPUT, \"A-ensembles-2-curves.svg\")) # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "\\figalt{RMS vs number of trees}{A-ensembles-2-curves.svg}\n",
    "\n",
    "So out of this curve we could decide for instance to go for 300 trees:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "forest.n = 300;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Tuning\n",
    "\n",
    "As `forest` is a composite model, it has nested hyperparameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "params(forest) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's define a range for the number of subfeatures and for the bagging fraction:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r_sf = range(forest, :(atom.n_subfeatures), lower=1, upper=12)\n",
    "r_bf = range(forest, :bagging_fraction, lower=0.4, upper=1.0);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "And build a tuned model as usual that we fit on a 80/20 split.\n",
    "We use a low-resolution grid here to make this tutorial faster but you could of course use a finer grid."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tuned_forest = TunedModel(model=forest,\n",
    "                          tuning=Grid(resolution=3),\n",
    "                          resampling=CV(nfolds=6, rng=32),\n",
    "                          ranges=[r_sf, r_bf],\n",
    "                          measure=rms)\n",
    "m = machine(tuned_forest, X, y)\n",
    "e = evaluate!(m, resampling=Holdout(fraction_train=0.8),\n",
    "              measure=[rms, rmslp1])\n",
    "e |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Reporting\n",
    "Again, you could show a 2D heatmap of the hyperparameters"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = report(m)\n",
    "\n",
    "figure(figsize=(8,6))\n",
    "\n",
    "res = r.plotting\n",
    "\n",
    "vals_sf = res.parameter_values[:, 1]\n",
    "vals_bf = res.parameter_values[:, 2]\n",
    "\n",
    "tricontourf(vals_sf, vals_bf, res.measurements)\n",
    "xticks(1:3:12, fontsize=12)\n",
    "xlabel(\"Number of sub-features\", fontsize=14)\n",
    "yticks(0.4:0.2:1, fontsize=12)\n",
    "ylabel(\"Bagging fraction\", fontsize=14)\n",
    "\n",
    "savefig(joinpath(@OUTPUT, \"A-ensembles-2-heatmap.svg\")) # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "\\fig{A-ensembles-2-heatmap.svg}\n",
    "\n",
    "Even though we've only done a very rough search, it seems that around 7 sub-features and a bagging fraction of around `0.75` work well.\n",
    "\n",
    "Now that the machine `m` is trained, you can use use it for predictions (implicitly, this will use the best model).\n",
    "For instance we could look at predictions on the whole dataset:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(m, X)\n",
    "rms(ŷ, y)\n",
    "\n",
    "PyPlot.close_figs() # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0-DEV.350"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.0-DEV.350",
   "language": "julia"
  }
 },
 "nbformat": 4
}
