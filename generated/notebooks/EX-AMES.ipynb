{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"MLJTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Baby steps\n",
    "\n",
    "Let's load a reduced version of the well-known Ames House Price data set (containing six of the more important categorical features and six of the more important numerical features).\n",
    "As \"iris\" the dataset is so common that you can load it directly with `@load_ames` and the reduced version via `@load_reduced_ames`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ, PrettyPrinting, DataFrames, Statistics\n",
    "MLJ.color_off() # hide\n",
    "\n",
    "X, y = @load_reduced_ames\n",
    "X = DataFrame(X)\n",
    "@show size(X)\n",
    "first(X, 3) |> pretty"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "and the target is a continuous vector:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show y[1:3]\n",
    "scitype(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "so this is a standard regression problem with a mix of categorical and continuous input.\n",
    "\n",
    "## Dummy model\n",
    "\n",
    "Remember that a model is just a container for hyperparameters; let's take a particularly simple one: the constant regression."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "creg = ConstantRegressor()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Wrapping the model in data creates a *machine* which will store training outcomes (*fit-results*)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cmach = machine(creg, X, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "You can now train the machine specifying the data it should be trained on (if unspecified, all the data will be used);"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.70, shuffle=true); # 70:30 split\n",
    "fit!(cmach, rows=train)\n",
    "ŷ = predict(cmach, rows=test)\n",
    "ŷ[1:3] |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Observe that the output is probabilistic, each element is a univariate normal distribution (with the same mean and variance as it's a constant model).\n",
    "\n",
    "You can recover deterministic output by either computing the mean of predictions or using `predict_mean` directly (the `mean` function can  bve applied to any distribution from [`Distributions.jl`](https://github.com/JuliaStats/Distributions.jl)):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict_mean(cmach, rows=test)\n",
    "ŷ[1:3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "You can then call one of the loss functions to assess the quality of the model by comparing the performances on the test set:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rmsl(ŷ, y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## KNN-Ridge blend\n",
    "\n",
    "Let's try something a bit fancier than a constant regressor.\n",
    "\n",
    "* one-hot-encode categorical inputs\n",
    "* log-transform the target\n",
    "* fit both a KNN regression and a Ridge regression on the data\n",
    "* Compute a weighted average of individual model predictions\n",
    "* inverse transform (exponentiate) the blended prediction\n",
    "\n",
    "You will first define a fixed model where all hyperparameters are specified or set to default. Then you will see how to create a model around a learning network that can be tuned."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load RidgeRegressor pkg=\"MultivariateStats\"\n",
    "@load KNNRegressor"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Using the expanded syntax\n",
    "\n",
    "Let's start by defining the source nodes:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xs = source(X)\n",
    "ys = source(y, kind=:target)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "On the \"first layer\", there's one hot encoder and a log transform, these will respectively lead to node `W` and node `z`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "hot = machine(OneHotEncoder(), Xs)\n",
    "\n",
    "W = transform(hot, Xs)\n",
    "z = log(ys);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "On the \"second layer\", there's a KNN regressor and a ridge regressor, these lead to node `ẑ₁` and `ẑ₂`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "knn   = machine(KNNRegressor(K=5), W, z)\n",
    "ridge = machine(RidgeRegressor(lambda=2.5), W, z)\n",
    "\n",
    "ẑ₁ = predict(ridge, W)\n",
    "ẑ₂ = predict(knn, W)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "On the \"third layer\", there's a weighted combination of the two regression models:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ẑ = 0.3ẑ₁ + 0.7ẑ₂;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "And finally we need to invert the initial transformation of the target (which was a log):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = exp(ẑ);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "You've now defined a full learning network which you can fit and use for prediction:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(ŷ, rows=train)\n",
    "ypreds = ŷ(rows=test)\n",
    "rmsl(y[test], ypreds)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Using the \"arrow\" syntax\n",
    "\n",
    "If you're using Julia 1.3, you can use the following syntax to do the same thing."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*First layer*: one hot encoding and log transform:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "W = Xs |> OneHotEncoder()\n",
    "z = ys |> log;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*Second layer*: KNN Regression and Ridge regression"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ẑ₁ = (W, z) |> KNNRegressor(K=5)\n",
    "ẑ₂ = (W, z) |> RidgeRegressor(lambda=2.5);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*Third layer*: weighted sum of the two models:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ẑ = 0.3ẑ₁ + 0.7ẑ₂;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "then the inverse transform"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = exp(ẑ);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "You can then fit and evaluate the model as usual:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(ŷ, rows=train)\n",
    "rmsl(y[test], ŷ(rows=test))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Tuning the model\n",
    "\n",
    "So far the hyperparameters were explicitly given but it makes more sense to learn them.\n",
    "For this, we define a model around the learning network which can then be trained and tuned as any model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mutable struct KNNRidgeBlend <: DeterministicNetwork\n",
    "    knn_model::KNNRegressor\n",
    "    ridge_model::RidgeRegressor\n",
    "    knn_weight::Float64\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We must specify how such a model should be fit, which is effectively just the learning network we had defined before except that now the parameters are contained in the struct:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function MLJ.fit(model::KNNRidgeBlend, verbosity::Int, X, y)\n",
    "    Xs = source(X)\n",
    "    ys = source(y, kind=:target)\n",
    "    hot = machine(OneHotEncoder(), Xs)\n",
    "    W = transform(hot, Xs)\n",
    "    z = log(ys)\n",
    "    ridge_model = model.ridge_model\n",
    "    knn_model = model.knn_model\n",
    "    ridge = machine(ridge_model, W, z)\n",
    "    knn = machine(knn_model, W, z)\n",
    "    # and finally\n",
    "    ẑ = model.knn_weight * predict(knn, W) + (1.0 - model.knn_weight) * predict(ridge, W)\n",
    "    ŷ = exp(ẑ)\n",
    "    fit!(ŷ, verbosity=0)\n",
    "    return fitresults(ŷ)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "**Note**: you really  want to set `verbosity=0` here otherwise in the tuning you will get a lot of verbose output!\n",
    "\n",
    "You can now instantiate and fit such a model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "krb = KNNRidgeBlend(KNNRegressor(K=5), RidgeRegressor(lambda=2.5), 0.3)\n",
    "mach = machine(krb, X, y)\n",
    "fit!(mach, rows=train)\n",
    "\n",
    "preds = predict(mach, rows=test)\n",
    "rmsl(y[test], preds)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "But more interestingly, the hyperparameters of the model can be tuned.\n",
    "\n",
    "Before we get started, it's important to note that the hyperparameters of the model have different levels of *nesting*. This becomes explicit when trying to access elements:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show krb.knn_weight\n",
    "@show krb.knn_model.K\n",
    "@show krb.ridge_model.lambda"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "You can also see all the hyperparameters using the `params` function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "params(krb) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The range of values to do your hyperparameter tuning over should follow the nesting structure reflected by `params`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "k_range = range(krb, :(knn_model.K), lower=2, upper=100, scale=:log10)\n",
    "l_range = range(krb, :(ridge_model.lambda), lower=1e-4, upper=10, scale=:log10)\n",
    "w_range = range(krb, :(knn_weight), lower=0.1, upper=0.9)\n",
    "\n",
    "ranges = [k_range, l_range, w_range]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Now there remains to define how the tuning should be done, let's just specify a very coarse grid tuning with cross validation and instantiate a tuned model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tuning = Grid(resolution=3)\n",
    "resampling = CV(nfolds=6)\n",
    "\n",
    "tm = TunedModel(model=krb, tuning=tuning, resampling=resampling,\n",
    "                ranges=ranges, measure=rmsl)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "which we can now finally fit..."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mtm = machine(tm, X, y)\n",
    "fit!(mtm, rows=train);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "To retrieve the best model, you can use:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "krb_best = fitted_params(mtm).best_model\n",
    "@show krb_best.knn_model.K\n",
    "@show krb_best.ridge_model.lambda\n",
    "@show krb_best.knn_weight"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "you can also use `mtm` to make predictions (which will be done using the best model)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "preds = predict(mtm, rows=test)\n",
    "rmsl(y[test], preds)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0-DEV.350"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.0-DEV.350",
   "language": "julia"
  }
 },
 "nbformat": 4
}
