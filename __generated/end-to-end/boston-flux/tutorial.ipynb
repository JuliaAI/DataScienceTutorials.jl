{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/end-to-end/boston-flux/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/end-to-end/boston-flux/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Main author**: Ayush Shridhar (ayush-1506)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "## Getting started\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import MLJFlux\n",
    "import MLJ\n",
    "import DataFrames: DataFrame\n",
    "import Statistics\n",
    "import Flux\n",
    "using Random\n",
    "\n",
    "Random.seed!(11)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the Boston dataset. Our aim will be to implement a\n",
    "neural network regressor to predict the price of a house,\n",
    "given a number of features."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "features, targets = MLJ.@load_boston\n",
    "features = DataFrame(features)\n",
    "@show size(features)\n",
    "@show targets[1:3]\n",
    "first(features, 3) |> MLJ.pretty"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next obvious steps: partitioning into train and test set"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = MLJ.partition(collect(eachindex(targets)), 0.70, rng=52)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us try to implement an Neural Network regressor using\n",
    "Flux.jl. MLJFlux.jl provides an MLJ interface to the Flux.jl\n",
    "deep learning framework. The package provides four essential\n",
    "models: `NeuralNetworkRegressor, MultitargetNeuralNetworkRegressor,\n",
    "NeuralNetworkClassifier` and `ImageClassifier`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the heart of these models is a neural network. This is specified using\n",
    "the `builder` parameter. Creating a builder object consists of two steps:\n",
    "Step 1: Creating a new struct inherited from `MLJFlux.Builder`. `MLJFlux.Builder`\n",
    "is an abstract structure used for the purpose of dispatching. Suppose we define\n",
    "a new struct called `MyNetworkBuilder`. This can contain any attribute required to\n",
    "build the model later. (Step 2). Let's use Dense Neural Network with 2 hidden layers."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mutable struct MyNetworkBuilder <: MLJFlux.Builder\n",
    "    n1::Int #Number of cells in the first hidden layer\n",
    "    n2::Int #Number of cells in the second hidden layer\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 2: Building the neural network from this object.  Extend the\n",
    "`MLJFlux.build` function. This takes in 4 arguments: The\n",
    "`MyNetworkBuilder` instance, a random number generator or seed\n",
    "`rng`, the input dimension (`n_in`) and output dimension (`n_out`)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function MLJFlux.build(model::MyNetworkBuilder, rng, n_in, n_out)\n",
    "    init = Flux.glorot_uniform(rng)\n",
    "    layer1 = Flux.Dense(n_in, model.n1, init=init)\n",
    "    layer2 = Flux.Dense(model.n1, model.n2, init=init)\n",
    "    layer3 = Flux.Dense(model.n2, n_out, init=init)\n",
    "    return Flux.Chain(layer1, layer2, layer3)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternatively, there a macro shortcut to take care of both steps at\n",
    "once. For details, do `?MLJFlux.@builder`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All definitions ready, let us create an object of this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "myregressor = MyNetworkBuilder(20, 10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the boston dataset is a regression problem, we'll be using\n",
    "`NeuralNetworkRegressor` here. One thing to remember is that\n",
    "a `NeuralNetworkRegressor` object works seamlessly like any other\n",
    "MLJ model: you can wrap it in an  MLJ `machine` and do anything\n",
    "you'd do otherwise."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by defining our NeuralNetworkRegressor object, that takes `myregressor`\n",
    "as it's parameter."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nnregressor = MLJFlux.NeuralNetworkRegressor(builder=myregressor, epochs=10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other parameters that NeuralNetworkRegressor takes can be found here:\n",
    "https://github.com/alan-turing-institute/MLJFlux.jl#model-hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`nnregressor` now acts like any other MLJ model. Let's try wrapping it in a\n",
    "MLJ machine and calling `fit!, predict`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = MLJ.machine(nnregressor, features, targets)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's fit this on the train set"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "MLJ.fit!(mach, rows=train, verbosity=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the training loss decreases at each epoch, showing the the neural network\n",
    "is gradually learning form the training set."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "preds = MLJ.predict(mach, features[test, :])\n",
    "\n",
    "print(preds[1:5])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's retrain our model. One thing to remember is that retrainig may OR may not\n",
    "re-initialize our neural network model parameters. For example, changing the number of\n",
    "epochs to 15 will not causes the model to train to 15 epcohs, but just 5 additional\n",
    "epochs."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nnregressor.epochs = 15\n",
    "\n",
    "MLJ.fit!(mach, rows=train, verbosity=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can always specify that you want to retrain the model from scratch using the force=true\n",
    "parameter. (Look at documentation for `fit!` for more)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, changing parameters such as batch_size will necessarily cause re-training from scratch."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nnregressor.batch_size = 2\n",
    "MLJ.fit!(mach, rows=train, verbosity=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another bit to remember here is that changing the optimiser doesn't cause retaining by default.\n",
    "However, the `optimiser_changes_trigger_retraining` in NeuralNetworkRegressor can be toggled to\n",
    "accomodate this. This allows one to modify the learning rate, for example, after an initial burn-in period."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspecting out-of-sample loss as a function of epochs"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = MLJ.range(nnregressor, :epochs, lower=1, upper=30, scale=:log10)\n",
    "curve = MLJ.learning_curve(nnregressor, features, targets,\n",
    "                       range=r,\n",
    "                       resampling=MLJ.Holdout(fraction_train=0.7),\n",
    "                       measure=MLJ.l2)\n",
    "\n",
    "using Plots\n",
    "\n",
    "plot(curve.parameter_values, curve.measurements, yaxis=:log, legend=false)\n",
    "\n",
    "xlabel!(curve.parameter_name)\n",
    "ylabel!(\"l2-log\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{BostonFlux1}{EX-boston-flux-g1.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## Tuning\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned above, `nnregressor` can act like any other MLJ model. Let's try to tune the\n",
    "batch_size parameter."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "bs = MLJ.range(nnregressor, :batch_size, lower=1, upper=5)\n",
    "\n",
    "tm = MLJ.TunedModel(model=nnregressor, ranges=[bs, ], measure=MLJ.l2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For more on tuning, refer to the model-tuning tutorial."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m = MLJ.machine(tm, features, targets)\n",
    "\n",
    "MLJ.fit!(m)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This evaluated the model at each value of our range.\n",
    "The best value is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "MLJ.fitted_params(m).best_model.batch_size"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
