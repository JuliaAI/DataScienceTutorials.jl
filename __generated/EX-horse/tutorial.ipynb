{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-horse/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-horse/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "## Initial data processing\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "In this example, we consider the [UCI \"horse colic\"\n",
    "dataset](http://archive.ics.uci.edu/ml/datasets/Horse+Colic)\n",
    "\n",
    "This is a reasonably messy classification problem with missing values etc and so some\n",
    "work should be expected in the feature processing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "### Getting the data\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "The data is pre-split in training and testing and we will keep it as such"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "using HTTP\n",
    "using CSV\n",
    "import DataFrames: DataFrame, select!, Not\n",
    "req1 = HTTP.get(\"http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data\")\n",
    "req2 = HTTP.get(\"http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.test\")\n",
    "header = [\"surgery\", \"age\", \"hospital_number\",\n",
    "    \"rectal_temperature\", \"pulse\",\n",
    "    \"respiratory_rate\", \"temperature_extremities\",\n",
    "    \"peripheral_pulse\", \"mucous_membranes\",\n",
    "    \"capillary_refill_time\", \"pain\",\n",
    "    \"peristalsis\", \"abdominal_distension\",\n",
    "    \"nasogastric_tube\", \"nasogastric_reflux\",\n",
    "    \"nasogastric_reflux_ph\", \"feces\", \"abdomen\",\n",
    "    \"packed_cell_volume\", \"total_protein\",\n",
    "    \"abdomcentesis_appearance\", \"abdomcentesis_total_protein\",\n",
    "    \"outcome\", \"surgical_lesion\", \"lesion_1\", \"lesion_2\", \"lesion_3\",\n",
    "    \"cp_data\"]\n",
    "csv_opts = (header=header, delim=' ', missingstring=\"?\",\n",
    "            ignorerepeated=true)\n",
    "data_train = CSV.read(req1.body, DataFrame; csv_opts...)\n",
    "data_test  = CSV.read(req2.body, DataFrame; csv_opts...)\n",
    "@show size(data_train)\n",
    "@show size(data_test)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Inspecting columns\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "To simplify the analysis, we will drop the columns `Lesion *` as they would need\n",
    "specific re-encoding which would distract us a bit."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "unwanted = [:lesion_1, :lesion_2, :lesion_3]\n",
    "data = vcat(data_train, data_test)\n",
    "select!(data, Not(unwanted));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also keep track of the initial train-test split"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train = 1:nrows(data_train)\n",
    "test = last(train) .+ (1:nrows(data_test));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We know from reading the description that some of these features represent multiclass\n",
    "data; to facilitate the interpretation, we can use `autotype` from `ScientificTypes`.\n",
    "By default, `autotype` will check all columns and suggest a Finite type assuming there\n",
    "are relatively few distinct values in the column.  More sophisticated rules can be\n",
    "passed, see\n",
    "[ScientificTypes.jl](https://alan-turing-institute.github.io/ScientificTypes.jl/dev/):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "coerce!(data, autotype(data));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see column by column whether it looks ok now"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "schema(data)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most columns are now treated as either Multiclass or Ordered, this corresponds to the\n",
    "[description of the data](https://archive.ics.uci.edu/ml/datasets/Horse+Colic). For\n",
    "instance:\n",
    "\n",
    "- `Surgery` is described as `1=yes / 2=no`\n",
    "- `Age` is described as `1=adult / 2=young`\n",
    "\n",
    "Inspecting the rest of the descriptions and the current scientific type,\n",
    "there are a few more things that can be observed:\n",
    "\n",
    "- hospital number is still a count, this means that there are relatively many hospitals and so  that's  probably not very useful,\n",
    "- pulse and respiratory rate are still as count but the data description suggests that they can be considered as continuous"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "length(unique(data.hospital_number))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "yeah let's drop that"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "data = select!(data, Not(:hospital_number));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "let's also coerce the pulse and respiratory rate, by coercing all remaining `Count`\n",
    "features to `Continuous`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "coerce!(data, Count => Continuous)\n",
    "schema(data)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Dealing with missing values\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "There's quite a lot of missing values, in this tutorial we'll be a bit rough in how we deal with them applying the following rules of thumb:\n",
    "\n",
    "- drop the rows where the outcome is unknown\n",
    "- drop columns with more than 20% missing values\n",
    "- simple imputation of whatever's left"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "missing_outcome = ismissing.(data.outcome)\n",
    "idx_missing_outcome = missing_outcome |> findall"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok there's only two row which is nice, let's remove them from the train/test indices:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train = setdiff!(train |> collect, idx_missing_outcome)\n",
    "test = setdiff!(test |> collect, idx_missing_outcome)\n",
    "all = vcat(train, test);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's look at how many missings there are per features"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for name in names(data)\n",
    "    col = data[all, name]\n",
    "    ratio_missing = sum(ismissing.(col)) / length(all) * 100\n",
    "    println(rpad(name, 30), round(ratio_missing, sigdigits=3))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's drop the ones with more than 20% (quite a few!)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "unwanted = [:peripheral_pulse, :nasogastric_tube, :nasogastric_reflux,\n",
    "            :nasogastric_reflux_ph, :feces, :abdomen,\n",
    "            :abdomcentesis_appearance, :abdomcentesis_total_protein]\n",
    "select!(data, Not(unwanted));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we could have done this better and investigated the nature of the features for\n",
    "which there's a lot of missing values but don't forget that our goal is to showcase MLJ!\n",
    "\n",
    "Let's conclude by filling all missing values and separating the features from the target:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load FillImputer\n",
    "filler = machine(FillImputer(), data[all, :]) |> fit!\n",
    "data = MLJ.transform(filler, data)\n",
    "\n",
    "y, X = unpack(data, ==(:outcome)); # a vector and a table"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## A baseline model\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "Let's define a first sensible model and get a baseline, basic steps are:\n",
    "- one-hot-encode the categoricals\n",
    "- feed all this into a classifier"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load OneHotEncoder\n",
    "MultinomialClassifier = @load MultinomialClassifier pkg=\"MLJLinearModels\""
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have convenient handles over the training data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xtrain = X[train,:]\n",
    "ytrain = y[train];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And let's define a pipeline corresponding to the operations above"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pipe = OneHotEncoder() |>  MultinomialClassifier()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are log loss and accuracy estimates of model performance, obtained by training\n",
    "on 90% of the `train` data, and computing the measures on the remaining 10%:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "metrics = [log_loss, accuracy]\n",
    "evaluate(\n",
    "    pipe, Xtrain, ytrain;\n",
    "    resampling = Holdout(fraction_train=0.9),\n",
    "    measures = metrics,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we perform cross-validation instead, we also get a very rough idea of the\n",
    "uncertainties in our estimates (\"SE\" is \"standard error\"):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "evaluate(pipe, Xtrain, ytrain; resampling=CV(nfolds=6), measures=metrics)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And for a final comparison, here's how we do on the test set, which we have not yet\n",
    "touched:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(pipe, Xtrain, ytrain) |> fit!\n",
    "fit!(mach, verbosity=0)\n",
    "yhat_prob = predict(mach, X[test,:])\n",
    "m = log_loss(yhat_prob, y[test])\n",
    "println(\"log loss: \", round(m, sigdigits=4))\n",
    "\n",
    "yhat = mode.(yhat_prob)\n",
    "m = accuracy(yhat, y[test])\n",
    "println(\"accuracy: \", round(m, sigdigits=4))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's not bad at all actually."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since our data set is relatively small, we do not expect a statistically significant\n",
    "improvement with hyperparameter tuning. However, for the sake of illustration we'll\n",
    "attempt this."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lambdas = range(pipe, :(multinomial_classifier.lambda), lower=1e-3, upper=100, scale=:log10)\n",
    "tuned_pipe = TunedModel(\n",
    "    pipe;\n",
    "    tuning=Grid(resolution=20),\n",
    "    range=lambdas, measure=log_loss,\n",
    "    acceleration=CPUThreads(),\n",
    ")\n",
    "mach = machine(tuned_pipe, Xtrain, ytrain) |> fit!\n",
    "best_pipe = fitted_params(mach).best_model\n",
    "\n",
    "evaluate!(mach; resampling=CV(nfolds=6), measures=metrics)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, not likely a statistically significant difference. Nevertheless, let's see how we do\n",
    "with accuracy on a holdout set:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(mach) # fit on all the train data\n",
    "yhat = predict_mode(mach, X[test,:])\n",
    "m = accuracy(yhat, y[test])\n",
    "println(\"accuracy: \", round(m, sigdigits=4))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "So an improvement after all on the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We've probably reached the limit of a simple linear model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## Trying another model\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "There are lots of categoricals, so maybe it's just better to use something that deals\n",
    "well with that like a tree-based classifier."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "EvoTreeClassifier = @load EvoTreeClassifier\n",
    "model = EvoTreeClassifier()\n",
    "mach = machine(model, Xtrain, ytrain)\n",
    "evaluate!(mach; resampling=CV(nfolds=6), measures=metrics)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's better accuracy, although not significantly better, than the other CV estimates\n",
    "based on `train`, and without any tuning. Do we actually do better on the `test` set?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(mach) # fit on all the train data\n",
    "yhat = predict_mode(mach, X[test,:])\n",
    "m = accuracy(yhat, y[test])\n",
    "println(\"accuracy: \", round(m, sigdigits=4))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, the best so far."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could investigate more, try tuning etc, but the key points of this tutorial was to\n",
    "show how to handle data with missing values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
