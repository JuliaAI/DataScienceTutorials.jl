{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/A-learning-networks/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/A-learning-networks/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminary steps\n",
    "\n",
    "Let's generate a `DataFrame` with some dummy regression data, let's also load the good old ridge regressor."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ, StableRNGs\n",
    "import DataFrames\n",
    "Ridge = @load RidgeRegressor pkg=MultivariateStats\n",
    "\n",
    "rng = StableRNG(551234) # for reproducibility\n",
    "\n",
    "x1 = rand(rng, 300)\n",
    "x2 = rand(rng, 300)\n",
    "x3 = rand(rng, 300)\n",
    "y = exp.(x1 - x2 -2x3 + 0.1*rand(rng, 300))\n",
    "\n",
    "X = DataFrames.DataFrame(x1=x1, x2=x2, x3=x3)\n",
    "first(X, 3) |> pretty"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also prepare the train and test split which will be useful later on."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "test, train = partition(eachindex(y), 0.8);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining a learning network\n",
    "\n",
    "In MLJ, a *learning network* is a directed acyclic graph (DAG) whose *nodes* apply trained or untrained operations such as a `predict` or `transform` (trained) or `+`, `vcat` etc. (untrained).\n",
    "Learning networks can be seen as pipelines on steroids.\n",
    "\n",
    "Let's consider the following simple DAG:\n",
    "\n",
    "![Operation DAG](/assets/diagrams/composite1.svg)\n",
    "\n",
    "It corresponds to a fairly standard regression workflow: the data is standardized, the target is transformed using a Box-Cox transformation, a ridge regression is applied and the result is converted back by inverting the transform.\n",
    "\n",
    "**Note**: actually  this DAG is simple enough that it could also have been done with a pipeline.\n",
    "\n",
    "### Sources and nodes\n",
    "\n",
    "In MLJ a learning network starts at **source** nodes and flows through nodes (`X` and `y`) defining operations/transformations (`W`, `z`, `ẑ`, `ŷ`).\n",
    "To define the source nodes, use the `source` function, you should specify whether it's a target:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xs = source(X)\n",
    "ys = source(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To define an \"trained-operation\" node, you must simply create a machine wrapping a model and another node (the data) and indicate which operation should be performed (e.g. `transform`):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "stand = machine(Standardizer(), Xs)\n",
    "W = transform(stand, Xs)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can `fit!` a trained-operation node at any point, MLJ will fit whatever it needs that is upstream of that node.\n",
    "In this case, there is just a source node upstream of `W` so fitting `W` will just fit the standardizer:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(W, rows=train);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to get the transformed data, you can then call the node speciying on which part of the data the operation should be performed:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "W()             # transforms all data\n",
    "W(rows=test, )  # transforms only test data\n",
    "W(X[3:4, :])    # transforms specific data"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now define the other nodes:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "box_model = UnivariateBoxCoxTransformer()\n",
    "box = machine(box_model, ys)\n",
    "z = transform(box, ys)\n",
    "\n",
    "ridge_model = Ridge(lambda=0.1)\n",
    "ridge = machine(ridge_model, W, z)\n",
    "ẑ = predict(ridge, W)\n",
    "\n",
    "ŷ = inverse_transform(box, ẑ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we have not yet done any training, but if we now call `fit!` on `ŷ`, it will fit all nodes upstream of `ŷ` that need to be re-trained:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(ŷ, rows=train);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that `ŷ` has been fitted, you can apply the full graph on test data (or any compatible data). For instance, let's get the `rms` between the ground truth and the predicted values:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rms(y[test], ŷ(rows=test))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modifying hyperparameters\n",
    "\n",
    "Hyperparameters can be accessed using the dot syntax as usual.\n",
    "Let's modify the regularisation parameter of the ridge regression:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ridge_model.lambda = 5.0;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the node `ẑ` corresponds to a machine that wraps `ridge_model`, that node has effectively changed and will be retrained:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(ŷ, rows=train)\n",
    "rms(y[test], ŷ(rows=test))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "kernelspec": {
   "name": "julia-1.7",
   "display_name": "Julia 1.7.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
