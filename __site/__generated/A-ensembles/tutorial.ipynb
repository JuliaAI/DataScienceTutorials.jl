{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/A-ensembles/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/A-ensembles/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "## Preliminary steps\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "Let's start by loading the relevant packages and generating some dummy data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "import DataFrames: DataFrame\n",
    "using StableRNGs\n",
    "\n",
    "rng = StableRNG(512)\n",
    "Xraw = rand(rng, 300, 3)\n",
    "y = exp.(Xraw[:,1] - Xraw[:,2] - 2Xraw[:,3] + 0.1*rand(rng, 300))\n",
    "X = DataFrame(Xraw, :auto)\n",
    "\n",
    "train, test = partition(eachindex(y), 0.7);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also load a simple model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "KNNRegressor = @load KNNRegressor\n",
    "knn_model = KNNRegressor(K=10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As before, let's instantiate a machine that wraps the model and data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "knn = machine(knn_model, X, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and fit it"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(knn, rows=train)\n",
    "ŷ = predict(knn, X[test, :]) # or use rows=test\n",
    "l2(ŷ, y[test]) # sum of squares loss"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The workflow above is equivalent to just calling `evaluate`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "evaluate(\n",
    "    knn_model,\n",
    "    X,\n",
    "    y;\n",
    "    resampling=Holdout(fraction_train=0.7, rng=StableRNG(666)),\n",
    "    measure=rms,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## Homogenous ensembles\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "MLJ offers basic support for ensembling such as\n",
    "[_bagging_](https://en.wikipedia.org/wiki/Bootstrap_aggregating).  Defining such an\n",
    "ensemble of simple \"atomic\" models is done via the `EnsembleModel` constructor:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ensemble_model = EnsembleModel(model=knn_model, n=20);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "where the `n=20` indicates how many models are present in the ensemble."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "### Training and testing an ensemble\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "Now that we've instantiated an ensemble, it can be trained and tested the same as any\n",
    "other model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "estimates = evaluate(ensemble_model, X, y, resampling=CV())\n",
    "estimates"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "here the implicit measure is the sum of squares loss (default for regressions). The\n",
    "`measurement` is the mean taken over the folds:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show estimates.measurement[1]\n",
    "@show mean(estimates.per_fold[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that multiple measures can be specified jointly. Here only one measure is\n",
    "(implicitly) specified but we still have to select the corresponding results (whence the\n",
    "`[1]` for both the `measurement` and `per_fold`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Systematic tuning\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "Let's simultaneously tune the ensemble's `bagging_fraction` and the K-Nearest neighbour\n",
    "hyperparameter `K`. Since one of our models is a field of the other, we have nested\n",
    "hyperparameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ensemble_model"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To define a tuning grid, we construct ranges for the two parameters and collate these\n",
    "ranges:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "B_range = range(\n",
    "    ensemble_model,\n",
    "    :bagging_fraction,\n",
    "    lower=0.5,\n",
    "    upper=1.0,)\n",
    "\n",
    "K_range = range(\n",
    "    ensemble_model,\n",
    "    :(model.K),\n",
    "    lower=1,\n",
    "    upper=20,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The scale for a tuning grid is linear by default but can be specified to `:log10` for\n",
    "logarithmic ranges.  Now we have to define a `TunedModel` and fit it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tm = TunedModel(\n",
    "    model=ensemble_model,\n",
    "    tuning=Grid(resolution=10), # 10x10 grid\n",
    "    resampling=Holdout(fraction_train=0.8, rng=StableRNG(42)),\n",
    "    ranges=[B_range, K_range],\n",
    ")\n",
    "\n",
    "tuned_ensemble = machine(tm, X, y)\n",
    "fit!(tuned_ensemble, rows=train);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note the `rng=42` seeds the random number generator for reproducibility of this example."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Reporting results\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "The best model can be accessed like so:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "best_ensemble = fitted_params(tuned_ensemble).best_model\n",
    "@show best_ensemble.model.K\n",
    "@show best_ensemble.bagging_fraction"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `report` method gives more detailed information on the tuning process:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = report(tuned_ensemble)\n",
    "keys(r)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For instance, `r.plotting` contains details about the optimization you might use in a\n",
    "plot:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r.plotting"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although for that we can also use a built-in plot recipe for `TunedModel`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Plots\n",
    "plot(tuned_ensemble)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Hyperparameter tuning}{A-ensembles-plot.svg}\n",
    "\n",
    "Finally you can always just evaluate the model by reporting `l2` on the test set:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(tuned_ensemble, rows=test)\n",
    "@show l2(ŷ, y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
