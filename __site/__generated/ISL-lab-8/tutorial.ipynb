{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/ISL-lab-8/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/ISL-lab-8/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting started"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "import RDatasets: dataset\n",
    "using PrettyPrinting\n",
    "import DataFrames: DataFrame, select, Not\n",
    "DTC = @load DecisionTreeClassifier pkg=DecisionTree\n",
    "\n",
    "carseats = dataset(\"ISLR\", \"Carseats\")\n",
    "\n",
    "first(carseats, 3) |> pretty"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We encode a new variable `High` based on whether the sales are higher or lower than 8 and add that column to the dataframe:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "High = ifelse.(carseats.Sales .<= 8, \"No\", \"Yes\") |> categorical;\n",
    "carseats[!, :High] = High;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now train a basic decision tree classifier for `High` given the other features after one-hot-encoding the categorical features:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X = select(carseats, Not([:Sales, :High]))\n",
    "y = carseats.High;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Tree Classifier"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "HotTreeClf = OneHotEncoder() |> DTC()\n",
    "\n",
    "mdl = HotTreeClf\n",
    "mach = machine(mdl, X, y)\n",
    "fit!(mach);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note `|>` is syntactic sugar for creating a `Pipeline` model from component model instances or model types.\n",
    "Note also that the machine `mach` is trained on the whole data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ypred = predict_mode(mach, X)\n",
    "misclassification_rate(ypred, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's right... it gets it perfectly; this tends to be classic behaviour for a DTC to overfit the data it's trained on.\n",
    "Let's see if it generalises:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.5, shuffle=true, rng=333)\n",
    "fit!(mach, rows=train)\n",
    "ypred = predict_mode(mach, rows=test)\n",
    "misclassification_rate(ypred, y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not really...\n",
    "\n",
    "### Tuning a DTC\n",
    "\n",
    "Let's try to do a bit of tuning"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r_mpi = range(mdl, :(decision_tree_classifier.max_depth), lower=1, upper=10)\n",
    "r_msl = range(mdl, :(decision_tree_classifier.min_samples_leaf), lower=1, upper=50)\n",
    "\n",
    "tm = TunedModel(\n",
    "    model=mdl,\n",
    "    ranges=[r_mpi, r_msl],\n",
    "    tuning=Grid(resolution=8),\n",
    "    resampling=CV(nfolds=5, rng=112),\n",
    "    operation=predict_mode,\n",
    "    measure=misclassification_rate\n",
    ")\n",
    "mtm = machine(tm, X, y)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "ypred = predict_mode(mtm, rows=test)\n",
    "misclassification_rate(ypred, y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can inspect the parameters of the best model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fitted_params(mtm).best_model.decision_tree_classifier"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Tree Regressor"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "DTR = @load DecisionTreeRegressor pkg=DecisionTree\n",
    "\n",
    "boston = dataset(\"MASS\", \"Boston\")\n",
    "\n",
    "y, X = unpack(boston, ==(:MedV))\n",
    "\n",
    "train, test = partition(eachindex(y), 0.5, shuffle=true, rng=551);\n",
    "\n",
    "scitype(X)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's recode the Count as Continuous and then fit a DTR"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X = coerce(X, autotype(X, rules=(:discrete_to_continuous,)))\n",
    "\n",
    "dtr_model = DTR()\n",
    "dtr = machine(dtr_model, X, y)\n",
    "\n",
    "fit!(dtr, rows=train)\n",
    "\n",
    "ypred = MLJ.predict(dtr, rows=test)\n",
    "round(rms(ypred, y[test]), sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again we can try tuning this a bit, since it's the same idea as before, let's just try to adjust the depth of the tree:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r_depth = range(dtr_model, :max_depth, lower=2, upper=20)\n",
    "\n",
    "tm = TunedModel(\n",
    "    model=dtr_model,\n",
    "    ranges=[r_depth],\n",
    "    tuning=Grid(resolution=10),\n",
    "    resampling=CV(nfolds=5, rng=1254),\n",
    "    measure=rms\n",
    ")\n",
    "\n",
    "mtm = machine(tm, X, y)\n",
    "\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "ypred = MLJ.predict(mtm, rows=test)\n",
    "round(rms(ypred, y[test]), sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest\n",
    "\n",
    "**Note**: the package [`DecisionTree.jl`](https://github.com/bensadeghi/DecisionTree.jl) also has a RandomForest model but it is not yet interfaced with in MLJ."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RFR = @load RandomForestRegressor pkg=ScikitLearn\n",
    "\n",
    "rf_mdl = RFR()\n",
    "rf = machine(rf_mdl, X, y)\n",
    "fit!(rf, rows=train)\n",
    "\n",
    "ypred = MLJ.predict(rf, rows=test)\n",
    "round(rms(ypred, y[test]), sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Boosting Machine"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "XGBR = @load XGBoostRegressor\n",
    "\n",
    "xgb_mdl = XGBR(num_round=10, max_depth=10)\n",
    "xgb = machine(xgb_mdl, X, y)\n",
    "fit!(xgb, rows=train)\n",
    "\n",
    "ypred = MLJ.predict(xgb, rows=test)\n",
    "round(rms(ypred, y[test]), sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again we could do some tuning for this."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "kernelspec": {
   "name": "julia-1.7",
   "display_name": "Julia 1.7.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
