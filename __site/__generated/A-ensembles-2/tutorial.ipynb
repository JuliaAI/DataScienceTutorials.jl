{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/A-ensembles-2/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/A-ensembles-2/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "## Prelims\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "This tutorial builds upon the previous ensemble tutorial with a home-made Random Forest regressor on the \"boston\" dataset."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "using PrettyPrinting\n",
    "using StableRNGs\n",
    "import DataFrames: DataFrame, describe\n",
    "\n",
    "X, y = @load_boston\n",
    "sch = schema(X)\n",
    "p = length(sch.names)\n",
    "describe(y)  # From DataFrames"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load the decision tree regressor"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "DecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's first check the performances of just a single Decision Tree Regressor (DTR for short):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tree = machine(DecisionTreeRegressor(), X, y)\n",
    "e = evaluate!(tree, resampling=Holdout(fraction_train=0.8),\n",
    "              measure=[rms, rmslp1])\n",
    "e"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that multiple measures can be reported simultaneously."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## Random forest\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "Let's create an ensemble of DTR and fix the number of subfeatures to 3 for now."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "forest = EnsembleModel(model=DecisionTreeRegressor())\n",
    "forest.model.n_subfeatures = 3"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "(**NB**: we could have fixed `n_subfeatures` in the DTR constructor too).\n",
    "\n",
    "To get an idea of how many trees are needed, we can follow the evaluation of the error (say the `rms`) for an increasing number of tree over several sampling round."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rng = StableRNG(5123) # for reproducibility\n",
    "m = machine(forest, X, y)\n",
    "r = range(forest, :n, lower=10, upper=1000)\n",
    "curves = learning_curve(m, resampling=Holdout(fraction_train=0.8, rng=rng),\n",
    "                         range=r, measure=rms);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "let's plot the curves"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Plots\n",
    "\n",
    "plot(curves.parameter_values, curves.measurements,\n",
    "xticks = [10, 100, 250, 500, 750, 1000],\n",
    "size=(800,600), linewidth=2, legend=false)\n",
    "xlabel!(\"Number of trees\")\n",
    "ylabel!(\"Root Mean Squared error\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{RMS vs number of trees}{A-ensembles-2-curves.svg}\n",
    "\n",
    "Let's go for 150 trees"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "forest.n = 150;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "### Tuning\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "As `forest` is a composite model, it has nested hyperparameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "params(forest) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define a range for the number of subfeatures and for the bagging fraction:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r_sf = range(forest, :(model.n_subfeatures), lower=1, upper=12)\n",
    "r_bf = range(forest, :bagging_fraction, lower=0.4, upper=1.0);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And build a tuned model as usual that we fit on a 80/20 split.\n",
    "We use a low-resolution grid here to make this tutorial faster but you could of course use a finer grid."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tuned_forest = TunedModel(model=forest,\n",
    "                          tuning=Grid(resolution=3),\n",
    "                          resampling=CV(nfolds=6, rng=StableRNG(32)),\n",
    "                          ranges=[r_sf, r_bf],\n",
    "                          measure=rms)\n",
    "m = machine(tuned_forest, X, y)\n",
    "e = evaluate!(m, resampling=Holdout(fraction_train=0.8),\n",
    "              measure=[rms, rmslp1])\n",
    "e"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Reporting\n",
    "@@\n",
    "@@dropdown-content\n",
    "Again, we can visualize the results from the hyperparameter search"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(m)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\fig{A-ensembles-2-heatmap.svg}\n",
    "\n",
    "Even though we've only done a very rough search, it seems that around 7 sub-features and a bagging fraction of around `0.75` work well.\n",
    "\n",
    "Now that the machine `m` is trained, you can use use it for predictions (implicitly, this will use the best model).\n",
    "For instance we could look at predictions on the whole dataset:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(m, X)\n",
    "@show rms(ŷ, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
