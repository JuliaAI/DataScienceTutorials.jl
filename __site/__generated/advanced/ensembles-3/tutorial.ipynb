{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/advanced/ensembles-3/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/advanced/ensembles-3/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Illustration of learning networks to create homogeneous ensemble using learning\n",
    "networks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Learning networks are an advanced MLJ feature which are covered in detail, with\n",
    "examples, in the [Learning\n",
    "networks](https://alan-turing-institute.github.io/MLJ.jl/dev/learning_networks/) section\n",
    "of the manual. In the \"Ensemble\" and \"Ensemble (2)\" tutorials it is shown how to create\n",
    "and apply homogeneous ensembles using MLJ's built-in `EnsembleModel` wrapper. To provide\n",
    "a simple illustration of learning networks we show how a user could build their own\n",
    "ensemble wrapper. We simplify the illustration by excluding bagging, which means all\n",
    "randomness has to be generated by the atomic models themselves (e.g., by the random\n",
    "selection of features in each split of a decision tree)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a more advanced illustration, see the \"Stacking\" tutorial."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some familiarity with the early parts of [Learning networks by\n",
    "example](https://alan-turing-institute.github.io/MLJ.jl/dev/learning_networks/#Learning-networks-by-example)\n",
    "will be helpful, but is not essential."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "## Definition of composite model type\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "import Statistics"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We load a model type we might want to use as an atomic model in our ensemble, and\n",
    "instantiate a default instance:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "DecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\n",
    "atom = DecisionTreeRegressor()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll be able to change this later on if we want."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The standard workflow for defining a new composite model type using learning networks is\n",
    "in two stages:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Define and test a learning network using some small test data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. \"Export\" the network as a new stand-alone model type, unattached to any data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's a small data set we can use for step 1:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X = (; x=rand(5))\n",
    "y = rand(5)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a warm-up exercise, we'll suppose we have only two models in the ensemble.  We start\n",
    "by wrapping the input data in source nodes. These nodes will be interface points for new\n",
    "training data when we `fit!` our new ensemble model type; `Xs` will also be an interface\n",
    "point for production data when we call `predict` on our new ensemble model type."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xs = source(X)\n",
    "ys = source(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are two distinct machines (for learning distinct trees) that share the same atomic\n",
    "model (hyperparameters):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach1 = machine(atom, Xs, ys)\n",
    "mach2 = machine(atom, Xs, ys)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are prediction nodes:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y1 = predict(mach1, Xs)\n",
    "y2 = predict(mach2, Xs)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It happens that `mean` immediately works on vectors of nodes, because `+` and division\n",
    "by a scalar works for nodes:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "yhat = mean([y1, y2])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test the network:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(yhat)\n",
    "Xnew = (; x=rand(2))\n",
    "yhat(Xnew)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great. No issues. Here's how we have an ensemble of any size:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 10\n",
    "machines = (machine(atom, Xs, ys) for i in 1:n)\n",
    "ys = [predict(m, Xs) for  m in machines]\n",
    "yhat = mean(ys);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can go ahead and test the modified network as before."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a struct for our new ensemble type:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mutable struct MyEnsemble <: DeterministicNetworkComposite\n",
    "    atom\n",
    "    n::Int64\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note carefully the supertype `DeterministicNetworkComposite`, which we are using because our\n",
    "atomic model will always be `Deterministic` predictors, and we are exporting a learning\n",
    "network to make a new composite model. Refer to documentation for other options here."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we wrap our learning network in a `prefit` method. In\n",
    "this case we leave out the test data, and substitute the actual `atom` we used with a\n",
    "symbolic \"placeholder\", with the name of the corresponding model field, in this case\n",
    "`:atom`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import MLJ.MLJBase.prefit\n",
    "function prefit(ensemble::MyEnsemble, verbosity, X, y)\n",
    "\n",
    "    Xs = source(X)\n",
    "    ys = source(y)\n",
    "\n",
    "    n = ensemble.n\n",
    "    machines = (machine(:atom, Xs, ys) for i in 1:n)\n",
    "    ys = [predict(m, Xs) for  m in machines]\n",
    "    yhat = mean(ys)\n",
    "\n",
    "    return (predict=yhat,)\n",
    "\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "â€Ž\n",
    "@@\n",
    "@@dropdown\n",
    "## Application to data\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X, y = @load_boston;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's a learning curve for the `min_samples_split` parameter of a *single* tree:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = range(\n",
    "    atom,\n",
    "    :min_samples_split,\n",
    "    lower=2,\n",
    "    upper=100,\n",
    "    scale=:log,\n",
    ")\n",
    "\n",
    "mach = machine(atom, X, y)\n",
    "\n",
    "curve = learning_curve(\n",
    "    mach,\n",
    "    range=r,\n",
    "    measure=mav,\n",
    "    resampling=CV(nfolds=6),\n",
    "    verbosity=0,\n",
    ")\n",
    "\n",
    "using Plots\n",
    "plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(curve.parameter_name)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\fig{e1.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll now generate a similar curve for a 100-tree ensemble of tree but this time we'll\n",
    "make sure to make the atom random:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "atom_rand = DecisionTreeRegressor(n_subfeatures=4)\n",
    "forest = MyEnsemble(atom_rand, 100)\n",
    "\n",
    "r = range(\n",
    "    forest,\n",
    "    :(atom.min_samples_split),\n",
    "    lower=2,\n",
    "    upper=100,\n",
    "    scale=:log,\n",
    ")\n",
    "\n",
    "mach = machine(forest, X, y)\n",
    "\n",
    "curve = learning_curve(\n",
    "    mach,\n",
    "    range=r,\n",
    "    measure=mav,\n",
    "    resampling=CV(nfolds=6),\n",
    "    verbosity=0,\n",
    "    acceleration_grid=CPUThreads(),\n",
    ")\n",
    "\n",
    "plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(curve.parameter_name)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\fig{e2}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "â€Ž\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
