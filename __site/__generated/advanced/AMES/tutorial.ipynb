{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/advanced/AMES/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/advanced/AMES/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build a model for the Ames House Price data set using a simple learning network to blend\n",
    "their predictions of two regressors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "## Baby steps\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load a reduced version of the well-known Ames House Price data set (containing six\n",
    "of the more important categorical features and six of the more important numerical\n",
    "features).  The dataset can be loaded directly with `@load_ames` and the reduced version\n",
    "via `@load_reduced_ames`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "import DataFrames: DataFrame\n",
    "import Statistics\n",
    "\n",
    "X, y = @load_reduced_ames\n",
    "X = DataFrame(X)\n",
    "@show size(X)\n",
    "first(X, 3)\n",
    "\n",
    "schema(X)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The target is a continuous vector:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show y[1:3]\n",
    "scitype(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "So this is a standard regression problem with a mix of categorical and continuous input."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## Dummy model\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember that a \"model\" in MLJ is just a container for hyperparameters; let's take a\n",
    "particularly simple one: constant regression."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "creg = ConstantRegressor()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wrapping the model in data creates a *machine* which will store training outcomes\n",
    "(*fit-results*)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(creg, X, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can now train the machine specifying the data it should be trained on (if\n",
    "unspecified, all the data will be used);"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(collect(eachindex(y)), 0.70, shuffle=true); # 70:30 split\n",
    "fit!(mach, rows=train)\n",
    "ŷ = predict(mach, rows=test);\n",
    "ŷ[1:3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observe that the output is probabilistic, each element is a univariate normal\n",
    "distribution (with the same mean and variance as it's a constant model).\n",
    "\n",
    "You can recover deterministic output by either computing the mean of predictions or\n",
    "using `predict_mean` directly (the `mean` function can be applied to any distribution\n",
    "from [`Distributions.jl`](https://github.com/JuliaStats/Distributions.jl)):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict_mean(mach, rows=test)\n",
    "ŷ[1:3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can then call any loss function from\n",
    "[StatisticalMeasures.jl](https://juliaai.github.io/StatisticalMeasures.jl/dev/) to\n",
    "assess the quality of the model by comparing the performances on the test set:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rmsl(ŷ, y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## KNN-Ridge blend\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try something a bit fancier than a constant regressor.\n",
    "\n",
    "* one-hot-encode categorical inputs\n",
    "* log-transform the target\n",
    "* fit both a KNN regression and a Ridge regression on the data\n",
    "* Compute a weighted average of individual model predictions\n",
    "* inverse transform (exponentiate) the blended prediction\n",
    "\n",
    "We are going to combine all this into a single new stand-alone composite model type,\n",
    "which will start by building and testing a [learning\n",
    "network](https://alan-turing-institute.github.io/MLJ.jl/dev/learning_networks/#Learning-Networks)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RidgeRegressor = @load RidgeRegressor pkg=\"MultivariateStats\"\n",
    "KNNRegressor = @load KNNRegressor"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "### A learning network\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by defining the source nodes of the network, which will wrap our data. Here\n",
    "we are including data only for testing purposes. Later when we \"export\" our functioning\n",
    "network, we'll remove reference to the data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xs = source(X)\n",
    "ys = source(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our first \"layer\", there's one-hot encoder and a log transformer; these will\n",
    "respectively lead to new nodes `W` and `z`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "hot = machine(OneHotEncoder(), Xs)\n",
    "\n",
    "W = transform(hot, Xs)\n",
    "z = log(ys)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the second \"layer\", there's a KNN regressor and a ridge regressor, these lead to nodes\n",
    "`ẑ₁` and `ẑ₂`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "knn   = machine(KNNRegressor(K=5), W, z)\n",
    "ridge = machine(RidgeRegressor(lambda=2.5), W, z)\n",
    "\n",
    "ẑ₁ = predict(knn, W)\n",
    "ẑ₂ = predict(ridge, W)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the third \"layer\", there's a weighted combination of the two regression models:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ẑ = 0.3ẑ₁ + 0.7ẑ₂;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And finally we need to invert the initial transformation of the target (which was a log):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = exp(ẑ);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You've now defined the learning network we need, which we test like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(ŷ, rows=train);\n",
    "preds = ŷ(rows=test);\n",
    "rmsl(preds, y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "While that's essentially all we need to solve our problem, we'll go one step further,\n",
    "exporting our learning network as a stand-alone model type we can apply to any data set,\n",
    "and treat like any other type. In particular, this will make tuning the (nested) model\n",
    "hyperparameters easier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Exporting the learning network\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's the struct for our new model type. Notice it has other models as hyperparameters."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mutable struct BlendedRegressor <: DeterministicNetworkComposite\n",
    "    knn_model\n",
    "    ridge_model\n",
    "    knn_weight::Float64\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note the supertype `DeterministicNetworkComposite` here, which we are using because our\n",
    "composite model will always make deterministic predictions, and because we are exporting\n",
    "a learning network to make our new composite model. Refer to documentation for other\n",
    "options here."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The other step we need is to wrap our learning network in a `prefit` definition,\n",
    "substituting the component models we used with symbol \"placeholders\" with names\n",
    "corresponding to fields of our new struct. We'll also use the `knn_weight` field of our\n",
    "struct to set the mix, instead of hard-coding it as we did above."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import MLJ.MLJBase.prefit\n",
    "function prefit(model::BlendedRegressor, verbosity, X, y)\n",
    "    Xs = source(X)\n",
    "    ys = source(y)\n",
    "\n",
    "    hot = machine(OneHotEncoder(), Xs)\n",
    "    W = transform(hot, Xs)\n",
    "\n",
    "    z = log(ys)\n",
    "\n",
    "    knn = machine(:knn_model, W, z)\n",
    "    ridge = machine(:ridge_model, W, z)\n",
    "    ẑ = model.knn_weight * predict(knn, W) + (1.0 - model.knn_weight) * predict(ridge, W)\n",
    "\n",
    "    ŷ = exp(ẑ)\n",
    "\n",
    "    (predict=ŷ,)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now instantiate and fit such a model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "blended = BlendedRegressor(KNNRegressor(K=5), RidgeRegressor(lambda=2.5), 0.3)\n",
    "mach = machine(blended, X, y)\n",
    "fit!(mach, rows=train)\n",
    "\n",
    "preds = predict(mach, rows=test)\n",
    "rmsl(preds, y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Tuning the blended model\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we get started, it's important to note that the hyperparameters of the model have\n",
    "different levels of *nesting*. This becomes explicit when trying to access elements:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show blended.knn_weight\n",
    "@show blended.knn_model.K\n",
    "@show blended.ridge_model.lambda"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see what names to use here from the way the model instance is displayed:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "blended"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The range of values to do your hyperparameter tuning over should follow the nesting\n",
    "structure reflected by `params`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "k_range = range(blended, :(knn_model.K), lower=2, upper=100, scale=:log10)\n",
    "l_range = range(blended, :(ridge_model.lambda), lower=1e-4, upper=10, scale=:log10)\n",
    "w_range = range(blended, :(knn_weight), lower=0.1, upper=0.9)\n",
    "\n",
    "ranges = [k_range, l_range, w_range]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now there remains to define how the tuning should be done. Let's just specify a coarse\n",
    "grid tuning with cross validation and instantiate a tuned model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tuned_blended = TunedModel(\n",
    "    blended;\n",
    "    tuning=Grid(resolution=7),\n",
    "    resampling=CV(nfolds=6),\n",
    "    ranges,\n",
    "    measure=rmsl,\n",
    "    acceleration=CPUThreads(),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For more tuning options, see [the\n",
    "docs](https://alan-turing-institute.github.io/MLJ.jl/dev/tuning_models/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now `tuned_blended` is a \"self-tuning\" version of the original model, with all the\n",
    "necessary resampling occurring under the hood. You can think of wrapping a model in\n",
    "`TunedModel` as moving the tuned hyperparameters to *learned* parameters."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(tuned_blended, X, y)\n",
    "fit!(mach, rows=train);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To retrieve the best model, you can use:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "blended_best = fitted_params(mach).best_model\n",
    "@show blended_best.knn_model.K\n",
    "@show blended_best.ridge_model.lambda\n",
    "@show blended_best.knn_weight"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "you can also use `mach` to make predictions (which will be done using the best model,\n",
    "trained on *all* the `train` data):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "preds = predict(mach, rows=test)\n",
    "rmsl(y[test], preds)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
