{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-boston-lgbm/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-boston-lgbm/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Main author**: Yaqub Alwan (IQVIA)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "## Getting started\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "import DataFrames\n",
    "import Statistics\n",
    "import StableRNGs.StableRNG\n",
    "\n",
    "LGBMRegressor = @load LGBMRegressor pkg=LightGBM"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us try LightGBM out by doing a regression task on the Boston house prices dataset.\n",
    "This is a commonly used dataset so there is a loader built into MLJ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, the objective is to show how LightGBM can do better than a Linear Regressor\n",
    "with minimal effort.\n",
    "\n",
    "We start out by taking a quick peek at the data itself and its statistical properties."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "features, targets = @load_boston\n",
    "features = DataFrames.DataFrame(features);\n",
    "@show size(features)\n",
    "@show targets[1:3]\n",
    "first(features, 3)\n",
    "\n",
    "schema(features)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also describe the dataframe"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "DataFrames.describe(features)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the usual train/test partitioning. This is important so we can estimate\n",
    "generalisation."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(targets), 0.70, rng=StableRNG(52))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us investigate some of the commonly tweaked LightGBM parameters. We start with\n",
    "looking at a learning curve for number of boostings."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lgb = LGBMRegressor() #initialised a model with default params\n",
    "mach = machine(lgb, features[train, :], targets[train, 1])\n",
    "curve = learning_curve(\n",
    "    mach,\n",
    "    resampling=CV(nfolds=5),\n",
    "    range=range(lgb, :num_iterations, lower=2, upper=500),\n",
    "    resolution=60,\n",
    "    measure=rms,\n",
    ")\n",
    "\n",
    "using Plots\n",
    "dims = (600, 370)\n",
    "plt = plot(curve.parameter_values, curve.measurements, size=dims)\n",
    "xlabel!(\"Number of rounds\", fontsize=14)\n",
    "ylabel!(\"RMSE\", fontsize=14)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\fig{lgbm_hp1.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It looks like that we don't need to go much past 100 boosts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since LightGBM is a gradient based learning method, we also have a learning rate\n",
    "parameter which controls the size of gradient updates."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us look at a learning curve for this parameter too"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lgb = LGBMRegressor() #initialised a model with default params\n",
    "mach = machine(lgb, features[train, :], targets[train, 1])\n",
    "\n",
    "curve = learning_curve(\n",
    "    mach,\n",
    "    resampling=CV(nfolds=5),\n",
    "    range=range(lgb, :learning_rate, lower=1e-3, upper=1, scale=:log),\n",
    "    resolution=60,\n",
    "    measure=rms,\n",
    ")\n",
    "\n",
    "plot(\n",
    "    curve.parameter_values,\n",
    "    curve.measurements,\n",
    "    size=dims,\n",
    "    xscale =:log10,\n",
    ")\n",
    "xlabel!(\"Learning rate (log scale)\", fontsize=14)\n",
    "ylabel!(\"RMSE\", fontsize=14)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\fig{lgbm_hp2.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like near 0.5 is a reasonable place. Bearing in mind that for lower values of\n",
    "learning rate we possibly require more boosting in order to converge, so the default\n",
    "value of 100 might not be sufficient for convergence. We leave this as an exercise to\n",
    "the reader.  We can still try to tune this parameter, however."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally let us check number of datapoints required to produce a leaf in an individual\n",
    "tree. This parameter controls the complexity of individual learner trees, and too low a\n",
    "value might lead to overfitting."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lgb = LGBMRegressor() #initialised a model with default params\n",
    "mach = machine(lgb, features[train, :], targets[train, 1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "dataset is small enough and the lower and upper sets the tree to have certain number of\n",
    "leaves"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "curve = learning_curve(\n",
    "    mach,\n",
    "    resampling=CV(nfolds=5),\n",
    "    range=range(lgb, :min_data_in_leaf, lower=1, upper=50),\n",
    "    measure=rms,\n",
    ")\n",
    "\n",
    "plot(curve.parameter_values, curve.measurements, size=dims)\n",
    "xlabel!(\"Min data in leaf\", fontsize=14)\n",
    "ylabel!(\"RMSE\", fontsize=14)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\fig{lgbm_hp3.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It does not seem like there is a huge risk for overfitting, and lower is better for this\n",
    "parameter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the learning curves above we can select some small-ish ranges to jointly search\n",
    "for the best combinations of these parameters via cross validation."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r1 = range(lgb, :num_iterations, lower=50, upper=100)\n",
    "r2 = range(lgb, :min_data_in_leaf, lower=2, upper=10)\n",
    "r3 = range(lgb, :learning_rate, lower=1e-1, upper=1e0)\n",
    "tuned_model = TunedModel(\n",
    "    lgb,\n",
    "    tuning=RandomSearch(),\n",
    "    resampling=CV(rng=StableRNG(123)),\n",
    "    ranges=[r1,r2,r3],\n",
    "    measure=rms,\n",
    "    n=100,\n",
    ")\n",
    "mach = machine(tuned_model, features, targets)\n",
    "fit!(mach, rows=train);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see what the cross validation best model parameters turned out to be?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "best_model = fitted_params(mach).best_model\n",
    "@show best_model.learning_rate\n",
    "@show best_model.min_data_in_leaf\n",
    "@show best_model.num_iterations"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great, and now let's predict using the held out data (predicting using the `TunedModel`\n",
    "machine uses the best model trained on all the `train` data):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "predictions = MLJ.predict(mach, rows=test)\n",
    "rms_score = round(rms(predictions, targets[test, 1]), sigdigits=4)\n",
    "\n",
    "@show rms_score"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "â€Ž\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
