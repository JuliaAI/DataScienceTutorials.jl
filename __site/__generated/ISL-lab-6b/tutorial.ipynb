{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/ISL-lab-6b/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/ISL-lab-6b/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "regression to the Hitters R dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "## Getting started\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "import RDatasets: dataset\n",
    "using PrettyPrinting\n",
    "import Distributions as D\n",
    "\n",
    "LinearRegressor = @load LinearRegressor pkg = MLJLinearModels\n",
    "RidgeRegressor = @load RidgeRegressor pkg = MLJLinearModels\n",
    "LassoRegressor = @load LassoRegressor pkg = MLJLinearModels"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We load the dataset using the `dataset` function, which takes the Package and\n",
    "dataset names as arguments."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "hitters = dataset(\"ISLR\", \"Hitters\")\n",
    "@show size(hitters)\n",
    "names(hitters) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's unpack the dataset with the `unpack` function.  In this case, the target is\n",
    "`Salary` (`==(:Salary)`); and all other columns are features, going into a table `X`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y, X = unpack(hitters, ==(:Salary));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The target has missing values which we will just ignore.\n",
    "We extract the row indices corresponding to non-missing values of the target.\n",
    "Note the use of the element-wise operator `.`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "no_miss = .!ismissing.(y);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We collect the non missing values of the target in an Array."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# And keep only the corresponding features values.\n",
    "\n",
    "y = collect(skipmissing(y))\n",
    "X = X[no_miss, :];\n",
    "\n",
    "# Let's now split our dataset into a train and test sets.\n",
    "train, test = partition(eachindex(y), 0.5, shuffle = true, rng = 424);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look at the target."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Plots\n",
    "\n",
    "plot(\n",
    "    y,\n",
    "    seriestype = :scatter,\n",
    "    markershape = :circle,\n",
    "    legend = false,\n",
    "    size = (800, 600),\n",
    ")\n",
    "\n",
    "xlabel!(\"Index\")\n",
    "ylabel!(\"Salary\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Salary}{ISL-lab-6-g1.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks quite skewed, let's have a look at a histogram:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "histogram(y, bins = 50, normalize = true, label = false, size = (800, 600))\n",
    "xlabel!(\"Salary\")\n",
    "ylabel!(\"Density\")\n",
    "\n",
    "edfit = D.fit_mle(D.Exponential, y)\n",
    "xx = range(minimum(y), 2500, length = 100)\n",
    "yy = pdf.(edfit, xx)\n",
    "plot!(\n",
    "    xx,\n",
    "    yy,\n",
    "    label = \"Exponential distribution fit\",\n",
    "    linecolor = :orange,\n",
    "    linewidth = 4,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Distribution of salary}{ISL-lab-6-g2.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "### Data preparation\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most features are currently encoded as integers but we will consider them as continuous.\n",
    "To coerce `int` features to `Float`, we nest the `autotype` function in the `coerce`\n",
    "function.  The `autotype` function returns a dictionary containing scientific types,\n",
    "which is then passed to the `coerce` function.  For more details on the use of\n",
    "`autotype`, see the [Scientific\n",
    "Types](https://alan-turing-institute.github.io/DataScienceTutorials.jl/data/scitype/index.html#autotype)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xc = coerce(X, autotype(X, rules = (:discrete_to_continuous,)))\n",
    "schema(Xc)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There're a few features that are categorical which we'll one-hot-encode."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## Ridge pipeline\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "### Baseline\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's first fit a simple pipeline with a standardizer, a one-hot-encoder and a basic\n",
    "linear regression:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Standardizer() |>  OneHotEncoder() |> LinearRegressor()\n",
    "\n",
    "pipe = machine(model, Xc, y)\n",
    "fit!(pipe, rows = train)\n",
    "ŷ = MLJ.predict(pipe, rows = test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits = 4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's get a feel for how we're doing"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "res = ŷ .- y[test]\n",
    "plot(\n",
    "    res,\n",
    "    line = :stem,\n",
    "    ylims = (-1300, 1000),\n",
    "    linewidth = 3,\n",
    "    marker = :circle,\n",
    "        legend = false,\n",
    "    size = ((800, 600)),\n",
    ")\n",
    "hline!([0], linewidth = 2, color = :red)\n",
    "xlabel!(\"Index\")\n",
    "ylabel!(\"Residual (ŷ - y)\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Residuals}{ISL-lab-6-g3.svg}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "histogram(\n",
    "    res,\n",
    "    bins = 30,\n",
    "    normalize = true,\n",
    "    color = :green,\n",
    "    label = false,\n",
    "    size = (800, 600),\n",
    "    xlims = (-1100, 1100),\n",
    ")\n",
    "\n",
    "xx    = range(-1100, 1100, length = 100)\n",
    "ndfit = D.fit_mle(D.Normal, res)\n",
    "lfit  = D.fit_mle(D.Laplace, res)\n",
    "\n",
    "plot!(xx, pdf.(ndfit, xx), linecolor = :orange, label = \"Normal fit\", linewidth = 3)\n",
    "plot!(xx, pdf.(lfit, xx), linecolor = :magenta, label = \"Laplace fit\", linewidth = 3)\n",
    "xlabel!(\"Residual (ŷ - y)\")\n",
    "ylabel!(\"Density\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Distribution of residuals}{ISL-lab-6-g4.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Basic Ridge\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now swap the linear regressor for a Ridge one without specifying the penalty (`1`\n",
    "by default): We modify the supervised model in the pipeline directly."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pipe.model.linear_regressor = RidgeRegressor()\n",
    "fit!(pipe, rows = train)\n",
    "ŷ = MLJ.predict(pipe, rows = test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits = 4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok that's a bit better but perhaps we can do better with an appropriate selection of the\n",
    "hyperparameter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Cross validating\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What penalty should you use? Let's do a simple CV to try to find out:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = range(\n",
    "    model,\n",
    "    :(linear_regressor.lambda),\n",
    "    lower = 1e-2,\n",
    "    upper = 100,\n",
    "    scale = :log10,\n",
    ")\n",
    "tm = TunedModel(\n",
    "    model,\n",
    "    ranges = r,\n",
    "    tuning = Grid(resolution = 50),\n",
    "    resampling = CV(nfolds = 3, rng = 4141),\n",
    "    measure = rms,\n",
    ")\n",
    "mtm = machine(tm, Xc, y)\n",
    "fit!(mtm, rows = train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "round(best_mdl.linear_regressor.lambda, sigdigits = 4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Right, and  with that we get:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = MLJ.predict(mtm, rows = test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits = 4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a poorer result, but that's not a complete surprise. To optimize `lambda` we've\n",
    "sacrificed some data (for the cross-validation), and we only have 263 observations\n",
    "total."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again visualizing the residuals:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "res = ŷ .- y[test]\n",
    "plot(\n",
    "    res,\n",
    "    line = :stem,\n",
    "    xlims = (1, length(res)),\n",
    "    ylims = (-1400, 1000),\n",
    "    linewidth = 3,\n",
    "    marker = :circle,\n",
    "    legend = false,\n",
    "    size = ((800, 600)),\n",
    ")\n",
    "hline!([0], linewidth = 2, color = :red)\n",
    "xlabel!(\"Index\")\n",
    "ylabel!(\"Residual (ŷ - y)\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Ridge residuals}{ISL-lab-6-g5.svg}\n",
    "\n",
    "You can compare that with the residuals obtained earlier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## Lasso pipeline\n",
    "@@\n",
    "@@dropdown-content\n",
    "\n",
    "Let's do the same as above but using a Lasso model (which also has a regularization\n",
    "parameter named `lambda`) but adjust the range a bit:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mtm.model.model.linear_regressor = LassoRegressor()\n",
    "mtm.model.range = range(\n",
    "    model,\n",
    "    :(linear_regressor.lambda),\n",
    "    lower = 5,\n",
    "    upper = 1000,\n",
    "    scale = :log10,\n",
    ")\n",
    "fit!(mtm, rows = train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "round(best_mdl.linear_regressor.lambda, sigdigits = 4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok and let's see how that does:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = MLJ.predict(mtm, rows = test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits = 4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "A pretty similar result to the previous one. Notice the parameters are reasonably\n",
    "sparse, as expected with an L1-regularizer:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "coefs, intercept = fitted_params(mtm.fitresult).linear_regressor\n",
    "@show coefs\n",
    "@show intercept"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "with around 50% sparsity:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "coef_vals = [c[2] for c in coefs]\n",
    "sum(coef_vals .≈ 0) / length(coefs)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualise this:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "name of the features including one-hot-encoded ones"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "all_names = [:AtBat, :Hits, :HmRun, :Runs, :RBI, :Walks, :Years,\n",
    "        :CAtBat, :CHits, :CHmRun, :CRuns, :CRBI, :CWalks,\n",
    "        :League__A, :League__N, :Div_E, :Div_W,\n",
    "        :PutOuts, :Assists, :Errors, :NewLeague_A, :NewLeague_N]\n",
    "\n",
    "idxshow = collect(1:length(coef_vals))[abs.(coef_vals).>0]\n",
    "\n",
    "plot(\n",
    "    coef_vals,\n",
    "    xticks = (idxshow, all_names),\n",
    "    legend = false,\n",
    "    xrotation = 90,\n",
    "    line = :stem,\n",
    "    marker = :circle,\n",
    "    size = ((800, 700)),\n",
    ")\n",
    "hline!([0], linewidth = 2, color = :red)\n",
    "ylabel!(\"Amplitude\")\n",
    "xlabel!(\"Coefficient\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Lasso coefficients}{ISL-lab-6-g6.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this baby dataset, simple ridge regression, with default hyperparameters, appears to\n",
    "work best. Of course, without a deeper analysis, we cannot say the differences observed\n",
    "are statistically significant. For this small data set, it's doubtful."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
