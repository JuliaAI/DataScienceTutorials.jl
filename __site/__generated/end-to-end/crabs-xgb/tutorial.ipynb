{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/end-to-end/crabs-xgb/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/end-to-end/crabs-xgb/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example is inspired from [this\n",
    "post](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "showing how to use XGBoost."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "## First steps\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "MLJ provides a built-in function to load the Crabs dataset:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "using StatsBase\n",
    "using Random\n",
    "using Plots\n",
    "import DataFrames\n",
    "import StableRNGs.StableRNG\n",
    "\n",
    "\n",
    "X, y = @load_crabs # a table and a vector\n",
    "X = DataFrames.DataFrame(X)\n",
    "@show size(X)\n",
    "@show y[1:3]\n",
    "first(X, 3)\n",
    "\n",
    "schema(X)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are looking at a classification problem with the following classes:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "levels(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's not a very big dataset so we will likely overfit it badly using something as\n",
    "sophisticated as XGBoost but it will do for a demonstration. Since our data set is\n",
    "ordered by target class, we'll be sure to create shuffled train/test index sets:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(collect(eachindex(y)), 0.70, rng=StableRNG(123))\n",
    "XGBC = @load XGBoostClassifier\n",
    "xgb_model = XGBC()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check whether the training and is balanced, `StatsBase.countmap` is useful for\n",
    "that:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "countmap(y[train])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "which is pretty balanced. You could check the same on the test set and full set and the\n",
    "same comment would still hold."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## XGBoost machine\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wrap a machine around an XGBoost model (XGB) and the data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xgb  = XGBC()\n",
    "mach = machine(xgb, X, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will tune it varying the number of rounds used and generate a learning curve"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = range(xgb, :num_round, lower=50, upper=500)\n",
    "curve = learning_curve(\n",
    "    mach,\n",
    "    range=r,\n",
    "    resolution=50,\n",
    "    measure=brier_loss,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(\"Number of rounds\", fontsize=14)\n",
    "ylabel!(\"Brier loss\", fontsize=14)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Brier loss vs Num Round}{EX-crabs-xgb-curve1.svg}\n",
    "\n",
    "Not a lot of improvement after 300 rounds."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xgb.num_round = 300;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "### More tuning (1)\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now tune the maximum depth of each tree and the minimum child weight in the\n",
    "boosting."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r1 = range(xgb, :max_depth, lower=3, upper=10)\n",
    "r2 = range(xgb, :min_child_weight, lower=0, upper=5)\n",
    "\n",
    "tuned_model = TunedModel(\n",
    "    xgb,\n",
    "    tuning=Grid(resolution=8),\n",
    "    resampling=CV(rng=11),\n",
    "    ranges=[r1,r2],\n",
    "    measure=brier_loss,\n",
    ")\n",
    "mach = machine(tuned_model, X, y)\n",
    "fit!(mach, rows=train)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualize details about the tuning:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Hyperparameter tuningplot}{EX-crabs-xgb-tuningplot.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's extract the optimal model and inspect its parameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xgb = fitted_params(mach).best_model\n",
    "@show xgb.max_depth\n",
    "@show xgb.min_child_weight"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### More tuning (2)\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's examine the effect of `gamma`. This time we'll use a visual approach:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(xgb, X, y)\n",
    "curve = learning_curve(\n",
    "    mach,\n",
    "    range= range(xgb, :gamma, lower=0, upper=10),\n",
    "    resolution=30,\n",
    "    measure=brier_loss,\n",
    ");\n",
    "\n",
    "plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(\"gamma\", fontsize=14)\n",
    "ylabel!(\"Brier loss\", fontsize=14)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Tuning gamma}{EX-crabs-xgb-gamma.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following choice looks about optimal:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xgb.gamma = 3.8"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### More tuning (3)\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's next examine the effect of `subsample` and `colsample_bytree`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r1 = range(xgb, :subsample, lower=0.6, upper=1.0)\n",
    "r2 = range(xgb, :colsample_bytree, lower=0.6, upper=1.0)\n",
    "\n",
    "tuned_model = TunedModel(\n",
    "    xgb,\n",
    "    tuning=Grid(resolution=8),\n",
    "    resampling=CV(rng=234),\n",
    "    ranges=[r1,r2],\n",
    "    measure=brier_loss,\n",
    ")\n",
    "mach = machine(tuned_model, X, y)\n",
    "fit!(mach, rows=train)\n",
    "\n",
    "plot(mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Hyperparameter tuningplot}{EX-crabs-xgb-tuningplot2.svg}\n",
    "\n",
    "Let's retrieve the best models:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xgb = fitted_params(mach).best_model\n",
    "@show xgb.subsample\n",
    "@show xgb.colsample_bytree"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could continue with more fine tuning but given how small the dataset is, it doesn't\n",
    "make much sense.  How does it fare on the test set?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict_mode(mach, rows=test)\n",
    "round(accuracy(ŷ, y[test]), sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not too bad."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
