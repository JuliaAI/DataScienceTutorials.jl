{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the\n",
    "tutorial-specific package environment, using this\n",
    "[`Project.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/end-to-end/creditfraud/Project.toml) and\n",
    "[this `Manifest.toml`](https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/end-to-end/creditfraud/Manifest.toml), or by following\n",
    "[these](https://juliaai.github.io/DataScienceTutorials.jl/#learning_by_doing) detailed instructions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classification of fraudulent/not credit card transactions (imbalanced data)\n",
    "By Kristian Bjarnason. The original script can be found [here](https://github.com/kbjarnason/credit-card-fraud-classification)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Editor's note.** To reduce training times, we have reduced the the original number of\n",
    "data observations. To re-instate the full dataset (290k observations) change\n",
    "`reduction=0.05` to `reduction=1`. The data is highly imbalanced, and this is ignored\n",
    "when training some models. Some other changes to Bjarnason's original notebook are noted\n",
    "at the end."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Dates, Statistics, LinearAlgebra, Random # standard libraries\n",
    "using MLJ, Plots, DataFrames, UrlDownload\n",
    "using CSV # needed for `urldownload` to work\n",
    "import StatsBase # needed for `countmap`"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adjusting fontsize in plotting:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Plots.scalefontsizes(0.85)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "## Data Preparation\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Divide the sample into two equal sub-samples. Keep the proportion of frauds the same in\n",
    "each sub-sample (246 frauds in each).  Use one sub-sample to estimate (train) your\n",
    "models and the second one to evaluate the out-of-sample performance of each model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing the data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "table = urldownload(\n",
    "\"https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv\",\n",
    ");\n",
    "data = DataFrame(table)\n",
    "first(data, 4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspecting the scientific types of variables contained in the DataFrame:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "schema(data)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Time column is not relevant to our analysis, we drop it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "select!(data, Not(:Time));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And the target variable, `Class`, should not be interpretted by our algorithms as a\n",
    "`Count` variable. We'll view it as an *ordered* factor (i.e., binary data with an\n",
    "intrinsic `positive` class, corresponding here to `1`, the second in the lexigrahic\n",
    "ordering)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "coerce!(data, :Class => OrderedFactor);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can check by calling `schema` again, or like this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "scitype(data.Class)\n",
    "\n",
    "levels(data.Class) # second element is `positive` class"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's get a summary of the remaining data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "describe(data)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the `Amount` variable spans a wide range of values.  To reduce variation in\n",
    "the data, we take logs. Since some values are `0`, we first add `1e-6` to eavh value.\n",
    "We transform in place using '!':"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "data[!,:Amount] = log.(data[!,:Amount] .+ 1e-6);\n",
    "histogram(data.Amount)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\fig{EX-creditfraud-amount.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we unpack the dataframe and creating a separate frame `X` for input features\n",
    "(predictors) and vector `y` for the target variable. Because of class imbalance, we make\n",
    "the partition stratified, and we also dump some observations, to reduce training\n",
    "times. Change the next line to `reduction = 1` to keep all the data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "reduction = 0.05\n",
    "frac_train = 0.8*reduction\n",
    "frac_test = 0.2*reduction\n",
    "\n",
    "y, X = unpack(data, ==(:Class))\n",
    "(Xtrain, Xtest, _), (ytrain, ytest, _) =\n",
    "    partition((X, y), frac_train, frac_test; stratify=y, multi=true, rng=111);\n",
    "\n",
    "StatsBase.countmap(ytrain)\n",
    "\n",
    "StatsBase.countmap(ytest)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## Estimation of models\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will estimate of three different models:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. logit\n",
    "2. support vector machines\n",
    "3. neural network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "@@dropdown\n",
    "### Logit\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Initial logit classification with lambda = 1.0\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n",
    "model_logit = LogisticClassifier(lambda=1.0)\n",
    "mach = machine(model_logit, Xtrain, ytrain) |> fit!"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`LogisticClassifier` is a probabilistic predictor, i.e. for each observation in the\n",
    "sample it attaches a probability to each of the possible values of the target.  To\n",
    "recover a deterministic output, we use `predict_mode` instead of `predict`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "yhat_logit = predict_mode(mach, Xtest);\n",
    "first(yhat_logit, 4)\n",
    "\n",
    "# How does this model perform?\n",
    "\n",
    "confusion_matrix(yhat_logit, ytest)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To plot a receiver operator characteristic, we need the *probabilistic* predictions:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "yhat = predict(mach, Xtest);\n",
    "yhat[1:3]\n",
    "\n",
    "false_positive_rates, true_positive_rates, thresholds =\n",
    "    roc_curve(yhat, ytest)\n",
    "plot(false_positive_rates, true_positive_rates)\n",
    "plot!([0, 1], [0, 1], linewidth=2, linestyle=:dash, color=:black, label=:none)\n",
    "xlabel!(\"false positive rate\")\n",
    "ylabel!(\"true positive rate\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\fig{EX-creditfraud-roc.svg}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "misclassification_rate(yhat_logit, ytest)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks like it's not too bad. Let's see if we can do even better by doing a little\n",
    "tuning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Tuned logit\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Still LogisticClassifier but implementing hyperparameter tuning."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = range(model_logit, :lambda, lower=1e-6, upper=100, scale=:log)\n",
    "\n",
    "self_tuning_logit_model = TunedModel(\n",
    "    model_logit,\n",
    "    tuning = Grid(resolution=10),\n",
    "    resampling = CV(nfolds=3),\n",
    "    range = r,\n",
    "    measure = misclassification_rate,\n",
    ")\n",
    "\n",
    "mach = machine(self_tuning_logit_model, Xtrain, ytrain) |> fit!"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Predictions"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "yhat_logit_tuned = predict_mode(mach, Xtest);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at the misclassification_rate. It is, surprisingly, slightly higher\n",
    "than the one calculated for the non tuned model."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show misclassification_rate(yhat_logit_tuned, ytest)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is lower, although the difference may not be statistically significant."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Support Vector Machine\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Initial SVM classification with cost = 1.0:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SVC = @load SVC pkg = LIBSVM"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To fit the SVM, we declare a pipeline which comprises both a standardizer and the\n",
    "model. Training is substantially longer than for the preceding linear model (over 10\n",
    "minutes):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model_svm = Standardizer() |>  SVC()\n",
    "mach = machine(model_svm, Xtrain, ytrain) |> fit!\n",
    "yhat_svm = predict(mach, Xtest)\n",
    "confusion_matrix(yhat_svm, ytest)\n",
    "\n",
    "@show misclassification_rate(yhat_svm, ytest)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tuned SVM"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = range(model_svm, :(svc.cost), lower=0.1, upper=3.5, scale=:linear)\n",
    "self_tuning_svm_model = TunedModel(\n",
    "    model_svm,\n",
    "    resampling = CV(nfolds=3),\n",
    "    tuning = Grid(resolution=6),\n",
    "    range = r,\n",
    "    measure = misclassification_rate,\n",
    ")\n",
    "mach = machine(self_tuning_svm_model, Xtrain, ytrain) |> fit!\n",
    "\n",
    "fitted_params(mach).best_model\n",
    "\n",
    "plot(mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\fig{EX-creditfraud-tuned_svm.svg}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "yhat_svm_tuned = predict(mach, Xtest)\n",
    "confusion_matrix(yhat_svm_tuned, ytest)\n",
    "\n",
    "misclassification_rate(yhat_svm_tuned, ytest)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "### Neural Network\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We assume familiarity with the building blocks of Flux models. In MLJFlux, a *builder*\n",
    "is essentially a rule for creating a Flux chain, once the data has been inspected for\n",
    "size. See the [MLJFlux\n",
    "documentation](https://github.com/FluxML/MLJFlux.jl/blob/dev/README.md) for further\n",
    "details. We do note specify the softmax \"finalizer\" because MLJFlux classifiers add that\n",
    "under the hood."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import MLJFlux.@builder\n",
    "using Flux\n",
    "\n",
    "builder = @builder Chain(\n",
    "    Dense(n_in, 16, relu),\n",
    "    Dropout(0.1; rng=rng),\n",
    "    Dense(16, n_out),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the @builder macro call, `n_in`, `n_out`, and `rng` are replaced with the actual\n",
    "number of input features found in the data, the actual number of output classes, and the\n",
    "`rng` specified in the model hyperparameters (see below)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are now ready to specify the MLJFlux model. If you have running with GPU, you can try\n",
    "adding the option `acceleration=CUDALibs()`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rng = Xoshiro(123)\n",
    "model = NeuralNetworkClassifier(\n",
    "    ; builder,\n",
    "    loss=(yhat, y)->Flux.tversky_loss(yhat, y, β=0.9), # combines precision and recall\n",
    "    batch_size = round(Int, reduction*2048),\n",
    "    epochs=50,\n",
    "    rng,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although we have not paid attention to it so far (and probably should have) there is\n",
    "substantial class imbalance for our target:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "StatsBase.countmap(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will address this by wrapping our model in a SMOTE overampling strategy, using MLJ's\n",
    "`BalancedModel` wrapper. Here are options for oversampling:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "models(\"oversampler\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll use SMOTE:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SMOTE = @load SMOTE pkg=Imbalance\n",
    "balanced_model = BalancedModel(model, oversampler=SMOTE())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our final model adds standarization as a pre-processor:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model_nn = Standardizer() |> balanced_model\n",
    "\n",
    "mach = machine(model_nn, Xtrain, ytrain) |> fit!\n",
    "yhat_nn = predict_mode(mach, Xtest);\n",
    "confusion_matrix(yhat_nn, ytest)\n",
    "\n",
    "misclassification_rate(yhat_nn, ytest)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@\n",
    "@@dropdown\n",
    "## Editorial notes\n",
    "@@\n",
    "@@dropdown-content"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- In the original notebook the train-test-validation split was not stratified."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- The original raw Flux model has been replaced with an MLJFlux model, for a common\n",
    "- interface."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- MLJ's `BalancedModel` wrapper has been used to correct for class imbalance in the\n",
    "  MLJFlux model, using the SMOTE algorithm. Originally, naive oversampling was applied\n",
    "  in a separate pre-processing step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- In tuning the metric used for the objective function is always\n",
    "  `misclassification_rate`, for consistency."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "‎\n",
    "@@"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
