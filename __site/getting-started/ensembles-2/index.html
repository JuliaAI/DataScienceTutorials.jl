<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/DataScienceTutorials.jl/libs/highlight/github.min.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/franklin.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/pure.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/side-menu.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/extra.css"> <title>Ensemble models &#40;2&#41;</title> <script src="/DataScienceTutorials.jl/libs/lunr/lunr.min.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunr_index.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunrclient.min.js"></script> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/DataScienceTutorials.jl/" id=menu-logo-link > <div class=menu-logo > <p><strong>Data Science Tutorials</strong></p> </div> </a> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/DataScienceTutorials.jl/search/index.html" style="visibility:hidden"> </form> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/DataScienceTutorials.jl/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Data basics</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/loading/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Loading data</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/dataframe/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Data Frames</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/categorical/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/scitype/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Scientific Type</a> </ul> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/fit-and-predict/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/model-tuning/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/composing-models/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/stacking/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Stacking</a> </ul> <li class=pure-menu-sublist-title ><strong>Intro to Stats Learning</strong> <ul class=pure-menu-sublist  id=isl> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 2</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 3</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-4/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 4</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-5/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 5</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-6b/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 6b</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-8/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 8</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-9/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 9</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-10/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 10</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/AMES/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/wine/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/crabs-xgb/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/horse/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Horse</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/HouseKingCounty/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> King County Houses</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/airfoil" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Airfoil </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-lgbm" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/glm/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Using GLM.jl </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/powergen/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Power Generation </a> </ul> </ul> </div> </div> <div id=main > <div class=franklin-content ><h1 id=ensemble_models_2 ><a href="#ensemble_models_2">Ensemble models &#40;2&#41;</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/notebooks/A-ensembles-2.ipynb" target=_blank ><em>notebook</em></a>, <em>the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/A-ensembles-2-raw.jl" target=_blank ><em>raw script</em></a>, <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/A-ensembles-2.jl" target=_blank ><em>annotated script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=franklin-toc ><ol><li><a href="#prelims">Prelims</a><li><a href="#random_forest">Random forest</a><ol><li><a href="#tuning">Tuning</a><li><a href="#reporting">Reporting</a></ol></ol></div><h2 id=prelims ><a href="#prelims">Prelims</a></h2> <p>This tutorial builds upon the previous ensemble tutorial with a home-made Random Forest regressor on the &quot;boston&quot; dataset.</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLJ
<span class=hljs-keyword >using</span> PyPlot
<span class=hljs-keyword >using</span> PrettyPrinting
<span class=hljs-keyword >using</span> StableRNGs
<span class=hljs-keyword >import</span> DataFrames

X, y = <span class=hljs-meta >@load_boston</span>
sch = schema(X)
p = length(sch.names)
n = sch.nrows
<span class=hljs-meta >@show</span> (n, p)
DataFrames.describe(y)</code></pre><pre><code class="plaintext hljs">(n, p) = (506, 12)
Summary Stats:
Length:         506
Missing Count:  0
Mean:           22.532806
Minimum:        5.000000
1st Quartile:   17.025000
Median:         21.200000
3rd Quartile:   25.000000
Maximum:        50.000000
Type:           Float64
</code></pre> <p>Let&#39;s load the decision tree regressor</p> <pre><code class="julia hljs"><span class=hljs-meta >@load</span> DecisionTreeRegressor</code></pre><pre><code class="plaintext hljs">DecisionTreeRegressor(
    max_depth = -1,
    min_samples_leaf = 5,
    min_samples_split = 2,
    min_purity_increase = 0.0,
    n_subfeatures = 0,
    post_prune = false,
    merge_purity_threshold = 1.0) @ 1…81</code></pre> <p>Let&#39;s first check the performances of just a single Decision Tree Regressor &#40;DTR for short&#41;:</p> <pre><code class="julia hljs">tree = machine(DecisionTreeRegressor(), X, y)
<span class=hljs-literal >e</span> = evaluate!(tree, resampling=Holdout(fraction_train=<span class=hljs-number >0.8</span>),
              measure=[rms, rmslp1])
<span class=hljs-literal >e</span> |&gt; pprint <span class=hljs-comment ># use PrettyPrinting</span></code></pre><pre><code class="plaintext hljs">(measure = [rms, rmslp1],
 measurement = [7.062202405332688, 0.3277498771713516],
 per_fold = [[7.062202405332688], [0.3277498771713516]],
 per_observation = [missing, missing])</code></pre> <p>Note that multiple measures can be reported simultaneously.</p> <h2 id=random_forest ><a href="#random_forest">Random forest</a></h2> <p>Let&#39;s create an ensemble of DTR and fix the number of subfeatures to 3 for now.</p> <pre><code class="julia hljs">forest = EnsembleModel(atom=DecisionTreeRegressor())
forest.atom.n_subfeatures = <span class=hljs-number >3</span></code></pre><pre><code class="plaintext hljs">3</code></pre>
<p>&#40;<strong>NB</strong>: we could have fixed <code>n_subfeatures</code> in the DTR constructor too&#41;.</p>
<p>To get an idea of how many trees are needed, we can follow the evaluation of the error &#40;say the <code>rms</code>&#41; for an increasing number of tree over several sampling round.</p>
<pre><code class="julia hljs">rng = StableRNG(<span class=hljs-number >5123</span>) <span class=hljs-comment ># for reproducibility</span>
m = machine(forest, X, y)
r = range(forest, :n, lower=<span class=hljs-number >10</span>, upper=<span class=hljs-number >1000</span>)
curves = learning_curve!(m, resampling=Holdout(fraction_train=<span class=hljs-number >0.8</span>, rng=rng),
                         range=r, measure=rms);</code></pre>
<p>let&#39;s plot the curves</p>
<pre><code class="julia hljs">figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
plot(curves.parameter_values, curves.measurements)
ylabel(<span class=hljs-string >"Root Mean Squared error"</span>, fontsize=<span class=hljs-number >16</span>)
xlabel(<span class=hljs-string >"Number of trees"</span>, fontsize=<span class=hljs-number >16</span>)
xticks([<span class=hljs-number >10</span>, <span class=hljs-number >250</span>, <span class=hljs-number >500</span>, <span class=hljs-number >750</span>, <span class=hljs-number >1000</span>], fontsize=<span class=hljs-number >14</span>)
yticks(fontsize=<span class=hljs-number >14</span>)</code></pre>
<img src="/DataScienceTutorials.jl/assets/getting-started/ensembles-2/code/output/A-ensembles-2-curves.svg" alt="RMS vs number of trees">
<p>The curve is pretty noisy but let&#39;s just go for 150 trees:</p>
<pre><code class="julia hljs">forest.n = <span class=hljs-number >150</span>;</code></pre>
<h3 id=tuning ><a href="#tuning">Tuning</a></h3>
<p>As <code>forest</code> is a composite model, it has nested hyperparameters:</p>
<pre><code class="julia hljs">params(forest) |&gt; pprint</code></pre><pre><code class="plaintext hljs">(atom = (max_depth = -1,
         min_samples_leaf = 5,
         min_samples_split = 2,
         min_purity_increase = 0.0,
         n_subfeatures = 3,
         post_prune = false,
         merge_purity_threshold = 1.0),
 atomic_weights = [],
 bagging_fraction = 0.8,
 rng = Random._GLOBAL_RNG(),
 n = 150,
 acceleration = CPU1{Nothing}(nothing),
 out_of_bag_measure = [])</code></pre>
<p>Let&#39;s define a range for the number of subfeatures and for the bagging fraction:</p>
<pre><code class="julia hljs">r_sf = range(forest, :(atom.n_subfeatures), lower=<span class=hljs-number >1</span>, upper=<span class=hljs-number >12</span>)
r_bf = range(forest, :bagging_fraction, lower=<span class=hljs-number >0.4</span>, upper=<span class=hljs-number >1.0</span>);</code></pre>
<p>And build a tuned model as usual that we fit on a 80/20 split. We use a low-resolution grid here to make this tutorial faster but you could of course use a finer grid.</p>
<pre><code class="julia hljs">tuned_forest = TunedModel(model=forest,
                          tuning=Grid(resolution=<span class=hljs-number >3</span>),
                          resampling=CV(nfolds=<span class=hljs-number >6</span>, rng=StableRNG(<span class=hljs-number >32</span>)),
                          ranges=[r_sf, r_bf],
                          measure=rms)
m = machine(tuned_forest, X, y)
<span class=hljs-literal >e</span> = evaluate!(m, resampling=Holdout(fraction_train=<span class=hljs-number >0.8</span>),
              measure=[rms, rmslp1])
<span class=hljs-literal >e</span> |&gt; pprint</code></pre><pre><code class="plaintext hljs">(measure = [rms, rmslp1],
 measurement = [3.9291969189624694, 0.24921244622176925],
 per_fold = [[3.9291969189624694], [0.24921244622176925]],
 per_observation = [missing, missing])</code></pre>
<h3 id=reporting ><a href="#reporting">Reporting</a></h3>  Again, you could show a 2D heatmap of the hyperparameters</p>
<pre><code class="julia hljs">r = report(m)

figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))

res = r.plotting

vals_sf = res.parameter_values[:, <span class=hljs-number >1</span>]
vals_bf = res.parameter_values[:, <span class=hljs-number >2</span>]

tricontourf(vals_sf, vals_bf, res.measurements)
xticks(<span class=hljs-number >1</span>:<span class=hljs-number >3</span>:<span class=hljs-number >12</span>, fontsize=<span class=hljs-number >12</span>)
xlabel(<span class=hljs-string >"Number of sub-features"</span>, fontsize=<span class=hljs-number >14</span>)
yticks(<span class=hljs-number >0.4</span>:<span class=hljs-number >0.2</span>:<span class=hljs-number >1</span>, fontsize=<span class=hljs-number >12</span>)
ylabel(<span class=hljs-string >"Bagging fraction"</span>, fontsize=<span class=hljs-number >14</span>)</code></pre>
<img src="/DataScienceTutorials.jl/assets/getting-started/ensembles-2/code/output/A-ensembles-2-heatmap.svg" alt="">
<p>Even though we&#39;ve only done a very rough search, it seems that around 7 sub-features and a bagging fraction of around <code>0.75</code> work well.</p>
<p>Now that the machine <code>m</code> is trained, you can use use it for predictions &#40;implicitly, this will use the best model&#41;. For instance we could look at predictions on the whole dataset:</p>
<pre><code class="julia hljs">ŷ = predict(m, X)
rms(ŷ, y)</code></pre><div class=page-foot >
  <div class=copyright >
    &copy; Thibaut Lienart, Anthony Blaom and collaborators. Last modified: May 24, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div>
      </div> 
  </div> 
  <script src="/DataScienceTutorials.jl/libs/pure/ui.min.js"></script>