<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/DataScienceTutorials.jl/libs/highlight/github.min.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/franklin.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/pure.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/side-menu.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/extra.css"> <title>Learning networks</title> <script src="/DataScienceTutorials.jl/libs/lunr/lunr.min.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunr_index.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunrclient.min.js"></script> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/DataScienceTutorials.jl/" id=menu-logo-link > <div class=menu-logo > <p><strong>Data Science Tutorials</strong></p> </div> </a> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/DataScienceTutorials.jl/search/index.html" style="visibility:hidden"> </form> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/DataScienceTutorials.jl/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Data basics</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/loading/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Loading data</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/dataframe/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Data Frames</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/categorical/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/scitype/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Scientific Type</a> </ul> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/fit-and-predict/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/model-tuning/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/composing-models/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/stacking/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Stacking</a> </ul> <li class=pure-menu-sublist-title ><strong>Intro to Stats Learning</strong> <ul class=pure-menu-sublist  id=isl> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 2</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 3</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-4/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 4</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-5/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 5</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-6b/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 6b</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-8/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 8</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-9/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 9</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-10/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 10</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/AMES/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/wine/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/crabs-xgb/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/horse/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Horse</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/HouseKingCounty/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> King County Houses</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/airfoil" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Airfoil </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-lgbm" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/glm/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Using GLM.jl </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/powergen/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Power Generation </a> </ul> </ul> </div> </div> <div id=main > <div class=franklin-content ><h1 id=learning_networks ><a href="#learning_networks">Learning networks</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/notebooks/A-learning-networks.ipynb" target=_blank ><em>notebook</em></a>, <em>the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/A-learning-networks-raw.jl" target=_blank ><em>raw script</em></a>, <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/A-learning-networks.jl" target=_blank ><em>annotated script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=franklin-toc ><ol><li><a href="#preliminary_steps">Preliminary steps</a><li><a href="#defining_a_learning_network">Defining a learning network</a><ol><li><a href="#sources_and_nodes">Sources and nodes</a><li><a href="#modifying_hyperparameters">Modifying hyperparameters</a></ol><li><a href="#quotarrowquot_syntax">&quot;Arrow&quot; syntax</a></ol></div><h2 id=preliminary_steps ><a href="#preliminary_steps">Preliminary steps</a></h2> <p>Let&#39;s generate a <code>DataFrame</code> with some dummy regression data, let&#39;s also load the good old ridge regressor.</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLJ, StableRNGs
<span class=hljs-keyword >import</span> DataFrames
<span class=hljs-meta >@load</span> RidgeRegressor pkg=MultivariateStats

rng = StableRNG(<span class=hljs-number >551234</span>) <span class=hljs-comment ># for reproducibility</span>

x1 = rand(rng, <span class=hljs-number >300</span>)
x2 = rand(rng, <span class=hljs-number >300</span>)
x3 = rand(rng, <span class=hljs-number >300</span>)
y = exp.(x1 - x2 -<span class=hljs-number >2</span>x3 + <span class=hljs-number >0.1</span>*rand(rng, <span class=hljs-number >300</span>))

X = DataFrames.DataFrame(x1=x1, x2=x2, x3=x3)
first(X, <span class=hljs-number >3</span>) |&gt; pretty</code></pre><pre><code class="plaintext hljs">┌────────────────────┬─────────────────────┬─────────────────────┐
│ x1                 │ x2                  │ x3                  │
│ Float64            │ Float64             │ Float64             │
│ Continuous         │ Continuous          │ Continuous          │
├────────────────────┼─────────────────────┼─────────────────────┤
│ 0.9840017609992084 │ 0.7714818111684167  │ 0.23209935449185903 │
│ 0.8917954915748527 │ 0.7473993120336746  │ 0.7709140827147394  │
│ 0.8063948246988288 │ 0.01827506280083635 │ 0.07216450827912091 │
└────────────────────┴─────────────────────┴─────────────────────┘
</code></pre> <p>Let&#39;s also prepare the train and test split which will be useful later on.</p> <pre><code class="julia hljs">test, train = partition(eachindex(y), <span class=hljs-number >0.8</span>);</code></pre>
<h2 id=defining_a_learning_network ><a href="#defining_a_learning_network">Defining a learning network</a></h2>
<p>In MLJ, a <em>learning network</em> is a directed acyclic graph &#40;DAG&#41; whose <em>nodes</em> apply trained or untrained operations such as a <code>predict</code> or <code>transform</code> &#40;trained&#41; or <code>&#43;</code>, <code>vcat</code> etc. &#40;untrained&#41;. Learning networks can be seen as pipelines on steroids.</p>
<p>Let&#39;s consider the following simple DAG:</p>
<p><img src="/DataScienceTutorials.jl/assets/diagrams/composite1.svg" alt="Operation DAG" /></p>
<p>It corresponds to a fairly standard regression workflow: the data is standardized, the target is transformed using a Box-Cox transformation, a ridge regression is applied and the result is converted back by inverting the transform.</p>
<p><strong>Note</strong>: actually  this DAG is simple enough that it could also have been done with a pipeline.</p>
<h3 id=sources_and_nodes ><a href="#sources_and_nodes">Sources and nodes</a></h3>
<p>In MLJ a learning network starts at <strong>source</strong> nodes and flows through nodes &#40;<code>X</code> and <code>y</code>&#41; defining operations/transformations &#40;<code>W</code>, <code>z</code>, <code>ẑ</code>, <code>ŷ</code>&#41;. To define the source nodes, use the <code>source</code> function, you should specify whether it&#39;s a target:</p>
<pre><code class="julia hljs">Xs = source(X)
ys = source(y, kind=:target)</code></pre><pre><code class="plaintext hljs">Source{:target} @ 1…97
</code></pre>
<p>To define an &quot;trained-operation&quot; node, you must simply create a machine wrapping a model and another node &#40;the data&#41; and indicate which operation should be performed &#40;e.g. <code>transform</code>&#41;:</p>
<pre><code class="julia hljs">stand = machine(Standardizer(), Xs)
W = transform(stand, Xs)</code></pre><pre><code class="plaintext hljs">Node @ 2…39 = transform(8…07, 1…34)</code></pre>
<p>You can <code>fit&#33;</code> a trained-operation node at any point, MLJ will fit whatever it needs that is upstream of that node. In this case, there is just a source node upstream of <code>W</code> so fitting <code>W</code> will just fit the standardizer:</p>
<pre><code class="julia hljs">fit!(W, rows=train);</code></pre>
<p>If you want to get the transformed data, you can then call the node speciying on which part of the data the operation should be performed:</p>
<pre><code class="julia hljs">W()             <span class=hljs-comment ># transforms all data</span>
W(rows=test, )  <span class=hljs-comment ># transforms only test data</span>
W(X[<span class=hljs-number >3</span>:<span class=hljs-number >4</span>, :])    <span class=hljs-comment ># transforms specific data</span></code></pre><pre><code class="plaintext hljs">2×3 DataFrame
│ Row │ x1       │ x2       │ x3        │
│     │ Float64  │ Float64  │ Float64   │
├─────┼──────────┼──────────┼───────────┤
│ 1   │ 0.856967 │ -1.59115 │ -1.48215  │
│ 2   │ -1.06436 │ -1.5056  │ -0.234452 │</code></pre>
<p>Let&#39;s now define the other nodes:</p>
<pre><code class="julia hljs">box_model = UnivariateBoxCoxTransformer()
box = machine(box_model, ys)
z = transform(box, ys)

ridge_model = RidgeRegressor(lambda=<span class=hljs-number >0.1</span>)
ridge = machine(ridge_model, W, z)
ẑ = predict(ridge, W)

ŷ = inverse_transform(box, ẑ)</code></pre><pre><code class="plaintext hljs">Node @ 1…39 = inverse_transform(1…43, predict(2…87, transform(8…07, 1…34)))</code></pre>
<p>Note that we have not yet done any training, but if we now call <code>fit&#33;</code> on <code>ŷ</code>, it will fit all nodes upstream of <code>ŷ</code> that need to be re-trained:</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train);</code></pre>
<p>Now that <code>ŷ</code> has been fitted, you can apply the full graph on test data &#40;or any compatible data&#41;. For instance, let&#39;s get the <code>rms</code> between the ground truth and the predicted values:</p>
<pre><code class="julia hljs">rms(y[test], ŷ(rows=test))</code></pre><pre><code class="plaintext hljs">0.033604963634078534</code></pre>
<h3 id=modifying_hyperparameters ><a href="#modifying_hyperparameters">Modifying hyperparameters</a></h3>
<p>Hyperparameters can be accessed using the dot syntax as usual. Let&#39;s modify the regularisation parameter of the ridge regression:</p>
<pre><code class="julia hljs">ridge_model.lambda = <span class=hljs-number >5.0</span>;</code></pre>
<p>Since the node <code>ẑ</code> corresponds to a machine that wraps <code>ridge_model</code>, that node has effectively changed and will be retrained:</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train)
rms(y[test], ŷ(rows=test))</code></pre><pre><code class="plaintext hljs">0.03834272597361209</code></pre>
<h2 id=quotarrowquot_syntax ><a href="#quotarrowquot_syntax">&quot;Arrow&quot; syntax</a></h2>  <strong>Important</strong>: for this to work, you need to be using <strong>Julia ≥ 1.3</strong>:</p>
<p>The syntax to define nodes etc. is a bit verbose. MLJ supports a shorter syntax which abstracts away some of the steps. We will refer to it as the &quot;arrow&quot; syntax as it makes use of the <code>|&gt;</code> operator which can be interpreted as &quot;data flow&quot;.</p>
<p>Let&#39;s start with <code>W</code> and <code>z</code> &#40;the &quot;first layer&quot;&#41;:</p>
<pre><code class="julia hljs">W = X |&gt; Standardizer()
z = y |&gt; UnivariateBoxCoxTransformer()</code></pre><pre><code class="plaintext hljs">Node @ 5…57 = transform(8…01, 7…32)</code></pre>
<p>Note that we feed <code>X</code> and <code>y</code> directly into models. In the background, MLJ will create source nodes and assumes that the operation is a <code>transform</code> given the models are unsupervised.</p>
<p>For a node that corresponds to a supervised model, you can feed a tuple where the first element corresponds to the input &#40;here <code>W</code>&#41; and the second corresponds to the target &#40;here <code>z</code>&#41;, MLJ will assume the operation is a <code>predict</code>:</p>
<pre><code class="julia hljs">ẑ = (W, z) |&gt; RidgeRegressor(lambda=<span class=hljs-number >0.1</span>);</code></pre>
<p>Finally we need to apply the inverse of the transform encapsulated in the node <code>z</code>, for this:</p>
<pre><code class="julia hljs">ŷ = ẑ |&gt; inverse_transform(z);</code></pre>
<p>That&#39;s it&#33; You can now fit the network as before:</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train)
rms(y[test], ŷ(rows=test))</code></pre><pre><code class="plaintext hljs">0.033604963634078534</code></pre>
<p>To <em>manually</em> modify hyperparameters on a node, you can access them like so:</p>
<pre><code class="julia hljs">ẑ[:lambda] = <span class=hljs-number >5.0</span>;</code></pre>
<p>Here remember that <code>ẑ</code> is a node with a machine that wraps around a ridge regression with a parameter <code>lambda</code> so the syntax above is equivalent to</p>
<pre><code class="julia hljs">ẑ.machine.model.lambda = <span class=hljs-number >5.0</span>;</code></pre>
<p>which is relevant if you want to tune the hyperparameter using a <code>TunedModel</code>.</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train)
rms(y[test], ŷ(rows=test))</code></pre><pre><code class="plaintext hljs">0.03834272597361209</code></pre><div class=page-foot >
  <div class=copyright >
    &copy; Thibaut Lienart, Anthony Blaom and collaborators. Last modified: May 24, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div>
      </div> 
  </div> 
  <script src="/DataScienceTutorials.jl/libs/pure/ui.min.js"></script>