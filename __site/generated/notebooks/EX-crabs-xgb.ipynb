{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"DataScienceTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example is inspired from [this post](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) showing how to use XGBoost.\n",
    "\n",
    "## First steps\n",
    "\n",
    "Again, the crabs dataset is so common that there is a  simple load function for it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "using StatsBase\n",
    "using Random\n",
    "using PyPlot\n",
    "using CategoricalArrays\n",
    "using PrettyPrinting\n",
    "import DataFrames\n",
    "using LossFunctions\n",
    "\n",
    "X, y = @load_crabs\n",
    "X = DataFrames.DataFrame(X)\n",
    "@show size(X)\n",
    "@show y[1:3]\n",
    "first(X, 3) |> pretty"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's a classification problem with the following classes:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "levels(y) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the dataset is currently sorted by target, let's shuffle it to avoid the obvious issues this may cause"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(523)\n",
    "perm = randperm(length(y))\n",
    "X = X[perm,:]\n",
    "y = y[perm];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's not a very big dataset so we will likely overfit it badly using something as sophisticated as XGBoost but it will do for a demonstration."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.70, shuffle=true, rng=52)\n",
    "@load XGBoostClassifier\n",
    "xgb_model = XGBoostClassifier()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check whether the training and  is balanced, `StatsBase.countmap` is useful for that:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "countmap(y[train]) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "which is pretty balanced. You could check the same on the test set and full set and the same comment would still hold.\n",
    "\n",
    "## XGBoost machine\n",
    "\n",
    "Wrap a machine around an XGBoost model (XGB) and the data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xgb  = XGBoostClassifier()\n",
    "xgbm = machine(xgb, X, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will tune it varying the number of rounds used and generate a learning curve"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = range(xgb, :num_round, lower=50, upper=500)\n",
    "curve = learning_curve!(xgbm, range=r, resolution=50,\n",
    "                        measure=HingeLoss())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "plot(curve.parameter_values, curve.measurements)\n",
    "xlabel(\"Number of rounds\", fontsize=14)\n",
    "ylabel(\"HingeLoss\", fontsize=14)\n",
    "xticks([10, 100, 200, 500], fontsize=12)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Cross entropy vs Num Round}{EX-crabs-xgb-curve1.svg}\n",
    "\n",
    "So, in short, using more rounds helps. Let's arbitrarily fix it to 200."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xgb.num_round = 200;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More tuning (1)\n",
    "\n",
    "Let's now tune the maximum depth of each tree and the minimum child weight in the boosting."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r1 = range(xgb, :max_depth, lower=3, upper=10)\n",
    "r2 = range(xgb, :min_child_weight, lower=0, upper=5)\n",
    "\n",
    "tm = TunedModel(model=xgb, tuning=Grid(resolution=8),\n",
    "                resampling=CV(rng=11), ranges=[r1,r2],\n",
    "                measure=cross_entropy)\n",
    "mtm = machine(tm, X, y)\n",
    "fit!(mtm, rows=train)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great, as always we can investigate the tuning by using `report` and can, for instance, plot a heatmap of the measurements:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = report(mtm)\n",
    "\n",
    "res = r.plotting\n",
    "\n",
    "md = res.parameter_values[:,1]\n",
    "mcw = res.parameter_values[:,2]\n",
    "\n",
    "figure(figsize=(8,6))\n",
    "tricontourf(md, mcw, res.measurements)\n",
    "\n",
    "xlabel(\"Maximum tree depth\", fontsize=14)\n",
    "ylabel(\"Minimum child weight\", fontsize=14)\n",
    "xticks(3:2:10, fontsize=12)\n",
    "yticks(fontsize=12)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Hyperparameter heatmap}{EX-crabs-xgb-heatmap.svg}\n",
    "\n",
    "Let's extract the optimal model and inspect its parameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xgb = fitted_params(mtm).best_model\n",
    "@show xgb.max_depth\n",
    "@show xgb.min_child_weight"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More tuning (2)\n",
    "\n",
    "Let's examine the effect of `gamma`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xgbm = machine(xgb, X, y)\n",
    "r = range(xgb, :gamma, lower=0, upper=10)\n",
    "curve = learning_curve!(xgbm, range=r, resolution=30,\n",
    "                        measure=cross_entropy);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "it looks like the `gamma` parameter substantially affects model performance:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show round(minimum(curve.measurements), sigdigits=3)\n",
    "@show round(maximum(curve.measurements), sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More tuning (3)\n",
    "\n",
    "Let's examine the effect of `subsample` and `colsample_bytree`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r1 = range(xgb, :subsample, lower=0.6, upper=1.0)\n",
    "r2 = range(xgb, :colsample_bytree, lower=0.6, upper=1.0)\n",
    "tm = TunedModel(model=xgb, tuning=Grid(resolution=8),\n",
    "                resampling=CV(rng=234), ranges=[r1,r2],\n",
    "                measure=cross_entropy)\n",
    "mtm = machine(tm, X, y)\n",
    "fit!(mtm, rows=train)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and the usual procedure to visualise it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = report(mtm)\n",
    "\n",
    "res = r.plotting\n",
    "\n",
    "ss = res.parameter_values[:,1]\n",
    "cbt = res.parameter_values[:,2]\n",
    "\n",
    "figure(figsize=(8,6))\n",
    "tricontourf(ss, cbt, res.measurements)\n",
    "\n",
    "xlabel(\"Sub sample\", fontsize=14)\n",
    "ylabel(\"Col sample by tree\", fontsize=14)\n",
    "xticks(fontsize=12)\n",
    "yticks(fontsize=12)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Hyperparameter heatmap}{EX-crabs-xgb-heatmap2.svg}\n",
    "\n",
    "Let's retrieve the best models:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xgb = fitted_params(mtm).best_model\n",
    "@show xgb.subsample\n",
    "@show xgb.colsample_bytree"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could continue with more fine tuning but given how small the dataset is, it doesn't make much sense.\n",
    "How does it fare on the test set?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict_mode(mtm, rows=test)\n",
    "round(accuracy(ŷ, y[test]), sigdigits=3)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "kernelspec": {
   "name": "julia-1.4",
   "display_name": "Julia 1.4.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
