{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"MLJTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Preliminary steps\n",
    "\n",
    "Let's start by loading the relevant packages and generating some dummy data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ, DataFrames, Statistics, PrettyPrinting\n",
    "MLJ.color_off() # hide\n",
    "Xraw = rand(300, 3)\n",
    "y = exp.(Xraw[:,1] - Xraw[:,2] - 2Xraw[:,3] + 0.1*rand(300))\n",
    "X = DataFrame(Xraw)\n",
    "\n",
    "train, test = partition(eachindex(y), 0.7);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's also load a simple model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load KNNRegressor\n",
    "knn_model = KNNRegressor(K=10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "As before, let's instantiate a machine that wraps the model and data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "knn = machine(knn_model, X, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "and fit it"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(knn, rows=train)\n",
    "ŷ = predict(knn, X[test, :]) # or use rows=test\n",
    "rms(ŷ, y[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The few steps above are equivalent to just calling `evaluate!`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "evaluate!(knn, resampling=Holdout(fraction_train=0.7),\n",
    "          measure=rms) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Homogenous ensembles\n",
    "\n",
    "MLJ offers basic support for ensembling such as [_bagging_](https://en.wikipedia.org/wiki/Bootstrap_aggregating).\n",
    "Defining such an ensemble of simple \"atomic\" models is done via the `EnsembleModel` constructor:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ensemble_model = EnsembleModel(atom=knn_model, n=20);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "where the `n=20` indicates how many models are present in the ensemble.\n",
    "\n",
    "### Training and testing an ensemble\n",
    "\n",
    "Now that we've instantiated an ensemble, it can be trained and tested the same as any other model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ensemble = machine(ensemble_model, X, y)\n",
    "estimates = evaluate!(ensemble, resampling=CV())\n",
    "estimates |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "here the implicit measure is the `rms` (default for regressions). The `measurement` is the mean taken over the folds:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show estimates.measurement[1]\n",
    "@show mean(estimates.per_fold[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note that multiple measurements can be specified jointly. Here only on measurement is (implicitly) specified but we still have to select the corresponding results (whence the `[1]` for both  the `measurement` and `per_fold`).\n",
    "\n",
    "### Systematic tuning\n",
    "\n",
    "Let's simultaneously tune the ensemble's `bagging_fraction` and the K-Nearest neighbour hyperparameter `K`. Since one of our models is  a field of the  other, we have nested hyperparameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "params(ensemble_model) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "To define a tuning grid, we construct ranges for the two parameters and collate these ranges:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "B_range = range(ensemble_model, :bagging_fraction,\n",
    "                lower=0.5, upper=1.0)\n",
    "K_range = range(ensemble_model, :(atom.K),\n",
    "                lower=1, upper=20);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "the scale for a tuning grid is linear by default but can be specified to `:log10` for logarithmic ranges.\n",
    "Now we have to define a `TunedModel` and fit it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tm = TunedModel(model=ensemble_model,\n",
    "                tuning=Grid(resolution=10), # 10x10 grid\n",
    "                resampling=Holdout(fraction_train=0.8, rng=42),\n",
    "                ranges=[B_range, K_range])\n",
    "\n",
    "tuned_ensemble = machine(tm, X, y)\n",
    "fit!(tuned_ensemble, rows=train);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note the `rng=42` seeds the random number generator for reproducibility of this example.\n",
    "\n",
    "### Reporting results\n",
    "\n",
    "The best model can be accessed like so:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "best_ensemble = fitted_params(tuned_ensemble).best_model\n",
    "@show best_ensemble.atom.K\n",
    "@show best_ensemble.bagging_fraction"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The `report` method gives more detailed information on the tuning process:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r = report(tuned_ensemble);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "For instance, `r.measurements` are the measurements for all pairs of hyperparameters which you could visualise nicely:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using PyPlot\n",
    "\n",
    "figure(figsize=(8,6))\n",
    "\n",
    "res = r.plotting\n",
    "vals_b = res.parameter_values[:, 1]\n",
    "vals_k = res.parameter_values[:, 2]\n",
    "\n",
    "tricontourf(vals_b, vals_k, res.measurements)\n",
    "xticks(0.5:0.1:1, fontsize=12)\n",
    "xlabel(\"Bagging fraction\", fontsize=14)\n",
    "yticks([1, 5, 10, 15, 20], fontsize=12)\n",
    "ylabel(\"Number of neighbors - K\", fontsize=14)\n",
    "\n",
    "savefig(joinpath(@OUTPUT, \"A-ensembles-heatmap.svg\")) # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "\\figalt{Hyperparameter heatmap}{A-ensembles-heatmap.svg}\n",
    "\n",
    "Finally you can always just evaluate the model by reporting `rms` on the test set:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(tuned_ensemble, rows=test)\n",
    "rms(ŷ, y[test])\n",
    "\n",
    "PyPlot.close_figs() # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0-DEV.350"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.0-DEV.350",
   "language": "julia"
  }
 },
 "nbformat": 4
}
