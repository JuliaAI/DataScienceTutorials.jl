{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"DataScienceTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"MLJTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classification of fraudulent/not credit card transactions (imbalanced data)\n",
    "By Kristian Bjarnason. The original script can be found [here](https://github.com/kbjarnason/credit-card-fraud-classification)\n",
    "This version implements train, test, val split rather than just train, test."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "using Pkg; Pkg.add(\"Revise\")\n",
    "Revise wasn't installed - so installed it, but it triggers update to instantiatd environment (Project.toml,Manifest.toml)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#Relevant packages\n",
    "using Revise, DataFrames, CSV, LinearAlgebra, Dates, Statistics, MLJ, MLJBase, MLJModels, MLJLinearModels, Plots, Flux\n",
    "using UrlDownload, MLBase, StatsBase# , ROC, EvalCurves, ==> problem with these last two packages\n",
    "using Flux:outdims, activations, @epochs, throttle\n",
    "using Flux.Data"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "using AUC # add git@github.com:paulstey/AUC.jl.git"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *Data Preparation*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Divide the sample into two equal sub-samples. Keep the proportion of frauds the same in each sub-sample (246 frauds in each).\n",
    "Use one sub-sample to estimate (train) your models and the second one to evaluate the out-of-sample performance of each model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "data = urldownload(\"https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv\")\n",
    "data = DataFrame(data)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's inspect the (types of) variables contained in the DataFrame"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "schema(data)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Time column is not relevant to our analysis, we drop it."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "select!(data, Not(:Time))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's get a summary of the remaining data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "describe(data)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the Amount variable spans a wide range of values.\n",
    "To reduce variation in the data, we take the natural logarithm. Note that some values are 0 and that log(0) will raise an error. We add 1e-6 so no values are 0 prior to being log transformed.\n",
    "We transform in place using '!'"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "data[!,:Amount] = log.(data[!,:Amount] .+ 1e-6)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's unpack the dataframe and create separate frames for our target variable and features."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y, X = unpack(data, ==(:Class), col -> true)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And partition between training and test observations"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.8, shuffle=true, rng=111)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Important: train and test are one dimensional arrays and respectively contain the row indices of the train and test sets' observations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup train and test arrays/vectors"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xtrain = selectrows(X, train)\n",
    "Xtest = selectrows(X, test)\n",
    "ytrain = selectrows(y, train)\n",
    "ytest = selectrows(y, test)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also create categorical versions of the target variable, for use in the LogisticClassifier"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ytrain_cat = categorical(selectrows(y, train))\n",
    "ytest_cat = categorical(selectrows(y, test))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create standardised features for SVM and NN"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "stand_model = Standardizer()\n",
    "std_m_fit = fit!(machine(stand_model, Xtrain))\n",
    "Xtrain_std = MLJModels.transform(std_m_fit, Xtrain)\n",
    "Xtest_std = MLJModels.transform(std_m_fit, Xtest)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Estimation of models##"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Estimation of three different models:\n",
    "1. logit\n",
    "2. support vector machines\n",
    "3. neural network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Initial logit classification with lambda = 1.0**"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load LogisticClassifier pkg=MLJLinearModels\n",
    "\n",
    "model_logit = LogisticClassifier(lambda=1.0)\n",
    "m_logit = machine(model_logit, Xtrain, ytrain_cat)\n",
    "fit!(m_logit)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Predictions**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The prediction from the LogisticClassifier is a probabilistic output, i.e. for each observation in the sample it attaches a probability to each of the possible values of the target.\n",
    "To recover a deterministic output, we take the mode of the distribution."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "yhat_logit_p = MLJBase.predict(m_logit, Xtest)\n",
    "yhat_logit = categorical(mode.(yhat_logit_p))\n",
    "\n",
    "#Â How does this model perform?\n",
    "@show confusion_matrix(yhat_logit, ytest_cat)\n",
    "@show misclassification_rate(yhat_logit, ytest_cat)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Looks like it's not too bad)\n",
    "Let' see if we can do even better by doing a little tuning."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#CSV.write(\"yhat_logit.csv\", yhat_logit)\n",
    "#CSV.write(\"yhat_logit_p.csv\", yhat_logit_p)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tuned logit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Still LogisticClassifier but implementing hyperparameter tuning."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model_logit = @load LogisticClassifier pkg=MLJLinearModels\n",
    "r = range(model_logit, :lambda, lower=1e-6, upper=100, scale=:log)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The tuning strategy is specified below; note the use of a probabilistic measure (cross_entropy)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "self_tuning_logit_model = TunedModel(model=model_logit,\n",
    "                                                  resampling = CV(nfolds=3),\n",
    "                                                  tuning = Grid(resolution=10),\n",
    "                                                  range = r,\n",
    "                                                  measure = cross_entropy)\n",
    "\n",
    "self_tuning_logit = machine(self_tuning_logit_model, Xtrain, ytrain_cat)\n",
    "fit!(self_tuning_logit)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predictions"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "yhat_logit_tuned_p = MLJBase.predict(self_tuning_logit, Xtest)\n",
    "yhat_logit_tuned = categorical(mode.(yhat_logit_tuned_p))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at the misclassification_rate. It is, surprisingly, slightly higher than the one calculated for the non tuned model."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@show misclassification_rate(yhat_logit_tuned, ytest_cat)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Support Vector Machine"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initial SVM classification with cost = 1.0"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load SVC"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To fit the SVM, we declare a pipeline which comprises both a standardizer and the model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model_svm = @pipeline Std_SVC(std_model = Standardizer(),\n",
    "                                      svc = SVC())\n",
    "\n",
    "svc = machine(model_svm, Xtrain, ytrain_cat)\n",
    "\n",
    "fit!(svc)\n",
    "\n",
    "yhat_svm = MLJBase.predict(svc, Xtest_std)\n",
    "#0.00163\n",
    "@show confusion_matrix(yhat_svm, ytest_cat)\n",
    "@show misclassification_rate(yhat_svm, ytest_cat)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "CSV.write(\"yhat_svm.csv\", yhat_svm)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tuned SVM"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#@load SVC #does this command need to be repeated?\n",
    "\n",
    "model_svm = @pipeline Std_SVC(std_model = Standardizer(),\n",
    "                              svc = SVC())\n",
    "\n",
    "r = range(model_svm, :(svc.cost), lower=0.0, upper=2.5, scale=:linear)\n",
    "iterator(r,6)\n",
    "\n",
    "self_tuning_svm_model = TunedModel(model=model_svm,\n",
    "                                   resampling = CV(nfolds=3),\n",
    "                                   tuning = Grid(resolution=6),\n",
    "                                   range = r,\n",
    "                                   measure = MLJ.precision)\n",
    "\n",
    "self_tuning_svm = machine(self_tuning_svm_model, Xtrain, ytrain_cat)\n",
    "\n",
    "fit!(self_tuning_svm)\n",
    "\n",
    "@show fitted_params(self_tuning_svm).best_model\n",
    "@show report(self_tuning_svm)\n",
    "\n",
    "yhat_svm_tuned = MLJBase.predict(self_tuning_svm, Xtest_std)\n",
    "\n",
    "@show misclassification_rate(yhat_svm_tuned, ytest_cat)\n",
    "@show confusion_matrix(yhat_svm_tuned, ytest_cat)\n",
    "\n",
    "#CSV.write(\"yhat_svm_tuned.csv\", yhat_svm_tuned)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#oversample fraudulent cases (since data so imbalanced)\n",
    "X_train_oversampled = vcat(X_train,repeat(data_fraud[1:Int(nrow(data_fraud)/2),1:29], 100))\n",
    "y_train_oversampled = vcat(y_train,repeat(data_fraud[1:Int(nrow(data_fraud)/2),30], 100))\n",
    "\n",
    "stand_model = Standardizer()\n",
    "X_train_oversampled_std = MLJModels.transform(fit!(machine(stand_model, X_train_oversampled)), X_train_oversampled)\n",
    "\n",
    "data1 = DataLoader(Array(X_train_oversampled_std)', y_train_oversampled, batchsize=2048, shuffle=true)\n",
    "\n",
    "n_inputs = ncol(X_train_oversampled)\n",
    "n_outputs = 1\n",
    "n_hidden1 = 16\n",
    "\n",
    "m = Chain(\n",
    "          Dense(n_inputs, n_hidden1, relu),\n",
    "          Dropout(0.1),\n",
    "          Dense(n_hidden1, n_outputs, Ï)\n",
    "          )\n",
    "\n",
    "loss(x, y) = Flux.tversky_loss(m(x), y, Î²=0.9) #tversky loss uses precision and recall, slower calc than crossentropy"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "loss(x, y) = Flux.crossentropy(m(x), y)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ps = Flux.params(m)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "opt = ADAM()"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "opt = Descent()\n",
    "\n",
    "@epochs 50 Flux.train!(loss, ps, data1, opt)\n",
    "\n",
    "yhat_nn_p = vec(m(Array(X_test_std)'))\n",
    "yhat_nn = categorical(Int.(yhat_nn_p .<= 0.5))\n",
    "\n",
    "cm_nn = confusion_matrix(yhat_nn, y_test)\n",
    "misclassification_rate(yhat_nn, y_test)\n",
    "\n",
    "CSV.write(\"yhat_nn.csv\", yhat_nn)\n",
    "\n",
    "#%%md\n",
    "*OOS results*\n",
    "#%%\n",
    "#if needed can reload from here\n",
    "yhat_logit_tuned = CSV.read(\"yhat_logit_tuned.csv\")\n",
    "yhat_svm_tuned = CSV.read(\"yhat_svm_tuned.csv\")\n",
    "yhat_nn = CSV.read(\"yhat_nn.csv\")\n",
    "\n",
    "#%%md\n",
    "Misclassification rate\n",
    "#%%\n",
    "misclassification_rate(yhat_logit_tuned, y_test)\n",
    "misclassification_rate(yhat_svm_tuned, y_test)\n",
    "misclassification_rate(yhat_nn, y_test)\n",
    "\n",
    "#%%md\n",
    "Confusion matrix\n",
    "#%%\n",
    "cm_logit = confusion_matrix(yhat_logit_tuned, y_test)\n",
    "cm_svm = confusion_matrix(yhat_svm_tuned , y_test)\n",
    "cm_nn = confusion_matrix(yhat_nn, y_test)\n",
    "\n",
    "#%%md\n",
    "ROC curves\n",
    "#%%\n",
    "#Due to different data output structure had to use different packages for ROC curves\n",
    "plot(roc_curve(yhat_logit_tuned_p, y_test))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "plot(ROC.roc(pdf.(yhat_logit_tuned_p), y_test, 1)))"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(ROC.roc(yhat_nn_p, y_test, 1))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "don't have score vectors for SVM\n",
    "plot(roc_curve(yhat_svm_p,y_test))\n",
    "plot(roc_curve(yhat_nn_p,y_test))"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#how to plot this??\n",
    "MLBase.roc(y_test_int, yhat_nn_p)\n",
    "\n",
    "#%%md\n",
    "Precision-Recall curve\n",
    "#%%\n",
    "plot(prcurve(pdf.(yhat_logit_tuned_p,1), y_test_int))\n",
    "plot(prcurve(yhat_nn_p, y_test_int))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "don't have score vectors for SVM\n",
    "plot(prcurve(pdf.(yhat_svm_p,1), y_test_int))"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "#END"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "kernelspec": {
   "name": "julia-1.4",
   "display_name": "Julia 1.4.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
