{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"MLJTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "[MLJ.jl]: https://github.com/alan-turing-institute/MLJ.jl\n",
    "[RDatasets.jl]: https://github.com/JuliaStats/RDatasets.jl\n",
    "[DecisionTree.jl]: https://github.com/bensadeghi/DecisionTree.jl\n",
    "\n",
    "## Preliminary steps\n",
    "\n",
    "### Data\n",
    "\n",
    "As in \"[choosing a model](/getting-started/choosing-a-model/)\", let's load the Iris dataset and unpack it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ, Statistics, PrettyPrinting\n",
    "MLJ.color_off() # hide\n",
    "X, y = @load_iris;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "let's also load the `DecisionTreeClassifier`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load DecisionTreeClassifier\n",
    "tree_model = DecisionTreeClassifier()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### MLJ Machine\n",
    "\n",
    "In MLJ, remember that a *model* is an object that only serves as a container for the hyperparameters of the model.\n",
    "A *machine* is an object wrapping both a model and data and can contain information on the *trained* model; it does *not* fit the model by itself.\n",
    "However, it does check that the model is compatible with the scientific type of the data and will warn you otherwise."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tree = machine(tree_model, X, y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "A machine is used both for supervised and unsupervised model.\n",
    "In this tutorial we give an example for the supervised model first and then go on with the unsupervised case.\n",
    "\n",
    "## Training and testing a supervised model\n",
    "\n",
    "Now that you've declared the model you'd like to consider and the data, we are left with the standard training and testing step for a supervised learning algorithm.\n",
    "\n",
    "### Splitting the data\n",
    "\n",
    "To split the data into a *training* and *testing* set, you can use the function `partition` to obtain indices for data points that should be considered either as training or testing data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(y), 0.7, shuffle=true)\n",
    "test[1:3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Fitting and testing the machine\n",
    "\n",
    "To fit the machine, you can use the function `fit!` specifying the rows to be used for the training:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(tree, rows=train)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note that this **modifies** the machine which now contains the trained parameters of the decision tree.\n",
    "You can inspect the result of the fitting with the `fitted_params` method:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fitted_params(tree) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "This `fitresult` will vary from model to model though classifiers will usually give out a tuple with the first element corresponding to the fitting and the second one keeping track of how classes are named (so that predictions can be appropriately named).\n",
    "\n",
    "You can now use the machine to make predictions with the `predict` function specifying rows to be used for the prediction:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(tree, rows=test)\n",
    "@show ŷ[1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note that the output is *probabilistic*, effectively a vector with a score for each class.\n",
    "You could get the mode by using the `mode` function on `ŷ` or using `predict_mode`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ȳ = predict_mode(tree, rows=test)\n",
    "@show ȳ[1]\n",
    "@show mode(ŷ[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "To measure the discrepancy between `ŷ` and `y` you could use the average cross entropy:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mce = cross_entropy(ŷ, y[test]) |> mean\n",
    "round(mce, digits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Unsupervised models\n",
    "\n",
    "Unsupervised models define a `transform` method,\n",
    "and may optionally implement an `inverse_transform` method.\n",
    "As in the supervised case, we use a machine to wrap the unsupervised model and the data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "v = [1, 2, 3, 4]\n",
    "stand_model = UnivariateStandardizer()\n",
    "stand = machine(stand_model, v)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We can then fit the machine and use it to apply the corresponding *data transformation*:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(stand)\n",
    "w = transform(stand, v)\n",
    "@show round.(w, digits=2)\n",
    "@show mean(w)\n",
    "@show std(w)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "In this case, the model also has an inverse transform:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "vv = inverse_transform(stand, w)\n",
    "sum(abs.(vv .- v))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0-DEV.350"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.0-DEV.350",
   "language": "julia"
  }
 },
 "nbformat": 4
}
