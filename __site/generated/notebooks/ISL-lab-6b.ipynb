{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"DataScienceTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# In this tutorial, we are exploring the application of Ridge and Lasso"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "regression to the Hitters R dataset.\n",
    "\n",
    "## Getting started"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "import RDatasets: dataset\n",
    "using PrettyPrinting\n",
    "import Distributions\n",
    "const D = Distributions\n",
    "\n",
    "@load LinearRegressor pkg=MLJLinearModels\n",
    "@load RidgeRegressor pkg=MLJLinearModels\n",
    "@load LassoRegressor pkg=MLJLinearModels"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We load the dataset using the `dataset` function, which takes the Package and\n",
    "dataset names as arguments."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "hitters = dataset(\"ISLR\", \"Hitters\")\n",
    "@show size(hitters)\n",
    "names(hitters) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's unpack the dataset with the `unpack` function.\n",
    "In this case, the target is `Salary` (`==(:Salary)`) and all other columns are features (`col->true`)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y, X = unpack(hitters, ==(:Salary), col->true);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The target has missing values which we will just ignore.\n",
    "We extract the row indices corresponding to non-missing values of the target.\n",
    "Note the use of the element-wise operator `.`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "no_miss = .!ismissing.(y);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We collect the non missing values of the target in an Array."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# And keep only the corresponding features values.\n",
    "y = collect(skipmissing(y))\n",
    "X = X[no_miss, :]\n",
    "\n",
    "# Let's now split our dataset into a train and test sets.\n",
    "train, test = partition(eachindex(y), 0.5, shuffle=true, rng=424);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look at the target."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using PyPlot\n",
    "\n",
    "figure(figsize=(8,6))\n",
    "plot(y, ls=\"none\", marker=\"o\")\n",
    "\n",
    "xticks(fontsize=12); yticks(fontsize=12)\n",
    "xlabel(\"Index\", fontsize=14), ylabel(\"Salary\", fontsize=14)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Salary}{ISL-lab-6-g1.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks quite skewed, let's have a look at a histogram:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "hist(y, bins=50, density=true)\n",
    "\n",
    "xticks(fontsize=12); yticks(fontsize=12)\n",
    "xlabel(\"Salary\", fontsize=14); ylabel(\"Density\", fontsize=14)\n",
    "\n",
    "edfit = D.fit_mle(D.Exponential, y)\n",
    "xx = range(minimum(y), 2500, length=100)\n",
    "yy = pdf.(edfit, xx)\n",
    "plot(xx, yy, lw=3, label=\"Exponential distribution fit\")\n",
    "\n",
    "legend(fontsize=12)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Distribution of salary}{ISL-lab-6-g2.svg}\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "Most features are currently encoded as integers but we will consider them as continuous.\n",
    "To coerce `int` features to `Float`, we nest the `autotype` function in the `coerce` function.\n",
    "The `autotype` function returns a dictionary containing scientific types, which is then passed to the `coerce` function.\n",
    "For more details on the use of `autotype`, see the [Scientific Types](https://alan-turing-institute.github.io/DataScienceTutorials.jl/data/scitype/index.html#autotype)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xc = coerce(X, autotype(X, rules=(:discrete_to_continuous,)))\n",
    "scitype(Xc)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There're a few features that are categorical which we'll one-hot-encode."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ridge pipeline\n",
    "### Baseline\n",
    "\n",
    "Let's first fit a simple pipeline with a standardizer, a one-hot-encoder and a basic linear regression:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = @pipeline(Standardizer(),\n",
    "                     OneHotEncoder(),\n",
    "                     LinearRegressor())\n",
    "\n",
    "pipe  = machine(model, Xc, y)\n",
    "fit!(pipe, rows=train)\n",
    "ŷ = predict(pipe, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's get a feel for how we're doing"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "\n",
    "res = ŷ .- y[test]\n",
    "stem(res)\n",
    "\n",
    "xticks(fontsize=12); yticks(fontsize=12)\n",
    "xlabel(\"Index\", fontsize=14); ylabel(\"Residual (ŷ - y)\", fontsize=14)\n",
    "\n",
    "ylim([-1300, 1000])\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Residuals}{ISL-lab-6-g3.svg}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "hist(res, bins=30, density=true, color=\"green\")\n",
    "\n",
    "xx = range(-1100, 1100, length=100)\n",
    "ndfit = D.fit_mle(D.Normal, res)\n",
    "lfit  = D.fit_mle(D.Laplace, res)\n",
    "\n",
    "plot(xx, pdf.(ndfit, xx), lw=3, color=\"orange\", label=\"Normal fit\")\n",
    "plot(xx, pdf.(lfit, xx), lw=3, color=\"magenta\", label=\"Laplace fit\")\n",
    "\n",
    "legend(fontsize=12)\n",
    "\n",
    "xticks(fontsize=12); yticks(fontsize=12)\n",
    "xlabel(\"Residual (ŷ - y)\", fontsize=14); ylabel(\"Density\", fontsize=14)\n",
    "xlim([-1100, 1100])\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Distribution of residuals}{ISL-lab-6-g4.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Ridge\n",
    "\n",
    "Let's now swap the linear regressor for a Ridge one without specifying the penalty (`1` by default):\n",
    "We modify the supervised model in the pipeline directly."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pipe.model.linear_regressor = RidgeRegressor()\n",
    "fit!(pipe, rows=train)\n",
    "ŷ = predict(pipe, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok that's a bit better but surely we can do better with an appropriate selection of the hyperparameter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross validating"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What penalty should you use? Let's do a simple CV to try to find out:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r  = range(model, :(linear_regressor.lambda), lower=1e-2, upper=100_000, scale=:log10)\n",
    "tm = TunedModel(model=model, ranges=r, tuning=Grid(resolution=50),\n",
    "                resampling=CV(nfolds=3, rng=4141), measure=rms)\n",
    "mtm = machine(tm, Xc, y)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "round(best_mdl.linear_regressor.lambda, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "right, and  with that we get:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(mtm, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "\n",
    "res = ŷ .- y[test]\n",
    "stem(res)\n",
    "\n",
    "xticks(fontsize=12); yticks(fontsize=12)\n",
    "xlabel(\"Index\", fontsize=14);\n",
    "ylabel(\"Residual (ŷ - y)\", fontsize=14)\n",
    "xlim(1, length(res))\n",
    "\n",
    "ylim([-1300, 1000])\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Ridge residuals}{ISL-lab-6-g5.svg}\n",
    "\n",
    "You can compare that with the residuals obtained earlier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lasso pipeline\n",
    "\n",
    "Let's do the same as above but using a Lasso model and adjusting the range a bit:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mtm.model.model.linear_regressor = LassoRegressor()\n",
    "mtm.model.range = range(model, :(linear_regressor.lambda), lower=500, upper=100_000, scale=:log10)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "round(best_mdl.linear_regressor.lambda, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok and let's see how that does:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(mtm, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pretty good! and the parameters are reasonably sparse as expected:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "coefs, intercept = fitted_params(mtm.fitresult).linear_regressor\n",
    "@show coefs\n",
    "@show intercept"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "with around 50% sparsity:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "coef_vals = [c[2] for c in coefs]\n",
    "sum(coef_vals .≈ 0) / length(coefs)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualise this:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "figure(figsize=(8,6))\n",
    "stem(coef_vals)\n",
    "\n",
    "# name of the features including one-hot-encoded ones\n",
    "all_names = [:AtBat, :Hits, :HmRun, :Runs, :RBI, :Walks, :Years,\n",
    "             :CAtBat, :CHits, :CHmRun, :CRuns, :CRBI, :CWalks,\n",
    "             :League__A, :League__N, :Div_E, :Div_W,\n",
    "             :PutOuts, :Assists, :Errors, :NewLeague_A, :NewLeague_N]\n",
    "\n",
    "idxshow = collect(1:length(coef_vals))[abs.(coef_vals) .> 10]\n",
    "xticks(idxshow .- 1, all_names[idxshow], rotation=45, fontsize=12)\n",
    "yticks(fontsize=12)\n",
    "ylabel(\"Amplitude\", fontsize=14)\n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\figalt{Lasso coefficients}{ISL-lab-6-g6.svg}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Elastic net pipeline"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load ElasticNetRegressor pkg=MLJLinearModels\n",
    "\n",
    "mtm.model.model.linear_regressor = ElasticNetRegressor()\n",
    "mtm.model.range = [range(model, :(linear_regressor.lambda), lower=0.1, upper=100, scale=:log10),\n",
    "                    range(model, :(linear_regressor.gamma),  lower=500, upper=10_000, scale=:log10)]\n",
    "mtm.model.tuning = Grid(resolution=10)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "@show round(best_mdl.linear_regressor.lambda, sigdigits=4)\n",
    "@show round(best_mdl.linear_regressor.gamma, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And it's not too bad in terms of accuracy either"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(mtm, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "But the simple ridge regression seems to work best here."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "kernelspec": {
   "name": "julia-1.4",
   "display_name": "Julia 1.4.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
