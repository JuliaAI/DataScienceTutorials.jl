<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/DataScienceTutorials.jl/libs/highlight/github.min.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/franklin.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/pure.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/side-menu.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/extra.css"> <title>Lab 6b - Ridge and Lasso regression</title> <script src="/DataScienceTutorials.jl/libs/lunr/lunr.min.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunr_index.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunrclient.min.js"></script> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/DataScienceTutorials.jl/" id=menu-logo-link > <div class=menu-logo > <p><strong>Data Science Tutorials</strong></p> </div> </a> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/DataScienceTutorials.jl/search/index.html" style="visibility:hidden"> </form> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/DataScienceTutorials.jl/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Data basics</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/loading/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Loading data</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/dataframe/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Data Frames</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/categorical/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/scitype/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Scientific Type</a> </ul> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/fit-and-predict/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/model-tuning/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/composing-models/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/stacking/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Stacking</a> </ul> <li class=pure-menu-sublist-title ><strong>Intro to Stats Learning</strong> <ul class=pure-menu-sublist  id=isl> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 2</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 3</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-4/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 4</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-5/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 5</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-6b/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 6b</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-8/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 8</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-9/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 9</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-10/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 10</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/AMES/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/wine/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/crabs-xgb/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/horse/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Horse</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/HouseKingCounty/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> King County Houses</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/airfoil" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Airfoil </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-lgbm" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/glm/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Using GLM.jl </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/powergen/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Power Generation </a> </ul> </ul> </div> </div> <div id=main > <div class=franklin-content ><h1 id=lab_6b_-_ridge_and_lasso_regression ><a href="#lab_6b_-_ridge_and_lasso_regression">Lab 6b - Ridge and Lasso regression</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/notebooks/ISL-lab-6b.ipynb" target=_blank ><em>notebook</em></a>, <em>the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/ISL-lab-6b-raw.jl" target=_blank ><em>raw script</em></a>, <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/ISL-lab-6b.jl" target=_blank ><em>annotated script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=franklin-toc ><ol><li><a href="#getting_started">Getting started</a><ol><li><a href="#data_preparation">Data preparation</a></ol><li><a href="#ridge_pipeline">Ridge pipeline</a><ol><li><a href="#baseline">Baseline</a><li><a href="#basic_ridge">Basic Ridge</a><li><a href="#cross_validating">Cross validating</a></ol><li><a href="#lasso_pipeline">Lasso pipeline</a><li><a href="#elastic_net_pipeline">Elastic net pipeline</a></ol></div><h2 id=getting_started ><a href="#getting_started">Getting started</a></h2> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLJ
<span class=hljs-keyword >import</span> RDatasets: dataset
<span class=hljs-keyword >using</span> PrettyPrinting
<span class=hljs-keyword >import</span> Distributions
<span class=hljs-keyword >const</span> D = Distributions

<span class=hljs-meta >@load</span> LinearRegressor pkg=MLJLinearModels
<span class=hljs-meta >@load</span> RidgeRegressor pkg=MLJLinearModels
<span class=hljs-meta >@load</span> LassoRegressor pkg=MLJLinearModels

hitters = dataset(<span class=hljs-string >"ISLR"</span>, <span class=hljs-string >"Hitters"</span>)
<span class=hljs-meta >@show</span> size(hitters)
names(hitters) |&gt; pprint</code></pre><pre><code class="plaintext hljs">size(hitters) = (322, 20)
["AtBat",
 "Hits",
 "HmRun",
 "Runs",
 "RBI",
 "Walks",
 "Years",
 "CAtBat",
 "CHits",
 "CHmRun",
 "CRuns",
 "CRBI",
 "CWalks",
 "League",
 "Division",
 "PutOuts",
 "Assists",
 "Errors",
 "Salary",
 "NewLeague"]</code></pre> <p>The target is <code>Salary</code> <pre><code class="julia hljs">y, X = unpack(hitters, ==(:Salary), col-&gt;<span class=hljs-literal >true</span>);</code></pre>
<p>It has missing values which we will just ignore:</p>
<pre><code class="julia hljs">no_miss = .!ismissing.(y)
y = collect(skipmissing(y))
X = X[no_miss, :]
train, test = partition(eachindex(y), <span class=hljs-number >0.5</span>, shuffle=<span class=hljs-literal >true</span>, rng=<span class=hljs-number >424</span>);</code></pre>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> PyPlot

figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
plot(y, ls=<span class=hljs-string >"none"</span>, marker=<span class=hljs-string >"o"</span>)

xticks(fontsize=<span class=hljs-number >12</span>); yticks(fontsize=<span class=hljs-number >12</span>)
xlabel(<span class=hljs-string >"Index"</span>, fontsize=<span class=hljs-number >14</span>), ylabel(<span class=hljs-string >"Salary"</span>, fontsize=<span class=hljs-number >14</span>)</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-6b/code/output/ISL-lab-6-g1.svg" alt=Salary >
<p>That looks quite skewed, let&#39;s have a look at a histogram:</p>
<pre><code class="julia hljs">figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
hist(y, bins=<span class=hljs-number >50</span>, density=<span class=hljs-literal >true</span>)

xticks(fontsize=<span class=hljs-number >12</span>); yticks(fontsize=<span class=hljs-number >12</span>)
xlabel(<span class=hljs-string >"Salary"</span>, fontsize=<span class=hljs-number >14</span>); ylabel(<span class=hljs-string >"Density"</span>, fontsize=<span class=hljs-number >14</span>)

edfit = D.fit_mle(D.Exponential, y)
xx = range(minimum(y), <span class=hljs-number >2500</span>, length=<span class=hljs-number >100</span>)
yy = pdf.(edfit, xx)
plot(xx, yy, lw=<span class=hljs-number >3</span>, label=<span class=hljs-string >"Exponential distribution fit"</span>)

legend(fontsize=<span class=hljs-number >12</span>)</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-6b/code/output/ISL-lab-6-g2.svg" alt="Distribution of salary">
<h3 id=data_preparation ><a href="#data_preparation">Data preparation</a></h3>
<p>Most features are currently encoded as integers but we will consider them as continuous</p>
<pre><code class="julia hljs">Xc = coerce(X, autotype(X, rules=(:discrete_to_continuous,)))
scitype(Xc)</code></pre><pre><code class="plaintext hljs">Table{Union{AbstractArray{Continuous,1}, AbstractArray{Multiclass{2},1}}}</code></pre>
<p>There&#39;re a few features that are categorical which we&#39;ll one-hot-encode.</p>
<h2 id=ridge_pipeline ><a href="#ridge_pipeline">Ridge pipeline</a></h2>  <h3 id=baseline ><a href="#baseline">Baseline</a></h3>
<p>Let&#39;s first fit a simple pipeline with a standardizer, a one-hot-encoder and a basic linear regression:</p>
<pre><code class="julia hljs"><span class=hljs-meta >@pipeline</span> RegPipe(std = Standardizer(),
                  hot = OneHotEncoder(),
                  reg = LinearRegressor())

model = RegPipe()
pipe  = machine(model, Xc, y)
fit!(pipe, rows=train)
ŷ = predict(pipe, rows=test)
round(rms(ŷ, y[test])^<span class=hljs-number >2</span>, sigdigits=<span class=hljs-number >4</span>)</code></pre><pre><code class="plaintext hljs">123500.0</code></pre>
<p>Let&#39;s get a feel for how we&#39;re doing</p>
<pre><code class="julia hljs">figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))

res = ŷ .- y[test]
stem(res)

xticks(fontsize=<span class=hljs-number >12</span>); yticks(fontsize=<span class=hljs-number >12</span>)
xlabel(<span class=hljs-string >"Index"</span>, fontsize=<span class=hljs-number >14</span>); ylabel(<span class=hljs-string >"Residual (ŷ - y)"</span>, fontsize=<span class=hljs-number >14</span>)

ylim([-<span class=hljs-number >1300</span>, <span class=hljs-number >1000</span>])</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-6b/code/output/ISL-lab-6-g3.svg" alt=Residuals >
<pre><code class="julia hljs">figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
hist(res, bins=<span class=hljs-number >30</span>, density=<span class=hljs-literal >true</span>, color=<span class=hljs-string >"green"</span>)

xx = range(-<span class=hljs-number >1100</span>, <span class=hljs-number >1100</span>, length=<span class=hljs-number >100</span>)
ndfit = D.fit_mle(D.Normal, res)
lfit  = D.fit_mle(D.Laplace, res)

plot(xx, pdf.(ndfit, xx), lw=<span class=hljs-number >3</span>, color=<span class=hljs-string >"orange"</span>, label=<span class=hljs-string >"Normal fit"</span>)
plot(xx, pdf.(lfit, xx), lw=<span class=hljs-number >3</span>, color=<span class=hljs-string >"magenta"</span>, label=<span class=hljs-string >"Laplace fit"</span>)

legend(fontsize=<span class=hljs-number >12</span>)

xticks(fontsize=<span class=hljs-number >12</span>); yticks(fontsize=<span class=hljs-number >12</span>)
xlabel(<span class=hljs-string >"Residual (ŷ - y)"</span>, fontsize=<span class=hljs-number >14</span>); ylabel(<span class=hljs-string >"Density"</span>, fontsize=<span class=hljs-number >14</span>)
xlim([-<span class=hljs-number >1100</span>, <span class=hljs-number >1100</span>])</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-6b/code/output/ISL-lab-6-g4.svg" alt="Distribution of residuals">
<h3 id=basic_ridge ><a href="#basic_ridge">Basic Ridge</a></h3>
<p>Let&#39;s now swap the linear regressor for a Ridge one without specifying the penalty &#40;<code>1</code> by default&#41;:</p>
<pre><code class="julia hljs">pipe.model.reg = RidgeRegressor()
fit!(pipe, rows=train)
ŷ = predict(pipe, rows=test)
round(rms(ŷ, y[test])^<span class=hljs-number >2</span>, sigdigits=<span class=hljs-number >4</span>)</code></pre><pre><code class="plaintext hljs">109600.0</code></pre>
<p>Ok that&#39;s a bit better but surely we can do better with an appropriate selection of the hyperparameter.</p>
<h3 id=cross_validating ><a href="#cross_validating">Cross validating</a></h3>
<p>What penalty should you use? Let&#39;s do a simple CV to try  to find out:</p>
<pre><code class="julia hljs">r  = range(model, :(reg.lambda), lower=<span class=hljs-number >1e-2</span>, upper=<span class=hljs-number >100_000</span>, scale=:log10)
tm = TunedModel(model=model, ranges=r, tuning=Grid(resolution=<span class=hljs-number >50</span>),
                resampling=CV(nfolds=<span class=hljs-number >3</span>, rng=<span class=hljs-number >4141</span>), measure=rms)
mtm = machine(tm, Xc, y)
fit!(mtm, rows=train)

best_mdl = fitted_params(mtm).best_model
round(best_mdl.reg.lambda, sigdigits=<span class=hljs-number >4</span>)</code></pre><pre><code class="plaintext hljs">3.728</code></pre>
<p>right, and  with that we get:</p>
<pre><code class="julia hljs">ŷ = predict(mtm, rows=test)
round(rms(ŷ, y[test])^<span class=hljs-number >2</span>, sigdigits=<span class=hljs-number >4</span>)</code></pre><pre><code class="plaintext hljs">98880.0</code></pre>
<p>Let&#39;s see:</p>
<pre><code class="julia hljs">figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))

res = ŷ .- y[test]
stem(res)

xticks(fontsize=<span class=hljs-number >12</span>); yticks(fontsize=<span class=hljs-number >12</span>)
xlabel(<span class=hljs-string >"Index"</span>, fontsize=<span class=hljs-number >14</span>);
ylabel(<span class=hljs-string >"Residual (ŷ - y)"</span>, fontsize=<span class=hljs-number >14</span>)
xlim(<span class=hljs-number >1</span>, length(res))

ylim([-<span class=hljs-number >1300</span>, <span class=hljs-number >1000</span>])</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-6b/code/output/ISL-lab-6-g5.svg" alt="Ridge residuals">
<p>You can compare that with the residuals obtained earlier.</p>
<h2 id=lasso_pipeline ><a href="#lasso_pipeline">Lasso pipeline</a></h2>
<p>Let&#39;s do the same as above but using a Lasso model and adjusting the range a bit:</p>
<pre><code class="julia hljs">mtm.model.model.reg = LassoRegressor()
mtm.model.range = range(model, :(reg.lambda), lower=<span class=hljs-number >500</span>, upper=<span class=hljs-number >100_000</span>, scale=:log10)
fit!(mtm, rows=train)

best_mdl = fitted_params(mtm).best_model
round(best_mdl.reg.lambda, sigdigits=<span class=hljs-number >4</span>)</code></pre><pre><code class="plaintext hljs">3501.0</code></pre>
<p>Ok and let&#39;s see how that does:</p>
<pre><code class="julia hljs">ŷ = predict(mtm, rows=test)
round(rms(ŷ, y[test])^<span class=hljs-number >2</span>, sigdigits=<span class=hljs-number >4</span>)</code></pre><pre><code class="plaintext hljs">100900.0</code></pre>
<p>Pretty good&#33; and the parameters are reasonably sparse as expected:</p>
<pre><code class="julia hljs">coefs, intercept = fitted_params(mtm.fitresult.fitresult.machine)
<span class=hljs-meta >@show</span> coefs
<span class=hljs-meta >@show</span> intercept</code></pre><pre><code class="plaintext hljs">coefs = [:AtBat =&gt; 0.0, :Hits =&gt; 0.0, :HmRun =&gt; 0.0, :Runs =&gt; 80.6678178141269, :RBI =&gt; 0.0, :Walks =&gt; 37.03471312303758, :Years =&gt; 0.0, :CAtBat =&gt; 0.0, :CHits =&gt; 145.20304491814704, :CHmRun =&gt; 0.0, :CRuns =&gt; 0.0, :CRBI =&gt; 28.78708238416367, :CWalks =&gt; 0.0, :League__A =&gt; -0.0, :League__N =&gt; 0.0, :Division__E =&gt; 33.39622922758369, :Division__W =&gt; -5.6903059369871, :PutOuts =&gt; 58.494282041618256, :Assists =&gt; 0.0, :Errors =&gt; -0.0, :NewLeague__A =&gt; -0.0, :NewLeague__N =&gt; 0.0]
intercept = 529.9504952264205
</code></pre>
<p>with around 50&#37; sparsity:</p>
<pre><code class="julia hljs">coef_vals = [c[<span class=hljs-number >2</span>] <span class=hljs-keyword >for</span> c <span class=hljs-keyword >in</span> coefs]
sum(coef_vals .≈ <span class=hljs-number >0</span>) / length(coefs)</code></pre><pre><code class="plaintext hljs">0.6818181818181818</code></pre>
<p>Let&#39;s visualise this:</p>
<pre><code class="julia hljs">figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
stem(coefs)

<span class=hljs-comment ># name of the features including one-hot-encoded ones</span>
all_names = [:AtBat, :Hits, :HmRun, :Runs, :RBI, :Walks, :Years,
             :CAtBat, :CHits, :CHmRun, :CRuns, :CRBI, :CWalks,
             :League__A, :League__N, :Div_E, :Div_W,
             :PutOuts, :Assists, :Errors, :NewLeague_A, :NewLeague_N]

idxshow = collect(<span class=hljs-number >1</span>:length(coefs))[abs.(coefs) .&gt; <span class=hljs-number >10</span>]
xticks(idxshow .- <span class=hljs-number >1</span>, all_names[idxshow], rotation=<span class=hljs-number >45</span>, fontsize=<span class=hljs-number >12</span>)
yticks(fontsize=<span class=hljs-number >12</span>)
ylabel(<span class=hljs-string >"Amplitude"</span>, fontsize=<span class=hljs-number >14</span>)</code></pre><pre><code class="plaintext hljs">PyError ($(Expr(:escape, :(ccall(#= /Users/tlienart/.julia/packages/PyCall/zqDXB/src/pyfncall.jl:43 =# @pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, pyargsptr, kw))))) &lt;class 'TypeError'&gt;
TypeError("unhashable type: 'numpy.ndarray'")
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/pyplot.py", line 2926, in stem
    None else {}))
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/__init__.py", line 1810, in inner
    return func(ax, *args, **kwargs)
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/axes/_axes.py", line 2678, in stem
    marker=markermarker, label="_nolegend_")
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/__init__.py", line 1810, in inner
    return func(ax, *args, **kwargs)
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/axes/_axes.py", line 1611, in plot
    for line in self._get_lines(*args, **kwargs):
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/axes/_base.py", line 393, in _grab_next_args
    yield from self._plot_args(this, kwargs)
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/axes/_base.py", line 370, in _plot_args
    x, y = self._xy_from_xy(x, y)
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/axes/_base.py", line 205, in _xy_from_xy
    by = self.axes.yaxis.update_units(y)
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/axis.py", line 1473, in update_units
    default = self.converter.default_units(data, self)
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/category.py", line 103, in default_units
    axis.set_units(UnitData(data))
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/category.py", line 169, in __init__
    self.update(data)
  File "/Users/tlienart/.julia/conda/3/lib/python3.7/site-packages/matplotlib/category.py", line 186, in update
    for val in OrderedDict.fromkeys(data):

</code></pre>
<p><span style="color:red;">// Image matching '/assets/isl/lab-6b/code/ISL-lab-6-g6.svg' not found. //</span></p>
<h2 id=elastic_net_pipeline ><a href="#elastic_net_pipeline">Elastic net pipeline</a></h2>
<pre><code class="julia hljs"><span class=hljs-meta >@load</span> ElasticNetRegressor pkg=MLJLinearModels

mtm.model.model.reg = ElasticNetRegressor()
mtm.model.range = [range(model, :(reg.lambda), lower=<span class=hljs-number >0.1</span>, upper=<span class=hljs-number >100</span>, scale=:log10),
                    range(model, :(reg.gamma),  lower=<span class=hljs-number >500</span>, upper=<span class=hljs-number >10_000</span>, scale=:log10)]
mtm.model.tuning = Grid(resolution=<span class=hljs-number >10</span>)
fit!(mtm, rows=train)

best_mdl = fitted_params(mtm).best_model
<span class=hljs-meta >@show</span> round(best_mdl.reg.lambda, sigdigits=<span class=hljs-number >4</span>)
<span class=hljs-meta >@show</span> round(best_mdl.reg.gamma, sigdigits=<span class=hljs-number >4</span>)</code></pre><pre><code class="plaintext hljs">round(best_mdl.reg.lambda, sigdigits = 4) = 2.154
round(best_mdl.reg.gamma, sigdigits = 4) = 2641.0
</code></pre>
<p>And it&#39;s not too bad in terms of accuracy either</p>
<pre><code class="julia hljs">ŷ = predict(mtm, rows=test)
round(rms(ŷ, y[test])^<span class=hljs-number >2</span>, sigdigits=<span class=hljs-number >4</span>)</code></pre><pre><code class="plaintext hljs">98230.0</code></pre>
<p>But the simple ridge regression seems to work best here.</p>
<div class=page-foot >
  <div class=copyright >
    &copy; Thibaut Lienart, Anthony Blaom and collaborators. Last modified: May 24, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div>
      </div> 
  </div> 
  <script src="/DataScienceTutorials.jl/libs/pure/ui.min.js"></script>