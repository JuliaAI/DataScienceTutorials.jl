<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <!-- Syntax highlighting via Prism, note: restricted langs -->
<link rel="stylesheet" href="/DataScienceTutorials.jl/libs/highlight/github.min.css">
 
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/franklin.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/pure.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/side-menu.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/extra.css">
  <!-- <link rel="icon" href="/DataScienceTutorials.jl/assets/infra/favicon.gif"> -->
   <title>Crabs with XGBoost</title>  
  <!-- LUNR -->
  <script src="/DataScienceTutorials.jl/libs/lunr/lunr.min.js"></script>
  <script src="/DataScienceTutorials.jl/libs/lunr/lunr_index.js"></script>
  <script src="/DataScienceTutorials.jl/libs/lunr/lunrclient.min.js"></script>
</head>
<body>
  <div id="layout">
    <!-- Menu toggle / hamburger icon -->
    <a href="#menu" id="menuLink" class="menu-link"><span></span></a>
    <div id="menu">
      <div class="pure-menu">
        <a href="/DataScienceTutorials.jl/" id="menu-logo-link">
          <div class="menu-logo">
            <!-- <img id="menu-logo" alt="MLJ Logo" src="/DataScienceTutorials.jl/assets/infra/MLJLogo2.svg" /> -->
            <p><strong>Data Science Tutorials</strong></p>
          </div>
        </a>
        <form id="lunrSearchForm" name="lunrSearchForm">
          <input class="search-input" name="q" placeholder="Enter search term" type="text">
          <input type="submit" value="Search" formaction="/DataScienceTutorials.jl/search/index.html" style="visibility:hidden">
        </form>
  <!-- LIST OF MENU ITEMS -->
  <ul class="pure-menu-list">
    <li class="pure-menu-item pure-menu-top-item "><a href="/DataScienceTutorials.jl/" class="pure-menu-link"><strong>Home</strong></a></li>

    <!-- DATA BASICS -->
    <li class="pure-menu-sublist-title"><strong>Data basics</strong></li>
    <ul class="pure-menu-sublist">
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/loading/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Loading data</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/dataframe/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data Frames</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/categorical/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/scitype/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Scientific Type</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/processing/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data processing</a></li>
    </ul>

    <!-- GETTING STARTED WITH MLJ -->
    <li class="pure-menu-sublist-title"><strong>Getting started</strong></li>
    <ul class="pure-menu-sublist">
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Choosing a model</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/fit-and-predict/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/model-tuning/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Model tuning</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-3/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/composing-models/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Composing models</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Learning networks</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/stacking/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Stacking</a></li>
    </ul>

    <!-- INTRO TO STATS LEARNING -->
    <li class="pure-menu-sublist-title"><strong>Intro to Stats Learning</strong></li>
    <ul class="pure-menu-sublist" id=isl>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 2</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-3/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 3</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-4/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 4</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-5/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 5</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-6b/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 6b</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-8/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 8</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-9/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 9</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-10/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 10</a></li>
    </ul>

    <!-- END TO END EXAMPLES -->
    <li class="pure-menu-sublist-title"><strong>End to end examples</strong></li>
    <ul class="pure-menu-sublist" id=e2e>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/AMES/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> AMES</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/wine/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Wine</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/crabs-xgb/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/horse/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Horse</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/HouseKingCounty/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> King County Houses</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/airfoil" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Airfoil </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-lgbm" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/glm/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Using GLM.jl </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/powergen/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Power Generation </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-flux" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Boston (Flux) </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/breastcancer" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Breast Cancer</a></li>
    </ul>
  </ul>
  <!-- END OF LIST OF MENU ITEMS -->
      </div>
    </div>
    <div id="main"> <!-- Closed in foot -->
      

<!-- Content appended here -->
<div class="franklin-content"><h1 id="crabs_with_xgboost"><a href="#crabs_with_xgboost" class="header-anchor">Crabs with XGBoost</a></h1>
<em>Download the 
  <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-crabs-xgb/tutorial.ipynb" target="_blank"><em>notebook</em></a>
  , the 
  <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-crabs-xgb/tutorial.jl" target="_blank"><em>annotated script</em></a>
   or the 
  <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-crabs-xgb/tutorial-raw.jl" target="_blank"><em>raw script</em></a>
   for this tutorial &#40;right-click on the relevant link and save-as&#41;. These rely on <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-crabs-xgb/Project.toml">this Project.toml</a> and <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-crabs-xgb/Manifest.toml">this Manifest.toml</a>.</em> <br/>   <em>You can also download the whole <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-crabs-xgb.tar.gz">project folder</a>.</em></p>
<p><em>If you have questions or suggestions about this tutorial, please open an issue <a href="https://github.com/JuliaAI/DataScienceTutorials.jl/issues/new">here</a>.</em></p>
<p><div class="franklin-toc"><ol><li><a href="#first_steps">First steps</a></li><li><a href="#xgboost_machine">XGBoost machine</a><ol><li><a href="#more_tuning_1">More tuning &#40;1&#41;</a></li><li><a href="#more_tuning_2">More tuning &#40;2&#41;</a></li><li><a href="#more_tuning_3">More tuning &#40;3&#41;</a></li></ol></li></ol></div>
<p>This example is inspired from <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">this post</a> showing how to use XGBoost.</p>
<h2 id="first_steps"><a href="#first_steps" class="header-anchor">First steps</a></h2>
<p>Again, the crabs dataset is so common that there is a  simple load function for it:</p>
<pre><code class="language-julia">using MLJ
using StatsBase
using Random
using PyPlot
using CategoricalArrays
using PrettyPrinting
import DataFrames

X, y &#61; @load_crabs
X &#61; DataFrames.DataFrame&#40;X&#41;
@show size&#40;X&#41;
@show y&#91;1:3&#93;
first&#40;X, 3&#41; |&gt; pretty</code></pre><pre><code class="plaintext code-output">size(X) = (200, 5)
y[1:3] = CategoricalArrays.CategoricalValue{String, UInt32}["B", "B", "B"]
┌────────────┬────────────┬────────────┬────────────┬────────────┐
│ FL         │ RW         │ CL         │ CW         │ BD         │
│ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │
│ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │
├────────────┼────────────┼────────────┼────────────┼────────────┤
│ 8.1        │ 6.7        │ 16.1       │ 19.0       │ 7.0        │
│ 8.8        │ 7.7        │ 18.1       │ 20.8       │ 7.4        │
│ 9.2        │ 7.8        │ 19.0       │ 22.4       │ 7.7        │
└────────────┴────────────┴────────────┴────────────┴────────────┘
</code></pre>
<p>It&#39;s a classification problem with the following classes:</p>
<pre><code class="language-julia">levels&#40;y&#41; |&gt; pprint</code></pre><pre><code class="plaintext code-output">["B", "O"]</code></pre>
<p>Note that the dataset is currently sorted by target, let&#39;s shuffle it to avoid the obvious issues this may cause</p>
<pre><code class="language-julia">Random.seed&#33;&#40;523&#41;
perm &#61; randperm&#40;length&#40;y&#41;&#41;
X &#61; X&#91;perm,:&#93;
y &#61; y&#91;perm&#93;;</code></pre>
<p>It&#39;s not a very big dataset so we will likely overfit it badly using something as sophisticated as XGBoost but it will do for a demonstration.</p>
<pre><code class="language-julia">train, test &#61; partition&#40;collect&#40;eachindex&#40;y&#41;&#41;, 0.70, shuffle&#61;true, rng&#61;52&#41;
XGBC &#61; @load XGBoostClassifier
xgb_model &#61; XGBC&#40;&#41;</code></pre><pre><code class="plaintext code-output">import MLJXGBoostInterface ✔
XGBoostClassifier(
    num_round = 100,
    booster = "gbtree",
    disable_default_eval_metric = 0,
    eta = 0.3,
    gamma = 0.0,
    max_depth = 6,
    min_child_weight = 1.0,
    max_delta_step = 0.0,
    subsample = 1.0,
    colsample_bytree = 1.0,
    colsample_bylevel = 1.0,
    lambda = 1.0,
    alpha = 0.0,
    tree_method = "auto",
    sketch_eps = 0.03,
    scale_pos_weight = 1.0,
    updater = "auto",
    refresh_leaf = 1,
    process_type = "default",
    grow_policy = "depthwise",
    max_leaves = 0,
    max_bin = 256,
    predictor = "cpu_predictor",
    sample_type = "uniform",
    normalize_type = "tree",
    rate_drop = 0.0,
    one_drop = 0,
    skip_drop = 0.0,
    feature_selector = "cyclic",
    top_k = 0,
    tweedie_variance_power = 1.5,
    objective = "automatic",
    base_score = 0.5,
    eval_metric = "mlogloss",
    seed = 0,
    nthread = 1)</code></pre>
<p>Let&#39;s check whether the training and  is balanced, <code>StatsBase.countmap</code> is useful for that:</p>
<pre><code class="language-julia">countmap&#40;y&#91;train&#93;&#41; |&gt; pprint</code></pre><pre><code class="plaintext code-output">Dict(CategoricalArrays.CategoricalValue{String, UInt32} "B" => 62,
     CategoricalArrays.CategoricalValue{String, UInt32} "O" => 78)</code></pre>
<p>which is pretty balanced. You could check the same on the test set and full set and the same comment would still hold.</p>
<h2 id="xgboost_machine"><a href="#xgboost_machine" class="header-anchor">XGBoost machine</a></h2>
<p>Wrap a machine around an XGBoost model &#40;XGB&#41; and the data:</p>
<pre><code class="language-julia">xgb  &#61; XGBC&#40;&#41;
xgbm &#61; machine&#40;xgb, X, y&#41;</code></pre><pre><code class="plaintext code-output">Machine{XGBoostClassifier,…} trained 0 times; caches data
  model: MLJXGBoostInterface.XGBoostClassifier
  args: 
    1:	Source @581 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`
    2:	Source @090 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{2}}`
</code></pre>
<p>We will tune it varying the number of rounds used and generate a learning curve</p>
<pre><code class="language-julia">r &#61; range&#40;xgb, :num_round, lower&#61;50, upper&#61;500&#41;
curve &#61; learning_curve&#40;xgbm, range&#61;r, resolution&#61;50,
                        measure&#61;L1HingeLoss&#40;&#41;&#41;</code></pre><pre><code class="plaintext code-output">(parameter_name = "num_round",
 parameter_scale = :linear,
 parameter_values = [50, 59, 68, 78, 87, 96, 105, 114, 123, 133, 142, 151, 160, 169, 179, 188, 197, 206, 215, 224, 234, 243, 252, 261, 270, 280, 289, 298, 307, 316, 326, 335, 344, 353, 362, 371, 381, 390, 399, 408, 417, 427, 436, 445, 454, 463, 472, 482, 491, 500],
 measurements = [0.24836397171020508, 0.24545636773109436, 0.24268636107444763, 0.2397555410861969, 0.23826450109481812, 0.2352793961763382, 0.23448707163333893, 0.23310495913028717, 0.2308061718940735, 0.22965149581432343, 0.22910834848880768, 0.22753143310546875, 0.22674864530563354, 0.22541509568691254, 0.22454336285591125, 0.22389936447143555, 0.22321505844593048, 0.2226034253835678, 0.22247187793254852, 0.22193248569965363, 0.22118815779685974, 0.2205360233783722, 0.2201593965291977, 0.2195645123720169, 0.21914902329444885, 0.21856757998466492, 0.2184644341468811, 0.21800978481769562, 0.2175247222185135, 0.2173323929309845, 0.21716448664665222, 0.2170332968235016, 0.2167222946882248, 0.2163572609424591, 0.21602852642536163, 0.21589162945747375, 0.21549758315086365, 0.2153221219778061, 0.21519102156162262, 0.21479657292366028, 0.21469055116176605, 0.21440592408180237, 0.21408171951770782, 0.2139209806919098, 0.213658407330513, 0.2135016769170761, 0.2134832739830017, 0.21321788430213928, 0.21296538412570953, 0.21279016137123108],)</code></pre>
<p>Let&#39;s have a look</p>
<pre><code class="language-julia">figure&#40;figsize&#61;&#40;8,6&#41;&#41;
plot&#40;curve.parameter_values, curve.measurements&#41;
xlabel&#40;&quot;Number of rounds&quot;, fontsize&#61;14&#41;
ylabel&#40;&quot;HingeLoss&quot;, fontsize&#61;14&#41;
xticks&#40;&#91;10, 100, 200, 500&#93;, fontsize&#61;12&#41;</code></pre>
<img src="/DataScienceTutorials.jl/assets/end-to-end/crabs-xgb/code/output/EX-crabs-xgb-curve1.svg" alt="Cross entropy vs Num Round">
<p>So, in short, using more rounds helps. Let&#39;s arbitrarily fix it to 200.</p>
<pre><code class="language-julia">xgb.num_round &#61; 200;</code></pre>
<h3 id="more_tuning_1"><a href="#more_tuning_1" class="header-anchor">More tuning &#40;1&#41;</a></h3>
<p>Let&#39;s now tune the maximum depth of each tree and the minimum child weight in the boosting.</p>
<pre><code class="language-julia">r1 &#61; range&#40;xgb, :max_depth, lower&#61;3, upper&#61;10&#41;
r2 &#61; range&#40;xgb, :min_child_weight, lower&#61;0, upper&#61;5&#41;

tm &#61; TunedModel&#40;model&#61;xgb, tuning&#61;Grid&#40;resolution&#61;8&#41;,
                resampling&#61;CV&#40;rng&#61;11&#41;, ranges&#61;&#91;r1,r2&#93;,
                measure&#61;cross_entropy&#41;
mtm &#61; machine&#40;tm, X, y&#41;
fit&#33;&#40;mtm, rows&#61;train&#41;</code></pre><pre><code class="plaintext code-output">Machine{ProbabilisticTunedModel{Grid,…},…} trained 1 time; caches data
  model: MLJTuning.ProbabilisticTunedModel{MLJTuning.Grid, MLJXGBoostInterface.XGBoostClassifier}
  args: 
    1:	Source @467 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`
    2:	Source @177 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{2}}`
</code></pre>
<p>Great, as always we can investigate the tuning by using <code>report</code> and can, for instance, plot a heatmap of the measurements:</p>
<pre><code class="language-julia">r &#61; report&#40;mtm&#41;

res &#61; r.plotting

md &#61; res.parameter_values&#91;:,1&#93;
mcw &#61; res.parameter_values&#91;:,2&#93;

figure&#40;figsize&#61;&#40;8,6&#41;&#41;
tricontourf&#40;md, mcw, res.measurements&#41;

xlabel&#40;&quot;Maximum tree depth&quot;, fontsize&#61;14&#41;
ylabel&#40;&quot;Minimum child weight&quot;, fontsize&#61;14&#41;
xticks&#40;3:2:10, fontsize&#61;12&#41;
yticks&#40;fontsize&#61;12&#41;</code></pre>
<img src="/DataScienceTutorials.jl/assets/end-to-end/crabs-xgb/code/output/EX-crabs-xgb-heatmap.svg" alt="Hyperparameter heatmap">
<p>Let&#39;s extract the optimal model and inspect its parameters:</p>
<pre><code class="language-julia">xgb &#61; fitted_params&#40;mtm&#41;.best_model
@show xgb.max_depth
@show xgb.min_child_weight</code></pre><pre><code class="plaintext code-output">xgb.max_depth = 7
xgb.min_child_weight = 1.4285714285714286
</code></pre>
<h3 id="more_tuning_2"><a href="#more_tuning_2" class="header-anchor">More tuning &#40;2&#41;</a></h3>
<p>Let&#39;s examine the effect of <code>gamma</code>:</p>
<pre><code class="language-julia">xgbm &#61; machine&#40;xgb, X, y&#41;
r &#61; range&#40;xgb, :gamma, lower&#61;0, upper&#61;10&#41;
curve &#61; learning_curve&#33;&#40;xgbm, range&#61;r, resolution&#61;30,
                        measure&#61;cross_entropy&#41;;</code></pre>
<p>it looks like the <code>gamma</code> parameter substantially affects model performance:</p>
<pre><code class="language-julia">@show round&#40;minimum&#40;curve.measurements&#41;, sigdigits&#61;3&#41;
@show round&#40;maximum&#40;curve.measurements&#41;, sigdigits&#61;3&#41;</code></pre><pre><code class="plaintext code-output">round(minimum(curve.measurements), sigdigits = 3) = 0.226
round(maximum(curve.measurements), sigdigits = 3) = 0.475
</code></pre>
<h3 id="more_tuning_3"><a href="#more_tuning_3" class="header-anchor">More tuning &#40;3&#41;</a></h3>
<p>Let&#39;s examine the effect of <code>subsample</code> and <code>colsample_bytree</code>:</p>
<pre><code class="language-julia">r1 &#61; range&#40;xgb, :subsample, lower&#61;0.6, upper&#61;1.0&#41;
r2 &#61; range&#40;xgb, :colsample_bytree, lower&#61;0.6, upper&#61;1.0&#41;
tm &#61; TunedModel&#40;model&#61;xgb, tuning&#61;Grid&#40;resolution&#61;8&#41;,
                resampling&#61;CV&#40;rng&#61;234&#41;, ranges&#61;&#91;r1,r2&#93;,
                measure&#61;cross_entropy&#41;
mtm &#61; machine&#40;tm, X, y&#41;
fit&#33;&#40;mtm, rows&#61;train&#41;</code></pre><pre><code class="plaintext code-output">Machine{ProbabilisticTunedModel{Grid,…},…} trained 1 time; caches data
  model: MLJTuning.ProbabilisticTunedModel{MLJTuning.Grid, MLJXGBoostInterface.XGBoostClassifier}
  args: 
    1:	Source @954 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`
    2:	Source @072 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{2}}`
</code></pre>
<p>and the usual procedure to visualise it:</p>
<pre><code class="language-julia">r &#61; report&#40;mtm&#41;

res &#61; r.plotting

ss &#61; res.parameter_values&#91;:,1&#93;
cbt &#61; res.parameter_values&#91;:,2&#93;

figure&#40;figsize&#61;&#40;8,6&#41;&#41;
tricontourf&#40;ss, cbt, res.measurements&#41;

xlabel&#40;&quot;Sub sample&quot;, fontsize&#61;14&#41;
ylabel&#40;&quot;Col sample by tree&quot;, fontsize&#61;14&#41;
xticks&#40;fontsize&#61;12&#41;
yticks&#40;fontsize&#61;12&#41;</code></pre>
<img src="/DataScienceTutorials.jl/assets/end-to-end/crabs-xgb/code/output/EX-crabs-xgb-heatmap2.svg" alt="Hyperparameter heatmap">
<p>Let&#39;s retrieve the best models:</p>
<pre><code class="language-julia">xgb &#61; fitted_params&#40;mtm&#41;.best_model
@show xgb.subsample
@show xgb.colsample_bytree</code></pre><pre><code class="plaintext code-output">xgb.subsample = 0.7142857142857143
xgb.colsample_bytree = 1.0
</code></pre>
<p>We could continue with more fine tuning but given how small the dataset is, it doesn&#39;t make much sense. How does it fare on the test set?</p>
<pre><code class="language-julia">ŷ &#61; predict_mode&#40;mtm, rows&#61;test&#41;
round&#40;accuracy&#40;ŷ, y&#91;test&#93;&#41;, sigdigits&#61;3&#41;</code></pre><pre><code class="plaintext code-output">0.9</code></pre>

<div class="page-foot">
  <div class="copyright">
    &copy; Thibaut Lienart, Anthony Blaom, Sebastian Vollmer and collaborators. Last modified: August 09, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
      </div> <!-- end of id=main -->
  </div> <!-- end of id=layout -->
  <script src="/DataScienceTutorials.jl/libs/pure/ui.min.js"></script>
  
  
      <script src="/DataScienceTutorials.jl/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

  
</body>
</html>
