<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <!-- Syntax highlighting via Prism, note: restricted langs -->
<link rel="stylesheet" href="/libs/highlight/github.min.css">
 
  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet" href="/css/pure.css">
  <link rel="stylesheet" href="/css/side-menu.css">
  <link rel="stylesheet" href="/css/extra.css">
  <!-- <link rel="icon" href="/assets/infra/favicon.gif"> -->
   <title>Boston with Flux</title>  
  <!-- LUNR -->
  <script src="/libs/lunr/lunr.min.js"></script>
  <script src="/libs/lunr/lunr_index.js"></script>
  <script src="/libs/lunr/lunrclient.min.js"></script>
</head>
<body>
  <div id="layout">
    <!-- Menu toggle / hamburger icon -->
    <a href="#menu" id="menuLink" class="menu-link"><span></span></a>
    <div id="menu">
      <div class="pure-menu">
        <a href="/" id="menu-logo-link">
          <div class="menu-logo">
            <!-- <img id="menu-logo" alt="MLJ Logo" src="/assets/infra/MLJLogo2.svg" /> -->
            <p><strong>Data Science Tutorials</strong></p>
          </div>
        </a>
        <form id="lunrSearchForm" name="lunrSearchForm">
          <input class="search-input" name="q" placeholder="Enter search term" type="text">
          <input type="submit" value="Search" formaction="/search/index.html" style="visibility:hidden">
        </form>
  <!-- LIST OF MENU ITEMS -->
  <ul class="pure-menu-list">
    <li class="pure-menu-item pure-menu-top-item "><a href="/" class="pure-menu-link"><strong>Home</strong></a></li>

    <!-- DATA BASICS -->
    <li class="pure-menu-sublist-title"><strong>Data basics</strong></li>
    <ul class="pure-menu-sublist">
      <li class="pure-menu-item "><a href="/data/loading/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Loading data</a></li>
      <li class="pure-menu-item "><a href="/data/dataframe/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data Frames</a></li>
      <li class="pure-menu-item "><a href="/data/categorical/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a></li>
      <li class="pure-menu-item "><a href="/data/scitype/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Scientific Type</a></li>
      <li class="pure-menu-item "><a href="/data/processing/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data processing</a></li>
    </ul>

    <!-- GETTING STARTED WITH MLJ -->
    <li class="pure-menu-sublist-title"><strong>Getting started</strong></li>
    <ul class="pure-menu-sublist">
      <li class="pure-menu-item "><a href="/getting-started/choosing-a-model/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Choosing a model</a></li>
      <li class="pure-menu-item "><a href="/getting-started/fit-and-predict/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a></li>
      <li class="pure-menu-item "><a href="/getting-started/model-tuning/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Model tuning</a></li>
      <li class="pure-menu-item "><a href="/getting-started/ensembles/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles</a></li>
      <li class="pure-menu-item "><a href="/getting-started/ensembles-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a></li>
      <li class="pure-menu-item "><a href="/getting-started/ensembles-3/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a></li>
      <li class="pure-menu-item "><a href="/getting-started/composing-models/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Composing models</a></li>
      <li class="pure-menu-item "><a href="/getting-started/learning-networks/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Learning networks</a></li>
      <li class="pure-menu-item "><a href="/getting-started/learning-networks-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a></li>
      <li class="pure-menu-item "><a href="/getting-started/stacking/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Stacking</a></li>
    </ul>

    <!-- INTRO TO STATS LEARNING -->
    <li class="pure-menu-sublist-title"><strong>Intro to Stats Learning</strong></li>
    <ul class="pure-menu-sublist" id=isl>
      <li class="pure-menu-item "><a href="/isl/lab-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 2</a></li>
      <li class="pure-menu-item "><a href="/isl/lab-3/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 3</a></li>
      <li class="pure-menu-item "><a href="/isl/lab-4/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 4</a></li>
      <li class="pure-menu-item "><a href="/isl/lab-5/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 5</a></li>
      <li class="pure-menu-item "><a href="/isl/lab-6b/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 6b</a></li>
      <li class="pure-menu-item "><a href="/isl/lab-8/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 8</a></li>
      <li class="pure-menu-item "><a href="/isl/lab-9/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 9</a></li>
      <li class="pure-menu-item "><a href="/isl/lab-10/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 10</a></li>
    </ul>

    <!-- END TO END EXAMPLES -->
    <li class="pure-menu-sublist-title"><strong>End to end examples</strong></li>
    <ul class="pure-menu-sublist" id=e2e>
      <li class="pure-menu-item "><a href="/end-to-end/AMES/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> AMES</a></li>
      <li class="pure-menu-item "><a href="/end-to-end/wine/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Wine</a></li>
      <li class="pure-menu-item "><a href="/end-to-end/crabs-xgb/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a></li>
      <li class="pure-menu-item "><a href="/end-to-end/horse/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Horse</a></li>
      <li class="pure-menu-item "><a href="/end-to-end/HouseKingCounty/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> King County Houses</a></li>
      <li class="pure-menu-item "><a href="/end-to-end/airfoil" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Airfoil </a></li>
      <li class="pure-menu-item "><a href="/end-to-end/boston-lgbm" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a></li>
      <li class="pure-menu-item "><a href="/end-to-end/glm/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Using GLM.jl </a></li>
      <li class="pure-menu-item "><a href="/end-to-end/powergen/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Power Generation </a></li>
      <li class="pure-menu-item "><a href="/end-to-end/boston-flux" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Boston (Flux) </a></li>
    </ul>
  </ul>
  <!-- END OF LIST OF MENU ITEMS -->
      </div>
    </div>
    <div id="main"> <!-- Closed in foot -->
      

<!-- Content appended here -->
<div class="franklin-content"><h1 id="boston_with_flux"><a href="#boston_with_flux" class="header-anchor">Boston with Flux</a></h1>
<div class="franklin-toc"><ol><li><a href="#getting_started">Getting started</a></li><li><a href="#tuning">Tuning</a></li></ol></div>
<p><strong>Main author</strong>: Ayush Shridhar &#40;ayush-1506&#41;.</p>
<h2 id="getting_started"><a href="#getting_started" class="header-anchor">Getting started</a></h2>
<pre><code class="language-julia">import MLJFlux
import MLJ
import DataFrames: DataFrame
import Statistics
import Flux
using Random
using PyPlot

Random.seed&#33;&#40;11&#41;</code></pre><pre><code class="plaintext code-output">MersenneTwister(11)</code></pre>
<p>Loading the Boston dataset. Our aim will be to implement a neural network regressor to predict the price of a house, given a number of features.</p>
<pre><code class="language-julia">features, targets &#61; MLJ.@load_boston
features &#61; DataFrame&#40;features&#41;
@show size&#40;features&#41;
@show targets&#91;1:3&#93;
first&#40;features, 3&#41; |&gt; MLJ.pretty</code></pre><pre><code class="plaintext code-output">size(features) = (506, 12)
targets[1:3] = [24.0, 21.6, 34.7]
┌────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┐
│ Crim       │ Zn         │ Indus      │ NOx        │ Rm         │ Age        │ Dis        │ Rad        │ Tax        │ PTRatio    │ Black      │ LStat      │
│ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │
│ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │
├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤
│ 0.00632    │ 18.0       │ 2.31       │ 0.538      │ 6.575      │ 65.2       │ 4.09       │ 1.0        │ 296.0      │ 15.3       │ 396.9      │ 4.98       │
│ 0.02731    │ 0.0        │ 7.07       │ 0.469      │ 6.421      │ 78.9       │ 4.9671     │ 2.0        │ 242.0      │ 17.8       │ 396.9      │ 9.14       │
│ 0.02729    │ 0.0        │ 7.07       │ 0.469      │ 7.185      │ 61.1       │ 4.9671     │ 2.0        │ 242.0      │ 17.8       │ 392.83     │ 4.03       │
└────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┘
</code></pre>
<p>Next obvious steps: partitioning into train and test set</p>
<pre><code class="language-julia">train, test &#61; partition&#40;collect&#40;eachindex&#40;targets&#41;&#41;, 0.70, rng&#61;52&#41;</code></pre><pre><code class="plaintext code-output">UndefVarError: partition not defined
</code></pre>
<p>Let us try to implement an Neural Network regressor using Flux.jl. MLJFlux.jl provides an MLJ interface to the Flux.jl deep learning framework. The package provides four essential models: <code>NeuralNetworkRegressor, MultitargetNeuralNetworkRegressor,
NeuralNetworkClassifier</code> and <code>ImageClassifier</code>.</p>
<p>At the heart of these models is a neural network. This is specified using the <code>builder</code> parameter. Creating a builder object consists of two steps: Step 1: Creating a new struct inherited from <code>MLJFlux.Builder</code>. <code>MLJFlux.Builder</code> is an abstract structure used for the purpose of dispatching. Suppose we define a new struct called <code>MyNetworkBuilder</code>. This can contain any attribute required to build the model later. &#40;Step 2&#41;. Let&#39;s use Dense Neural Network with 2 hidden layers.</p>
<pre><code class="language-julia">mutable struct MyNetworkBuilder &lt;: MLJFlux.Builder
    n1::Int #Number of cells in the first hidden layer
    n2::Int #Number of cells in the second hidden layer
end</code></pre>
<p>Step 2: Building the neural network from this object. Extend the <code>MLJFlux.build</code> function. This takes in 3 arguments: The object of <code>MyNetworkBuilder</code>, input dimension &#40;ip&#41; and output dimension &#40;op&#41;.</p>
<pre><code class="language-julia">function MLJFlux.build&#40;model::MyNetworkBuilder, input_dims, output_dims&#41;
    layer1 &#61; Flux.Dense&#40;input_dims, model.n1&#41;
    layer2 &#61; Flux.Dense&#40;model.n1, model.n2&#41;
    layer3 &#61; Flux.Dense&#40;model.n2, output_dims&#41;
    return Flux.Chain&#40;layer1, layer2, layer3&#41;
end</code></pre>
<p>With all definitions ready, let us create an object of this:</p>
<pre><code class="language-julia">myregressor &#61; MyNetworkBuilder&#40;20, 10&#41;</code></pre><pre><code class="plaintext code-output">MyNetworkBuilder @079
</code></pre>
<p>Since the boston dataset is a regression problem, we&#39;ll be using <code>NeuralNetworkRegressor</code> here. One thing to remember is that a <code>NeuralNetworkRegressor</code> object works seamlessly like any other MLJ model: you can wrap it in an  MLJ <code>machine</code> and do anything you&#39;d do otherwise.</p>
<p>Let&#39;s start by defining our NeuralNetworkRegressor object, that takes <code>myregressor</code> as it&#39;s parameter.</p>
<pre><code class="language-julia">nnregressor &#61; MLJFlux.NeuralNetworkRegressor&#40;builder&#61;myregressor, epochs&#61;10&#41;</code></pre><pre><code class="plaintext code-output">NeuralNetworkRegressor(
    builder = MyNetworkBuilder(
            n1 = 20,
            n2 = 10),
    optimiser = Flux.Optimise.ADAM(0.001, (0.9, 0.999), IdDict{Any, Any}()),
    loss = Flux.Losses.mse,
    epochs = 10,
    batch_size = 1,
    lambda = 0.0,
    alpha = 0.0,
    rng = Random._GLOBAL_RNG(),
    optimiser_changes_trigger_retraining = false,
    acceleration = ComputationalResources.CPU1{Nothing}(nothing)) @872</code></pre>
<p>Other parameters that NeuralNetworkRegressor takes can be found here: https://github.com/alan-turing-institute/MLJFlux.jl#model-hyperparameters</p>
<p><code>nnregressor</code> now acts like any other MLJ model. Let&#39;s try wrapping it in a MLJ machine and calling <code>fit&#33;, predict</code>.</p>
<pre><code class="language-julia">mach &#61; MLJ.machine&#40;nnregressor, features, targets&#41;</code></pre><pre><code class="plaintext code-output">Machine{NeuralNetworkRegressor{MyNetworkBuilder,…},…} @460 trained 0 times; caches data
  args: 
    1:	Source @487 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`
    2:	Source @284 ⏎ `AbstractVector{ScientificTypesBase.Continuous}`
</code></pre>
<p>Let&#39;s fit this on the train set</p>
<pre><code class="language-julia">MLJ.fit&#33;&#40;mach, rows&#61;train, verbosity&#61;3&#41;</code></pre><pre><code class="plaintext code-output">UndefVarError: train not defined
</code></pre>
<p>As we can see, the training loss decreases at each epoch, showing the the neural network is gradually learning form the training set.</p>
<pre><code class="language-julia">preds &#61; MLJ.predict&#40;mach, features&#91;test, :&#93;&#41;

print&#40;preds&#91;1:5&#93;&#41;</code></pre><pre><code class="plaintext code-output">UndefVarError: test not defined
</code></pre>
<p>Now let&#39;s retrain our model. One thing to remember is that retrainig may OR may not re-initialize our neural network model parameters. For example, changing the number of epochs to 15 will not causes the model to train to 15 epcohs, but just 5 additional epochs.</p>
<pre><code class="language-julia">nnregressor.epochs &#61; 15

MLJ.fit&#33;&#40;mach, rows&#61;train, verbosity&#61;3&#41;</code></pre><pre><code class="plaintext code-output">UndefVarError: train not defined
</code></pre>
<p>You can always specify that you want to retrain the model from scratch using the force&#61;true parameter. &#40;Look at documentation for <code>fit&#33;</code> for more&#41;.</p>
<p>However, changing parameters such as batch_size will necessarily cause re-training from scratch.</p>
<pre><code class="language-julia">nnregressor.batch_size &#61; 2
MLJ.fit&#33;&#40;mach, rows&#61;train, verbosity&#61;3&#41;</code></pre><pre><code class="plaintext code-output">UndefVarError: train not defined
</code></pre>
<p>Another bit to remember here is that changing the optimiser doesn&#39;t cause retaining by default. However, the <code>optimiser_changes_trigger_retraining</code> in NeuralNetworkRegressor can be toggled to accomodate this. This allows one to modify the learning rate, for example, after an initial burn-in period.</p>
<pre><code class="language-julia"># Inspecting out-of-sample loss as a function of epochs

r &#61; MLJ.range&#40;nnregressor, :epochs, lower&#61;1, upper&#61;30, scale&#61;:log10&#41;
curve &#61; MLJ.learning_curve&#40;nnregressor, features, targets,
                       range&#61;r,
                       resampling&#61;MLJ.Holdout&#40;fraction_train&#61;0.7&#41;,
                       measure&#61;MLJ.l2&#41;

figure&#40;figsize&#61;&#40;8,6&#41;&#41;

plt.plot&#40;curve.parameter_values,
    curve.measurements&#41;

yscale&#40;&quot;log&quot;&#41;
xlabel&#40;curve.parameter_name&#41;
ylabel&#40;&quot;l2&quot;&#41;</code></pre><pre><code class="plaintext code-output">MethodError: no method matching build(::Main.FD_SANDBOX_1443500922528775838.MyNetworkBuilder, ::Random._GLOBAL_RNG, ::Int64, ::Int64)
Closest candidates are:
  build(!Matched::MLJFlux.Linear, ::Any, ::Integer, ::Integer) at /Users/tlienart/.julia/packages/MLJFlux/ISEPm/src/builders.jl:30
  build(!Matched::MLJFlux.Short, ::Any, ::Any, ::Any) at /Users/tlienart/.julia/packages/MLJFlux/ISEPm/src/builders.jl:54
  build(!Matched::MLJFlux.GenericBuilder, ::Any, ::Any, ::Any) at /Users/tlienart/.julia/packages/MLJFlux/ISEPm/src/builders.jl:99
  ...
</code></pre>
<p><span style="color:red;">// Image matching '/assets/end-to-end/boston-flux/code/EX-boston-flux-g1.svg' not found. //</span></p>
<h2 id="tuning"><a href="#tuning" class="header-anchor">Tuning</a></h2>
<p>As mentioned above, <code>nnregressor</code> can act like any other MLJ model. Let&#39;s try to tune the batch_size parameter.</p>
<pre><code class="language-julia">bs &#61; MLJ.range&#40;nnregressor, :batch_size, lower&#61;1, upper&#61;5&#41;

tm &#61; MLJ.TunedModel&#40;model&#61;nnregressor, ranges&#61;&#91;bs, &#93;, measure&#61;MLJ.l2&#41;</code></pre><pre><code class="plaintext code-output">DeterministicTunedModel(
    model = NeuralNetworkRegressor(
            builder = MyNetworkBuilder @079,
            optimiser = Flux.Optimise.ADAM(0.001, (0.9, 0.999), IdDict{Any, Any}()),
            loss = Flux.Losses.mse,
            epochs = 15,
            batch_size = 2,
            lambda = 0.0,
            alpha = 0.0,
            rng = Random._GLOBAL_RNG(),
            optimiser_changes_trigger_retraining = false,
            acceleration = ComputationalResources.CPU1{Nothing}(nothing)),
    tuning = Grid(
            goal = nothing,
            resolution = 10,
            shuffle = true,
            rng = Random._GLOBAL_RNG()),
    resampling = Holdout(
            fraction_train = 0.7,
            shuffle = false,
            rng = Random._GLOBAL_RNG()),
    measure = LPLoss(
            p = 2),
    weights = nothing,
    operation = MLJModelInterface.predict,
    range = MLJBase.NumericRange{Int64, MLJBase.Bounded, Symbol}[NumericRange{Int64,…} @083],
    selection_heuristic = MLJTuning.NaiveSelection(nothing),
    train_best = true,
    repeats = 1,
    n = nothing,
    acceleration = ComputationalResources.CPU1{Nothing}(nothing),
    acceleration_resampling = ComputationalResources.CPU1{Nothing}(nothing),
    check_measure = true,
    cache = true) @033</code></pre>
<p>For more on tuning, refer to the model-tuning tutorial.</p>
<pre><code class="language-julia">m &#61; MLJ.machine&#40;tm, features, targets&#41;

MLJ.fit&#33;&#40;m&#41;</code></pre><pre><code class="plaintext code-output">MethodError: no method matching build(::Main.FD_SANDBOX_1443500922528775838.MyNetworkBuilder, ::Random._GLOBAL_RNG, ::Int64, ::Int64)
Closest candidates are:
  build(!Matched::MLJFlux.Linear, ::Any, ::Integer, ::Integer) at /Users/tlienart/.julia/packages/MLJFlux/ISEPm/src/builders.jl:30
  build(!Matched::MLJFlux.Short, ::Any, ::Any, ::Any) at /Users/tlienart/.julia/packages/MLJFlux/ISEPm/src/builders.jl:54
  build(!Matched::MLJFlux.GenericBuilder, ::Any, ::Any, ::Any) at /Users/tlienart/.julia/packages/MLJFlux/ISEPm/src/builders.jl:99
  ...
</code></pre>
<p>This evaluated the model at each value of our range. The best value is:</p>
<pre><code class="language-julia">MLJ.fitted_params&#40;m&#41;.best_model.batch_size</code></pre><pre><code class="plaintext code-output">Machine{DeterministicTunedModel{Grid,…},…} @101 has not been trained. Call `fit!` on the machine, or, if you meant to create a learning network `Node`, use the syntax `node(fitted_params, mach::Machine)`. 
</code></pre>

<div class="page-foot">
  <div class="copyright">
    &copy; Thibaut Lienart, Anthony Blaom, Sebastian Vollmer and collaborators. Last modified: August 02, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
      </div> <!-- end of id=main -->
  </div> <!-- end of id=layout -->
  <script src="/libs/pure/ui.min.js"></script>
  
  
      <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

  
</body>
</html>
