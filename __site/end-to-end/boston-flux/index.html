<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <!-- Syntax highlighting via Prism, note: restricted langs -->
<link rel="stylesheet" href="/DataScienceTutorials.jl/libs/highlight/github.min.css">
 
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/landing.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/franklin.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/pure.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/side-menu.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/nav.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/extra.css">
  <!-- <link rel="icon" href="/DataScienceTutorials.jl/assets/infra/favicon.gif"> -->
   <title>Boston with Flux</title> 
  <!-- LUNR -->
  <script src="/DataScienceTutorials.jl/libs/lunr/lunr.min.js"></script>
  <script src="/DataScienceTutorials.jl/libs/lunr/lunr_index.js"></script>
  <script src="/DataScienceTutorials.jl/libs/lunr/lunrclient.min.js"></script>
</head>

<body>
  <div id="layout">
    <!-- Menu toggle / hamburger icon -->
    <a href="#menu" id="menuLink" class="menu-link"><span></span></a>
    <div id="menu" style="display: none;">
      <div class="pure-menu">
        <a href="/DataScienceTutorials.jl/" id="menu-logo-link">
          <div class="menu-logo">
            <!-- <img id="menu-logo" alt="MLJ Logo" src="/DataScienceTutorials.jl/assets/infra/MLJLogo2.svg" /> -->
            <p><strong>Data Science Tutorials</strong></p>
          </div>
        </a>
        <form id="lunrSearchForm" name="lunrSearchForm">
          <input class="search-input" name="q" placeholder="Search in tutorials..." type="text">
          <input type="submit" value="Search" formaction="/DataScienceTutorials.jl/search/index.html" style="display:none">
        </form>
        <!-- LIST OF MENU ITEMS -->
        <ul class="pure-menu-list">
          <li class="pure-menu-item pure-menu-top-item "><a href="/DataScienceTutorials.jl/"
              class="pure-menu-link"><strong>Home</strong></a></li>

          <!-- DATA BASICS -->
          <div class="dropdown">
            <li class="pure-menu-sublist-title"><strong>Data Basics</strong></li>
          </div>
          <div class="collapse dropdown-content">
            <ul class="pure-menu-sublist">
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/data/loading/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Loading
                  data</a></li>
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/data/dataframe/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data
                  Frames</a></li>
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/data/categorical/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span>
                  Categorical Arrays</a></li>
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/data/scitype/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Scientific
                  Type</a></li>
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/data/processing/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data
                  processing</a></li>
            </ul>
          </div>

          <!-- GETTING STARTED WITH MLJ -->
          <div class="dropdown">
            <li class="pure-menu-sublist-title"><strong>Getting Started</strong></li>
          </div>
          <div class="collapse dropdown-content">
            <ul class="pure-menu-sublist">
              <li
                class="pure-menu-item ">
                <a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Choosing a model</a>
              </li>
              <li class="pure-menu-item ">
                <a href="/DataScienceTutorials.jl/getting-started/fit-and-predict/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Fit, predict, transform</a>
              </li>
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/getting-started/model-tuning/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Model tuning</a></li>
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/getting-started/ensembles/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span>
                  Ensembles</a></li>
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/getting-started/ensembles-2/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Ensembles 2</a></li>
              <li
                class="pure-menu-item ">
                <a href="/DataScienceTutorials.jl/getting-started/composing-models/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Composing models</a>
              </li>
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/getting-started/stacking/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span>
                  Stacking</a></li>
            </ul>
          </div>
          <!-- INTRO TO STATS LEARNING -->
          <div class="dropdown">
            <li class="pure-menu-sublist-title"><strong>Intro to Stats Learning</strong></li>
          </div>
          <div class="collapse dropdown-content">
            <ul class="pure-menu-sublist" id=isl>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-2/"
                  class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 2</a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-3/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Lab 3</a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-4/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Lab 4</a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-5/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Lab 5</a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-6b/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Lab 6b</a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-8/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Lab 8</a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-9/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Lab 9</a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-10/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Lab 10</a></li>
            </ul>
          </div>
          <!-- End to End -->
          <div class="dropdown">
            <li class="pure-menu-sublist-title"><strong>End to End</strong></li>
          </div>
          <div class="dropdown-content collapse">
            <ul class="pure-menu-sublist" >
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/end-to-end/telco/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span>Telco
                  Churn</a></li>
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/end-to-end/AMES/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> AMES</a>
              </li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/wine/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Wine</a></li>
              <li class="pure-menu-item "><a
                  href="/DataScienceTutorials.jl/end-to-end/crabs-xgb/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span>
                  Crabs (XGB)</a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/horse/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Horse</a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/HouseKingCounty/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> King County Houses</a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/airfoil" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Airfoil </a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-lgbm" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Boston (lgbm) </a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/glm/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Using GLM.jl </a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/powergen/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Power Generation </a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-flux" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Boston (Flux) </a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/breastcancer" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Breast Cancer</a></li>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/creditfraud" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Credit Fraud</a></li>
            </ul>
          </div>
          <!-- ADVANCED EXAMPLES -->
          <div class="dropdown">
            <li class="pure-menu-sublist-title"><strong>Advanced Examples</strong></li>
          </div>
          <div class="dropdown-content collapse">
            <ul class="pure-menu-sublist" id=adv>
              <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/advanced/ensembles-3/" class="pure-menu-link"><span
                    style="padding-right:0.5rem;">•</span> Ensembles (3)</a></li>
            </ul>
          </div>
      </div>
      </ul>
      <!-- END OF LIST OF MENU ITEMS -->
    </div>
  </div>
  <div id="nav" class="navigation">
    <div class="nav-container">
      <div class="brand">
        <a href="/DataScienceTutorials.jl/">DataScienceTutorials.jl</a>
      </div>
      <nav>
        <div class="nav-mobile"><a id="nav-toggle" href="#!"><span></span></a></div>
        <ul class="nav-list">
        <!-- horizontal navigation bar gets injected -->
        </ul>
      </nav>
    </div>
  </div>
  <div id="main"> <!-- Closed in foot -->
    

    <!-- Content appended here --><div class="franklin-content"><h1 id="boston_with_flux"><a href="#boston_with_flux" class="header-anchor">Boston with Flux</a></h1>
<em>To ensure code in this tutorial runs as shown, download the tutorial <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-boston-flux.tar.gz">project folder</a> and follow <a href="/DataScienceTutorials.jl/#learning_by_doing">these instructions</a>.</em></p>
<p><em>If you have questions or suggestions about this tutorial, please open an issue <a href="https://github.com/JuliaAI/DataScienceTutorials.jl/issues/new">here</a>.</em></p>
<p><div class="franklin-toc"><ol><li><a href="#getting_started">Getting started</a></li><li><a href="#tuning">Tuning</a></li></ol></div>
<p><strong>Main author</strong>: Ayush Shridhar &#40;ayush-1506&#41;.</p>
<div class="dropdown"><h2 id="getting_started"><a href="#getting_started" class="header-anchor">Getting started</a></h2></div>
<div class="dropdown-content"><pre><code class="language-julia">import MLJFlux
import MLJ
import DataFrames: DataFrame
import Statistics
import Flux
using Random

Random.seed&#33;&#40;11&#41;</code></pre><pre><code class="plaintext code-output">Random.TaskLocalRNG()</code></pre>
<p>Loading the Boston dataset. Our aim will be to implement a neural network regressor to predict the price of a house, given a number of features.</p>
<pre><code class="language-julia">features, targets &#61; MLJ.@load_boston
features &#61; DataFrame&#40;features&#41;
@show size&#40;features&#41;
@show targets&#91;1:3&#93;
first&#40;features, 3&#41; |&gt; MLJ.pretty</code></pre><pre><code class="plaintext code-output">size(features) = (506, 12)
targets[1:3] = [24.0, 21.6, 34.7]
┌────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┐
│ Crim       │ Zn         │ Indus      │ NOx        │ Rm         │ Age        │ Dis        │ Rad        │ Tax        │ PTRatio    │ Black      │ LStat      │
│ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │
│ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │
├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤
│ 0.00632    │ 18.0       │ 2.31       │ 0.538      │ 6.575      │ 65.2       │ 4.09       │ 1.0        │ 296.0      │ 15.3       │ 396.9      │ 4.98       │
│ 0.02731    │ 0.0        │ 7.07       │ 0.469      │ 6.421      │ 78.9       │ 4.9671     │ 2.0        │ 242.0      │ 17.8       │ 396.9      │ 9.14       │
│ 0.02729    │ 0.0        │ 7.07       │ 0.469      │ 7.185      │ 61.1       │ 4.9671     │ 2.0        │ 242.0      │ 17.8       │ 392.83     │ 4.03       │
└────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┘
</code></pre>
<p>Next obvious steps: partitioning into train and test set</p>
<pre><code class="language-julia">train, test &#61; MLJ.partition&#40;collect&#40;eachindex&#40;targets&#41;&#41;, 0.70, rng&#61;52&#41;</code></pre><pre><code class="plaintext code-output">([358, 422, 334, 476, 1, 441, 12, 115, 240, 104, 208, 158, 46, 504, 462, 101, 157, 92, 287, 360, 385, 330, 475, 465, 117, 300, 246, 230, 105, 38, 436, 481, 424, 44, 73, 296, 61, 244, 371, 14, 195, 444, 489, 235, 143, 428, 172, 66, 318, 323, 232, 74, 338, 77, 57, 23, 357, 437, 401, 127, 397, 356, 404, 136, 260, 4, 327, 121, 432, 445, 43, 19, 304, 468, 141, 47, 280, 85, 342, 440, 51, 169, 67, 168, 231, 361, 126, 54, 396, 190, 270, 164, 409, 176, 383, 352, 184, 322, 156, 416, 398, 197, 329, 220, 377, 60, 71, 494, 266, 491, 479, 130, 369, 109, 53, 214, 179, 380, 39, 119, 233, 316, 469, 213, 114, 457, 211, 152, 408, 324, 155, 319, 171, 276, 50, 102, 482, 82, 139, 420, 15, 206, 151, 486, 410, 209, 203, 364, 473, 10, 34, 282, 120, 285, 227, 68, 317, 98, 7, 459, 100, 133, 478, 439, 186, 97, 177, 159, 18, 228, 466, 362, 320, 99, 267, 212, 484, 40, 153, 279, 337, 339, 281, 249, 359, 349, 302, 224, 25, 325, 488, 69, 76, 265, 429, 268, 91, 255, 333, 123, 111, 415, 321, 33, 226, 256, 106, 129, 183, 307, 165, 95, 471, 196, 435, 229, 70, 348, 273, 137, 373, 26, 90, 506, 28, 303, 161, 449, 311, 447, 204, 414, 116, 378, 326, 480, 63, 382, 312, 306, 501, 8, 41, 247, 288, 393, 163, 388, 328, 310, 6, 474, 89, 375, 167, 16, 505, 201, 79, 443, 346, 49, 202, 347, 110, 374, 35, 405, 425, 309, 258, 187, 341, 86, 216, 24, 343, 138, 94, 248, 314, 455, 308, 88, 294, 419, 78, 81, 293, 215, 406, 427, 407, 417, 376, 194, 490, 344, 118, 27, 472, 103, 182, 42, 198, 36, 386, 236, 87, 200, 289, 52, 413, 456, 336, 400, 144, 83, 389, 237, 502, 412, 181, 162, 134, 191, 430, 219, 9, 331, 292, 173, 438, 243, 446, 125, 188, 252, 262, 58, 205, 175, 477, 301, 250, 497, 345, 132, 291, 277, 257, 379, 218, 166], [225, 189, 245, 418, 295, 135, 463, 487, 37, 207, 332, 434, 210, 283, 391, 21, 297, 59, 17, 238, 193, 387, 241, 275, 448, 217, 62, 458, 298, 452, 146, 150, 22, 470, 45, 503, 11, 426, 363, 467, 128, 498, 32, 154, 461, 56, 423, 160, 402, 251, 3, 131, 199, 464, 495, 353, 254, 64, 234, 96, 263, 284, 442, 372, 399, 313, 365, 500, 80, 454, 122, 5, 367, 113, 20, 223, 315, 29, 384, 72, 272, 499, 421, 394, 286, 174, 261, 453, 450, 112, 366, 269, 274, 93, 13, 185, 492, 148, 354, 278, 2, 305, 259, 239, 124, 335, 392, 75, 142, 108, 170, 140, 149, 350, 180, 460, 192, 340, 290, 451, 264, 431, 395, 485, 351, 381, 271, 145, 178, 55, 496, 411, 493, 370, 390, 107, 403, 65, 31, 222, 221, 299, 355, 483, 30, 433, 84, 242, 368, 147, 48, 253])</code></pre>
<p>Let us try to implement an Neural Network regressor using Flux.jl. MLJFlux.jl provides an MLJ interface to the Flux.jl deep learning framework. The package provides four essential models: <code>NeuralNetworkRegressor, MultitargetNeuralNetworkRegressor,
NeuralNetworkClassifier</code> and <code>ImageClassifier</code>.</p>
<p>At the heart of these models is a neural network. This is specified using the <code>builder</code> parameter. Creating a builder object consists of two steps: Step 1: Creating a new struct inherited from <code>MLJFlux.Builder</code>. <code>MLJFlux.Builder</code> is an abstract structure used for the purpose of dispatching. Suppose we define a new struct called <code>MyNetworkBuilder</code>. This can contain any attribute required to build the model later. &#40;Step 2&#41;. Let&#39;s use Dense Neural Network with 2 hidden layers.</p>
<pre><code class="language-julia">mutable struct MyNetworkBuilder &lt;: MLJFlux.Builder
    n1::Int #Number of cells in the first hidden layer
    n2::Int #Number of cells in the second hidden layer
end</code></pre>
<p>Step 2: Building the neural network from this object.  Extend the <code>MLJFlux.build</code> function. This takes in 4 arguments: The <code>MyNetworkBuilder</code> instance, a random number generator or seed <code>rng</code>, the input dimension &#40;<code>n_in</code>&#41; and output dimension &#40;<code>n_out</code>&#41;.</p>
<pre><code class="language-julia">function MLJFlux.build&#40;model::MyNetworkBuilder, rng, n_in, n_out&#41;
    init &#61; Flux.glorot_uniform&#40;rng&#41;
    layer1 &#61; Flux.Dense&#40;n_in, model.n1, init&#61;init&#41;
    layer2 &#61; Flux.Dense&#40;model.n1, model.n2, init&#61;init&#41;
    layer3 &#61; Flux.Dense&#40;model.n2, n_out, init&#61;init&#41;
    return Flux.Chain&#40;layer1, layer2, layer3&#41;
end</code></pre>
<p>Alternatively, there a macro shortcut to take care of both steps at once. For details, do <code>?MLJFlux.@builder</code>.</p>
<p>All definitions ready, let us create an object of this:</p>
<pre><code class="language-julia">myregressor &#61; MyNetworkBuilder&#40;20, 10&#41;</code></pre><pre><code class="plaintext code-output">MyNetworkBuilder(n1 = 20, …)
</code></pre>
<p>Since the boston dataset is a regression problem, we&#39;ll be using <code>NeuralNetworkRegressor</code> here. One thing to remember is that a <code>NeuralNetworkRegressor</code> object works seamlessly like any other MLJ model: you can wrap it in an  MLJ <code>machine</code> and do anything you&#39;d do otherwise.</p>
<p>Let&#39;s start by defining our NeuralNetworkRegressor object, that takes <code>myregressor</code> as it&#39;s parameter.</p>
<pre><code class="language-julia">nnregressor &#61; MLJFlux.NeuralNetworkRegressor&#40;builder&#61;myregressor, epochs&#61;10&#41;</code></pre><pre><code class="plaintext code-output">NeuralNetworkRegressor(
  builder = MyNetworkBuilder(
        n1 = 20, 
        n2 = 10), 
  optimiser = Flux.Optimise.Adam(0.001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}()), 
  loss = Flux.Losses.mse, 
  epochs = 10, 
  batch_size = 1, 
  lambda = 0.0, 
  alpha = 0.0, 
  rng = Random._GLOBAL_RNG(), 
  optimiser_changes_trigger_retraining = false, 
  acceleration = ComputationalResources.CPU1{Nothing}(nothing))</code></pre>
<p>Other parameters that NeuralNetworkRegressor takes can be found here: https://github.com/alan-turing-institute/MLJFlux.jl#model-hyperparameters</p>
<p><code>nnregressor</code> now acts like any other MLJ model. Let&#39;s try wrapping it in a MLJ machine and calling <code>fit&#33;, predict</code>.</p>
<pre><code class="language-julia">mach &#61; MLJ.machine&#40;nnregressor, features, targets&#41;</code></pre><pre><code class="plaintext code-output">untrained Machine; caches model-specific representations of data
  model: NeuralNetworkRegressor(builder = MyNetworkBuilder(n1 = 20, …), …)
  args: 
    1:	Source @631 ⏎ ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}
    2:	Source @304 ⏎ AbstractVector{ScientificTypesBase.Continuous}
</code></pre>
<p>Let&#39;s fit this on the train set</p>
<pre><code class="language-julia">MLJ.fit&#33;&#40;mach, rows&#61;train, verbosity&#61;3&#41;</code></pre><pre><code class="plaintext code-output">trained Machine; caches model-specific representations of data
  model: NeuralNetworkRegressor(builder = MyNetworkBuilder(n1 = 20, …), …)
  args: 
    1:	Source @631 ⏎ ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}
    2:	Source @304 ⏎ AbstractVector{ScientificTypesBase.Continuous}
</code></pre>
<p>As we can see, the training loss decreases at each epoch, showing the the neural network is gradually learning form the training set.</p>
<pre><code class="language-julia">preds &#61; MLJ.predict&#40;mach, features&#91;test, :&#93;&#41;

print&#40;preds&#91;1:5&#93;&#41;</code></pre><pre><code class="plaintext code-output">Float32[31.564112, 29.851883, 24.773237, -9.3287525, 22.552029]</code></pre>
<p>Now let&#39;s retrain our model. One thing to remember is that retrainig may OR may not re-initialize our neural network model parameters. For example, changing the number of epochs to 15 will not causes the model to train to 15 epcohs, but just 5 additional epochs.</p>
<pre><code class="language-julia">nnregressor.epochs &#61; 15

MLJ.fit&#33;&#40;mach, rows&#61;train, verbosity&#61;3&#41;</code></pre><pre><code class="plaintext code-output">trained Machine; caches model-specific representations of data
  model: NeuralNetworkRegressor(builder = MyNetworkBuilder(n1 = 20, …), …)
  args: 
    1:	Source @631 ⏎ ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}
    2:	Source @304 ⏎ AbstractVector{ScientificTypesBase.Continuous}
</code></pre>
<p>You can always specify that you want to retrain the model from scratch using the force&#61;true parameter. &#40;Look at documentation for <code>fit&#33;</code> for more&#41;.</p>
<p>However, changing parameters such as batch_size will necessarily cause re-training from scratch.</p>
<pre><code class="language-julia">nnregressor.batch_size &#61; 2
MLJ.fit&#33;&#40;mach, rows&#61;train, verbosity&#61;3&#41;</code></pre><pre><code class="plaintext code-output">trained Machine; caches model-specific representations of data
  model: NeuralNetworkRegressor(builder = MyNetworkBuilder(n1 = 20, …), …)
  args: 
    1:	Source @631 ⏎ ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}
    2:	Source @304 ⏎ AbstractVector{ScientificTypesBase.Continuous}
</code></pre>
<p>Another bit to remember here is that changing the optimiser doesn&#39;t cause retaining by default. However, the <code>optimiser_changes_trigger_retraining</code> in NeuralNetworkRegressor can be toggled to accomodate this. This allows one to modify the learning rate, for example, after an initial burn-in period.</p>
<pre><code class="language-julia"># Inspecting out-of-sample loss as a function of epochs

r &#61; MLJ.range&#40;nnregressor, :epochs, lower&#61;1, upper&#61;30, scale&#61;:log10&#41;
curve &#61; MLJ.learning_curve&#40;nnregressor, features, targets,
                       range&#61;r,
                       resampling&#61;MLJ.Holdout&#40;fraction_train&#61;0.7&#41;,
                       measure&#61;MLJ.l2&#41;

using Plots

plot&#40;curve.parameter_values, curve.measurements, yaxis&#61;:log, legend&#61;false&#41;

xlabel&#33;&#40;curve.parameter_name&#41;
ylabel&#33;&#40;&quot;l2-log&quot;&#41;</code></pre>
<img src="/DataScienceTutorials.jl/assets/end-to-end/boston-flux/code/output/EX-boston-flux-g1.svg" alt="BostonFlux1">
<p>‎</p></div>
<div class="dropdown"><h2 id="tuning"><a href="#tuning" class="header-anchor">Tuning</a></h2></div>
<div class="dropdown-content"><p>As mentioned above, <code>nnregressor</code> can act like any other MLJ model. Let&#39;s try to tune the batch_size parameter.</p>
<pre><code class="language-julia">bs &#61; MLJ.range&#40;nnregressor, :batch_size, lower&#61;1, upper&#61;5&#41;

tm &#61; MLJ.TunedModel&#40;model&#61;nnregressor, ranges&#61;&#91;bs, &#93;, measure&#61;MLJ.l2&#41;</code></pre><pre><code class="plaintext code-output">DeterministicTunedModel(
  model = NeuralNetworkRegressor(
        builder = MyNetworkBuilder(n1 = 20, …), 
        optimiser = Flux.Optimise.Adam(0.001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}()), 
        loss = Flux.Losses.mse, 
        epochs = 15, 
        batch_size = 2, 
        lambda = 0.0, 
        alpha = 0.0, 
        rng = Random._GLOBAL_RNG(), 
        optimiser_changes_trigger_retraining = false, 
        acceleration = ComputationalResources.CPU1{Nothing}(nothing)), 
  tuning = RandomSearch(
        bounded = Distributions.Uniform, 
        positive_unbounded = Distributions.Gamma, 
        other = Distributions.Normal, 
        rng = Random._GLOBAL_RNG()), 
  resampling = Holdout(
        fraction_train = 0.7, 
        shuffle = false, 
        rng = Random._GLOBAL_RNG()), 
  measure = LPLoss(p = 2), 
  weights = nothing, 
  class_weights = nothing, 
  operation = nothing, 
  range = MLJBase.NumericRange{Int64, MLJBase.Bounded, Symbol}[NumericRange(1 ≤ batch_size ≤ 5; origin=3.0, unit=2.0)], 
  selection_heuristic = MLJTuning.NaiveSelection(nothing), 
  train_best = true, 
  repeats = 1, 
  n = nothing, 
  acceleration = ComputationalResources.CPU1{Nothing}(nothing), 
  acceleration_resampling = ComputationalResources.CPU1{Nothing}(nothing), 
  check_measure = true, 
  cache = true)</code></pre>
<p>For more on tuning, refer to the model-tuning tutorial.</p>
<pre><code class="language-julia">m &#61; MLJ.machine&#40;tm, features, targets&#41;

MLJ.fit&#33;&#40;m&#41;</code></pre><pre><code class="plaintext code-output">trained Machine; does not cache data
  model: DeterministicTunedModel(model = NeuralNetworkRegressor(builder = MyNetworkBuilder(n1 = 20, …), …), …)
  args: 
    1:	Source @326 ⏎ ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}
    2:	Source @252 ⏎ AbstractVector{ScientificTypesBase.Continuous}
</code></pre>
<p>This evaluated the model at each value of our range. The best value is:</p>
<pre><code class="language-julia">MLJ.fitted_params&#40;m&#41;.best_model.batch_size</code></pre><pre><code class="plaintext code-output">5</code></pre>
<p>‎</p></div>

<div class="bottom-nav-container">
  <a id="prev-tutorial" style="text-decoration: none"><Button class="bottom-nav">
        <div>← Previous Tutorial</div>
        <div id="prev-label" class="button-label"> Home</div>
    </Button></a>
  <a id="next-tutorial" style="text-decoration: none"><Button class="bottom-nav">
        <div>Next Tutorial →</div>
        <div id="next-label" class="button-label">Home</div> 
    </Button></a>
</div>
<div class="page-foot">
  <div class="copyright">
    &copy; Thibaut Lienart, Anthony Blaom, Sebastian Vollmer and collaborators. Last modified: January 31, 2024. Website built with <a
      href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div></div><!-- CONTENT ENDS HERE -->
</div> <!-- end of id=main -->
</div> <!-- end of id=layout -->
<!-- for collapse functionality -->
<script src="/DataScienceTutorials.jl/libs/collapse/collapse.js"></script>
<script src="/DataScienceTutorials.jl/libs/pure/ui.min.js"></script>
<!-- head and footer-nav -->
<script src="/DataScienceTutorials.jl/libs/nav/head.js"></script>
<script src="/DataScienceTutorials.jl/libs/nav/footer-nav.js"></script>
<!-- landing page -->
<script src="/DataScienceTutorials.jl/libs/landing/landing.js"></script>
<!-- navigation bar -->
<script src="/DataScienceTutorials.jl/libs/nav/nav.js"></script>
<!-- responsive navigation bar -->
<script src="/DataScienceTutorials.jl/libs/nav/responsive.js"></script>


<script src="/DataScienceTutorials.jl/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>


</body>

</html>