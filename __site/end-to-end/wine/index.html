<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/DataScienceTutorials.jl/libs/highlight/github.min.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/franklin.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/pure.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/side-menu.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/extra.css"> <title>Wine</title> <script src="/DataScienceTutorials.jl/libs/lunr/lunr.min.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunr_index.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunrclient.min.js"></script> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/DataScienceTutorials.jl/" id=menu-logo-link > <div class=menu-logo > <img id=menu-logo  alt="MLJ Logo" src="/DataScienceTutorials.jl/assets/infra/MLJLogo2.svg" /> <p><strong>MLJ Tutorials</strong></p> </div> </a> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/DataScienceTutorials.jl/search/index.html" style="visibility:hidden"> </form> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/DataScienceTutorials.jl/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Data basics</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/loading/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Loading data</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/dataframe/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Data Frames</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/categorical/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/scitype/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Scientific Type</a> </ul> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/fit-and-predict/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/model-tuning/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/composing-models/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/stacking/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Stacking</a> </ul> <li class=pure-menu-sublist-title ><strong>Intro to Stats Learning</strong> <ul class=pure-menu-sublist  id=isl> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 2</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 3</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-4/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 4</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-5/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 5</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-6b/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 6b</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-8/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 8</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-9/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 9</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-10/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 10</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/AMES/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/wine/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/crabs-xgb/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/horse/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Horse</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/HouseKingCounty/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> King County Houses</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/airfoil" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Airfoil </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-lgbm" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a> </ul> </ul> </div> </div> <div id=main > <div class=franklin-content ><h1 id=wine ><a href="#wine">Wine</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/generated/notebooks/EX-wine.ipynb" target=_blank ><em>notebook</em></a>, <em>the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/generated/scripts/EX-wine-raw.jl" target=_blank ><em>raw script</em></a>, <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/generated/scripts/EX-wine.jl" target=_blank ><em>annotated script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=franklin-toc ><ol><li><a href="#initial_data_processing">Initial data processing</a><ol><li><a href="#getting_the_data">Getting the data</a><li><a href="#setting_the_scientific_type">Setting the scientific type</a></ol><li><a href="#getting_a_baseline">Getting a baseline</a><li><a href="#visualising_the_classes">Visualising the classes</a></ol></div><h2 id=initial_data_processing ><a href="#initial_data_processing">Initial data processing</a></h2> <p>In this example, we consider the <a href="http://archive.ics.uci.edu/ml/datasets/wine">UCI &quot;wine&quot; dataset</a></p> <blockquote> <p>These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.</p> </blockquote> <h3 id=getting_the_data ><a href="#getting_the_data">Getting the data</a></h3> Let&#39;s download the data thanks to the <a href="HTTP.get&#40;&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;&#41;">HTTP.jl</a> package and load it into a DataFrame via the <a href="https://github.com/JuliaData/CSV.jl">CSV.jl</a> package:</p> <pre><code class=language-julia >using HTTP
using MLJ
using CSV
using PyPlot
import DataFrames: describe
req = HTTP.get("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data")
data = CSV.read(req.body,
                header=["Class", "Alcool", "Malic acid",
                        "Ash", "Alcalinity of ash", "Magnesium",
                        "Total phenols", "Flavanoids",
                        "Nonflavanoid phenols", "Proanthcyanins",
                        "Color intensity", "Hue",
                        "OD280/OD315 of diluted wines", "Proline"])
# the target is the Class column, everything else is a feature
y, X = unpack(data, ==(:Class), colname->true);</code></pre> <h3 id=setting_the_scientific_type ><a href="#setting_the_scientific_type">Setting the scientific type</a></h3> <p>Let&#39;s explore the scientific type attributed by default to the target and the features</p> <pre><code class=language-julia >scitype(y)</code></pre><pre><code class=plaintext >AbstractArray{ScientificTypes.Count,1}</code></pre>
<p>this should be changed as it should be considered as an ordered factor</p>
<pre><code class=language-julia >yc = coerce(y, OrderedFactor);</code></pre>
<p>Let&#39;s now consider the features</p>
<pre><code class=language-julia >scitype(X)</code></pre><pre><code class=plaintext >ScientificTypes.Table{Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}}</code></pre>
<p>So there are <code>Continuous</code> values &#40;encoded as floating point&#41; and <code>Count</code> values &#40;integer&#41;. Note also that there are no missing value &#40;otherwise one of the scientific type would have been a <code>Union&#123;Missing,*&#125;</code>&#41;. Let&#39;s check which column is what:</p>
<pre><code class=language-julia >schema(X)</code></pre><pre><code class=plaintext >MLJScientificTypes.Schema{(:Alcool, Symbol("Malic acid"), :Ash, Symbol("Alcalinity of ash"), :Magnesium, Symbol("Total phenols"), :Flavanoids, Symbol("Nonflavanoid phenols"), :Proanthcyanins, Symbol("Color intensity"), :Hue, Symbol("OD280/OD315 of diluted wines"), :Proline),Tuple{Float64,Float64,Float64,Float64,Int64,Float64,Float64,Float64,Float64,Float64,Float64,Float64,Int64},Tuple{ScientificTypes.Continuous,ScientificTypes.Continuous,ScientificTypes.Continuous,ScientificTypes.Continuous,ScientificTypes.Count,ScientificTypes.Continuous,ScientificTypes.Continuous,ScientificTypes.Continuous,ScientificTypes.Continuous,ScientificTypes.Continuous,ScientificTypes.Continuous,ScientificTypes.Continuous,ScientificTypes.Count},178}()</code></pre>
<p>The two variable that are encoded as <code>Count</code> can  probably be re-interpreted; let&#39;s have a look at the <code>Proline</code> one to see what it looks like</p>
<pre><code class=language-julia >X[1:5, :Proline]</code></pre><pre><code class=plaintext >5-element Array{Int64,1}:
 1065
 1050
 1185
 1480
  735</code></pre>
<p>It can likely be interpreted as a Continuous as well &#40;it would be better to know precisely what it is but for now let&#39;s just go with the hunch&#41;. We&#39;ll do the same with <code>:Magnesium</code>:</p>
<pre><code class=language-julia >Xc = coerce(X, :Proline=>Continuous, :Magnesium=>Continuous);</code></pre>
<p>Finally, let&#39;s have a quick look at the mean and standard deviation of each feature to get a feel for their amplitude:</p>
<pre><code class=language-julia >describe(Xc, :mean, :std)</code></pre><pre><code class=plaintext >DataFrames.DataFrame(AbstractArray{T,1} where T[[:Alcool, Symbol("Malic acid"), :Ash, Symbol("Alcalinity of ash"), :Magnesium, Symbol("Total phenols"), :Flavanoids, Symbol("Nonflavanoid phenols"), :Proanthcyanins, Symbol("Color intensity"), :Hue, Symbol("OD280/OD315 of diluted wines"), :Proline], [13.000617977528083, 2.336348314606741, 2.3665168539325854, 19.49494382022472, 99.74157303370787, 2.295112359550562, 2.0292696629213474, 0.36185393258426973, 1.5908988764044953, 5.058089882022473, 0.9574494382022468, 2.6116853932584254, 746.8932584269663], [0.8118265380058577, 1.1171460976144627, 0.2743440090608148, 3.3395637671735052, 14.282483515295668, 0.6258510488339891, 0.9988586850169465, 0.12445334029667939, 0.5723588626747611, 2.318285871822413, 0.22857156582982338, 0.7099904287650505, 314.9074742768489]], DataFrames.Index(Dict(:std => 3,:variable => 1,:mean => 2), [:variable, :mean, :std]))</code></pre>
<p>Right so it varies a fair bit which would invite to standardise the data.</p>
<p><strong>Note</strong>: to complete such a first step, one could explore histograms of the various features for instance, check that there is enough variation among the continuous features and that there does not seem to be problems in the encoding, we cut this out to shorten the tutorial. We could also have checked that the data was balanced.</p>
<h2 id=getting_a_baseline ><a href="#getting_a_baseline">Getting a baseline</a></h2>
<p>It&#39;s a multiclass classification problem with continuous inputs so a sensible start is  to test two very simple classifiers to get a baseline. We&#39;ll train two simple pipelines:</p>
<ul>
<li><p>a Standardizer &#43; KNN classifier and</p>

<li><p>a Standardizer &#43; Multinomial classifier &#40;logistic regression&#41;.</p>

</ul>
<pre><code class=language-julia >@load KNNClassifier pkg="NearestNeighbors"
@load MultinomialClassifier pkg="MLJLinearModels";

@pipeline KnnPipe(std=Standardizer(), clf=KNNClassifier()) is_probabilistic=true
@pipeline MnPipe(std=Standardizer(), clf=MultinomialClassifier()) is_probabilistic=true</code></pre><pre><code class=plaintext >Main.FD_SANDBOX_6379546592481509243.MnPipe(MLJModels.Standardizer(Symbol[], false, false, false), MLJLinearModels.MultinomialClassifier(1.0, 0.0, :l2, true, false, nothing, 2))</code></pre>
<p>We can now fit this on a train split of the data setting aside 20&#37; of the data for eventual testing.</p>
<pre><code class=language-julia >train, test = partition(eachindex(yc), 0.8, shuffle=true, rng=111)
Xtrain = selectrows(Xc, train)
Xtest = selectrows(Xc, test)
ytrain = selectrows(yc, train)
ytest = selectrows(yc, test);</code></pre>
<p>Let&#39;s now wrap an instance of these models with data &#40;all hyperparameters are set to default here&#41;:</p>
<pre><code class=language-julia >knn = machine(KnnPipe(), Xtrain, ytrain)
multi = machine(MnPipe(), Xtrain, ytrain)</code></pre><pre><code class=plaintext >MLJBase.Machine{Main.FD_SANDBOX_6379546592481509243.MnPipe}(Main.FD_SANDBOX_6379546592481509243.MnPipe(MLJModels.Standardizer(Symbol[], false, false, false), MLJLinearModels.MultinomialClassifier(1.0, 0.0, :l2, true, false, nothing, 2)), #undef, #undef, #undef, (DataFrames.DataFrame(AbstractArray{T,1} where T[[12.85, 12.21, 12.08, 14.13, 14.1, 13.88, 12.88, 13.69, 11.84, 13.82, 13.78, 12.84, 11.96, 11.87, 12.53, 13.05, 13.73, 13.56, 13.71, 13.36, 12.43, 13.27, 12.42, 13.63, 12.79, 13.68, 11.81, 13.05, 13.28, 13.83, 13.11, 14.2, 12.37, 14.38, 14.02, 12.37, 12.29, 13.94, 12.22, 13.72, 12.72, 13.87, 11.03, 12.82, 12.42, 14.37, 13.17, 13.39, 12.37, 12.64, 13.86, 11.82, 13.4, 13.48, 13.03, 12.96, 12.16, 12.17, 14.22, 12.93, 12.08, 12.36, 12.0, 12.04, 13.76, 13.58, 13.62, 12.25, 13.86, 12.2, 13.51, 14.06, 12.47, 14.19, 13.41, 13.49, 14.38, 13.05, 13.74, 12.25, 12.33, 12.08, 12.37, 12.29, 13.83, 13.07, 12.07, 13.11, 14.39, 13.16, 13.24, 12.85, 13.05, 14.3, 13.17, 12.34, 12.86, 14.75, 12.77, 14.34, 13.84, 13.64, 12.51, 13.24, 13.9, 13.2, 12.58, 12.42, 11.61, 12.08, 13.4, 11.62, 12.81, 13.23, 12.77, 13.08, 14.21, 11.76, 13.32, 11.56, 11.46, 13.5, 13.16, 11.66, 13.75, 13.73, 14.83, 12.7, 11.41, 13.52, 14.1, 13.29, 13.3, 11.45, 14.22, 11.64, 13.49, 13.48, 13.88, 13.71, 12.0, 12.25], [1.6, 1.19, 1.83, 4.1, 2.16, 5.04, 2.99, 3.26, 0.89, 1.75, 2.76, 2.96, 1.09, 4.31, 5.51, 3.86, 4.36, 1.71, 1.86, 2.56, 1.53, 4.28, 2.55, 1.81, 2.67, 1.83, 2.12, 2.05, 1.64, 1.57, 1.01, 1.76, 1.63, 3.59, 1.68, 0.94, 1.41, 1.73, 1.29, 1.43, 1.81, 1.9, 1.51, 3.37, 4.43, 1.95, 5.19, 1.77, 1.13, 1.36, 1.35, 1.72, 3.91, 1.81, 0.9, 3.45, 1.61, 1.45, 1.7, 2.81, 1.39, 3.83, 3.43, 4.3, 1.53, 2.58, 4.95, 1.73, 1.51, 3.03, 1.8, 2.15, 1.52, 1.59, 3.84, 1.66, 1.87, 5.8, 1.67, 3.88, 1.1, 2.08, 1.07, 3.17, 1.65, 1.5, 2.16, 1.9, 1.87, 3.57, 2.59, 3.27, 1.77, 1.92, 2.59, 2.45, 1.35, 1.73, 2.39, 1.68, 4.12, 3.1, 1.73, 3.98, 1.68, 1.78, 1.29, 1.61, 1.35, 1.33, 4.6, 1.99, 2.31, 3.3, 3.43, 3.9, 4.04, 2.68, 3.24, 2.05, 3.74, 1.81, 2.36, 1.88, 1.73, 1.5, 1.64, 3.87, 0.74, 3.17, 2.02, 1.97, 1.72, 2.4, 3.99, 2.06, 3.59, 1.67, 1.89, 5.65, 0.92, 4.72], [2.52, 1.75, 2.32, 2.74, 2.3, 2.23, 2.4, 2.54, 2.58, 2.42, 2.3, 2.61, 2.3, 2.39, 2.64, 2.32, 2.26, 2.31, 2.36, 2.35, 2.29, 2.26, 2.27, 2.7, 2.48, 2.36, 2.74, 3.22, 2.84, 2.62, 1.7, 2.45, 2.3, 2.28, 2.21, 1.36, 1.98, 2.27, 1.94, 2.5, 2.2, 2.8, 2.2, 2.3, 2.73, 2.5, 2.32, 2.62, 2.16, 2.02, 2.27, 1.88, 2.48, 2.41, 1.71, 2.35, 2.31, 2.53, 2.3, 2.7, 2.5, 2.38, 2.0, 2.38, 2.7, 2.69, 2.35, 2.12, 2.67, 2.32, 2.65, 2.61, 2.2, 2.48, 2.12, 2.24, 2.38, 2.13, 2.25, 2.2, 2.28, 1.7, 2.1, 2.21, 2.6, 2.1, 2.17, 2.75, 2.45, 2.15, 2.87, 2.58, 2.1, 2.72, 2.37, 2.46, 2.32, 2.39, 2.28, 2.7, 2.38, 2.56, 1.98, 2.29, 2.12, 2.14, 2.1, 2.19, 2.7, 2.3, 2.86, 2.28, 2.4, 2.28, 1.98, 2.36, 2.44, 2.92, 2.38, 3.23, 1.82, 2.61, 2.67, 1.92, 2.41, 2.7, 2.17, 2.4, 2.5, 2.72, 2.4, 2.68, 2.14, 2.42, 2.51, 2.46, 2.19, 2.64, 2.59, 2.45, 2.0, 2.54], [17.8, 16.8, 18.5, 24.5, 18.0, 20.0, 20.0, 20.0, 18.0, 14.0, 22.0, 24.0, 21.0, 21.0, 25.0, 22.5, 22.5, 16.2, 16.6, 20.0, 21.5, 20.0, 22.0, 17.2, 22.0, 17.2, 21.5, 25.0, 15.5, 20.0, 15.0, 15.2, 24.5, 16.0, 16.0, 10.6, 16.0, 17.4, 19.0, 16.7, 18.8, 19.4, 21.5, 19.5, 26.5, 16.8, 22.0, 16.1, 19.0, 16.8, 16.0, 19.5, 23.0, 20.5, 16.0, 18.5, 22.8, 19.0, 16.3, 21.0, 22.5, 21.0, 19.0, 22.0, 19.5, 24.5, 20.0, 19.0, 25.0, 19.0, 19.0, 17.6, 19.0, 16.5, 18.8, 24.0, 12.0, 21.5, 16.4, 18.5, 16.0, 17.5, 18.5, 18.0, 17.2, 15.5, 21.0, 25.5, 14.6, 21.0, 21.0, 22.0, 17.0, 20.0, 20.0, 21.0, 18.0, 11.4, 19.5, 25.0, 19.5, 15.2, 20.5, 17.5, 16.0, 11.2, 20.0, 22.5, 20.0, 23.6, 25.0, 18.0, 24.0, 18.5, 16.0, 21.5, 18.9, 20.0, 21.5, 28.5, 19.5, 20.0, 18.6, 16.0, 16.0, 22.5, 14.0, 23.0, 21.0, 23.5, 18.8, 16.8, 17.0, 20.0, 13.2, 21.6, 19.5, 22.5, 15.0, 20.5, 19.0, 21.0], [95.0, 151.0, 81.0, 96.0, 105.0, 80.0, 104.0, 107.0, 94.0, 111.0, 90.0, 101.0, 101.0, 82.0, 96.0, 85.0, 88.0, 117.0, 101.0, 89.0, 86.0, 120.0, 90.0, 112.0, 112.0, 104.0, 134.0, 124.0, 110.0, 115.0, 78.0, 112.0, 88.0, 102.0, 96.0, 88.0, 85.0, 108.0, 92.0, 108.0, 86.0, 107.0, 85.0, 88.0, 102.0, 113.0, 93.0, 93.0, 87.0, 100.0, 98.0, 86.0, 102.0, 100.0, 86.0, 106.0, 90.0, 104.0, 118.0, 96.0, 84.0, 88.0, 87.0, 80.0, 132.0, 105.0, 92.0, 80.0, 86.0, 96.0, 110.0, 121.0, 162.0, 108.0, 90.0, 87.0, 102.0, 86.0, 118.0, 112.0, 101.0, 97.0, 88.0, 88.0, 94.0, 98.0, 85.0, 116.0, 96.0, 102.0, 118.0, 106.0, 107.0, 120.0, 120.0, 98.0, 122.0, 91.0, 86.0, 98.0, 89.0, 116.0, 85.0, 103.0, 101.0, 100.0, 103.0, 108.0, 94.0, 70.0, 112.0, 98.0, 98.0, 98.0, 80.0, 113.0, 111.0, 103.0, 92.0, 119.0, 107.0, 96.0, 101.0, 97.0, 89.0, 101.0, 97.0, 101.0, 88.0, 97.0, 103.0, 102.0, 94.0, 96.0, 128.0, 84.0, 88.0, 89.0, 101.0, 95.0, 86.0, 89.0], [2.48, 1.85, 1.6, 2.05, 2.95, 0.98, 1.3, 1.83, 2.2, 3.88, 1.35, 2.32, 3.38, 2.86, 1.79, 1.65, 1.28, 3.15, 2.61, 1.4, 2.74, 1.59, 1.68, 2.85, 1.48, 2.42, 1.6, 2.63, 2.6, 2.95, 2.98, 3.27, 2.22, 3.25, 2.65, 1.98, 2.55, 2.88, 2.36, 3.4, 2.2, 2.95, 2.46, 1.48, 2.2, 3.85, 1.74, 2.85, 3.5, 2.02, 2.98, 2.5, 1.8, 2.7, 1.95, 1.39, 1.78, 1.89, 3.2, 1.54, 2.56, 2.3, 2.0, 2.1, 2.95, 1.55, 2.0, 1.65, 2.95, 1.25, 2.35, 2.6, 2.5, 3.3, 2.45, 1.88, 3.3, 2.62, 2.6, 1.38, 2.05, 2.23, 3.52, 2.85, 2.45, 2.4, 2.6, 2.2, 2.5, 1.5, 2.8, 1.65, 3.0, 2.8, 1.65, 2.56, 1.51, 3.1, 1.39, 2.8, 1.8, 2.7, 2.2, 2.64, 3.1, 2.65, 1.48, 2.0, 2.74, 2.2, 1.98, 3.02, 1.15, 1.8, 1.63, 1.41, 2.85, 1.75, 1.93, 3.18, 3.18, 2.53, 2.8, 1.61, 2.6, 3.0, 2.8, 2.83, 2.48, 1.55, 2.75, 3.0, 2.4, 2.9, 3.0, 1.95, 1.62, 2.6, 3.25, 1.68, 2.42, 1.38], [2.37, 1.28, 1.5, 0.76, 3.32, 0.34, 1.22, 0.56, 2.21, 3.74, 0.68, 0.6, 2.14, 3.03, 0.6, 1.59, 0.47, 3.29, 2.88, 0.5, 3.15, 0.69, 1.84, 2.91, 1.36, 2.69, 0.99, 2.68, 2.68, 3.4, 3.18, 3.39, 2.45, 3.17, 2.33, 0.57, 2.5, 3.54, 2.04, 3.67, 2.53, 2.97, 2.17, 0.66, 2.13, 3.49, 0.63, 2.94, 3.1, 1.41, 3.15, 1.64, 0.75, 2.98, 2.03, 0.7, 1.69, 1.75, 3.0, 0.5, 2.29, 0.92, 1.64, 1.75, 2.74, 0.84, 0.8, 2.03, 2.86, 0.49, 2.53, 2.51, 2.27, 3.93, 2.68, 1.84, 3.64, 2.65, 2.9, 0.78, 1.09, 2.17, 3.75, 2.99, 2.99, 2.64, 2.65, 1.28, 2.52, 0.55, 2.69, 0.6, 3.0, 3.14, 0.68, 2.11, 1.25, 3.69, 0.51, 1.31, 0.83, 3.03, 1.92, 2.63, 3.39, 2.76, 0.58, 2.09, 2.92, 1.59, 0.96, 2.26, 1.09, 0.83, 1.25, 1.39, 2.65, 2.03, 0.76, 5.08, 2.58, 2.61, 3.24, 1.57, 2.76, 3.25, 2.98, 2.55, 2.01, 0.52, 2.92, 3.23, 2.19, 2.79, 3.04, 1.69, 0.48, 1.1, 3.56, 0.61, 2.26, 0.47], [0.26, 0.14, 0.52, 0.56, 0.22, 0.4, 0.24, 0.5, 0.22, 0.32, 0.41, 0.53, 0.13, 0.21, 0.63, 0.61, 0.52, 0.34, 0.27, 0.37, 0.39, 0.43, 0.66, 0.3, 0.24, 0.42, 0.14, 0.47, 0.34, 0.4, 0.26, 0.34, 0.4, 0.27, 0.26, 0.28, 0.29, 0.32, 0.39, 0.19, 0.26, 0.37, 0.52, 0.4, 0.43, 0.24, 0.61, 0.34, 0.19, 0.53, 0.22, 0.37, 0.43, 0.26, 0.24, 0.4, 0.43, 0.45, 0.26, 0.53, 0.43, 0.5, 0.37, 0.42, 0.5, 0.39, 0.47, 0.37, 0.21, 0.4, 0.29, 0.31, 0.32, 0.32, 0.27, 0.27, 0.29, 0.3, 0.21, 0.29, 0.63, 0.26, 0.24, 0.45, 0.22, 0.28, 0.37, 0.26, 0.3, 0.43, 0.39, 0.6, 0.28, 0.33, 0.53, 0.34, 0.21, 0.43, 0.48, 0.53, 0.48, 0.17, 0.32, 0.32, 0.21, 0.26, 0.53, 0.34, 0.29, 0.42, 0.27, 0.17, 0.27, 0.61, 0.43, 0.34, 0.3, 0.6, 0.45, 0.47, 0.24, 0.28, 0.3, 0.34, 0.29, 0.29, 0.29, 0.43, 0.42, 0.5, 0.32, 0.31, 0.27, 0.32, 0.2, 0.48, 0.58, 0.52, 0.17, 0.52, 0.3, 0.53], [1.46, 2.5, 1.64, 1.35, 2.38, 0.68, 0.83, 0.8, 2.35, 1.87, 1.03, 0.81, 1.65, 2.91, 1.1, 1.62, 1.15, 2.34, 1.69, 0.64, 1.77, 1.35, 1.42, 1.46, 1.26, 1.97, 1.56, 1.92, 1.36, 1.72, 2.28, 1.97, 1.9, 2.19, 1.98, 0.42, 1.77, 2.08, 2.08, 2.04, 1.77, 1.76, 2.01, 0.97, 1.71, 2.18, 1.55, 1.45, 1.87, 0.62, 1.85, 1.42, 1.41, 1.86, 1.46, 0.94, 1.56, 1.03, 2.03, 0.75, 1.04, 1.04, 1.87, 1.35, 1.35, 1.54, 1.02, 1.63, 1.87, 0.73, 1.54, 1.25, 3.28, 1.86, 1.48, 1.03, 2.96, 2.01, 1.62, 1.14, 0.41, 1.4, 1.95, 2.81, 2.29, 1.37, 1.35, 1.56, 1.98, 1.3, 1.82, 0.96, 2.03, 1.97, 1.46, 1.31, 0.94, 2.81, 0.64, 2.7, 1.56, 1.66, 1.48, 1.66, 2.14, 1.28, 1.4, 1.61, 2.49, 1.38, 1.11, 1.35, 0.83, 1.87, 0.83, 1.14, 1.25, 1.05, 1.25, 1.87, 3.58, 1.66, 2.81, 1.15, 1.81, 2.38, 1.98, 1.95, 1.44, 0.55, 2.38, 1.66, 1.35, 1.83, 2.08, 1.35, 0.88, 2.29, 1.7, 1.06, 1.43, 0.8], [3.93, 2.85, 2.4, 9.2, 5.75, 4.9, 5.4, 5.88, 3.05, 7.05, 9.58, 4.92, 3.21, 2.8, 5.0, 4.8, 6.62, 6.13, 3.8, 5.6, 3.94, 10.2, 2.7, 7.3, 10.8, 3.84, 2.5, 3.58, 4.6, 6.6, 5.3, 6.75, 2.12, 4.9, 4.7, 1.95, 2.9, 8.9, 2.7, 6.8, 3.9, 4.5, 1.9, 10.26, 2.08, 7.8, 7.9, 4.8, 4.45, 5.75, 7.22, 2.06, 7.3, 5.1, 4.6, 5.28, 2.45, 2.95, 6.38, 4.6, 2.9, 7.65, 1.28, 2.6, 5.4, 8.66, 4.4, 3.4, 3.38, 5.5, 4.2, 5.05, 2.6, 8.7, 4.28, 3.74, 7.5, 2.6, 5.85, 8.21, 3.27, 3.3, 4.5, 2.3, 5.6, 3.7, 2.76, 7.1, 5.25, 4.0, 4.32, 5.58, 5.04, 6.2, 9.3, 2.8, 4.1, 5.4, 9.899999, 13.0, 9.01, 5.1, 2.94, 4.36, 6.1, 4.38, 7.6, 2.06, 2.65, 1.74, 8.5, 3.25, 5.7, 10.52, 3.4, 9.4, 5.24, 3.8, 8.42, 6.0, 2.9, 3.52, 5.68, 3.8, 5.6, 5.7, 5.2, 2.57, 3.08, 4.35, 6.2, 6.0, 3.95, 3.25, 5.1, 2.8, 5.7, 11.75, 5.43, 7.7, 2.5, 3.85], [1.09, 1.28, 1.08, 0.61, 1.25, 0.58, 0.74, 0.96, 0.79, 1.01, 0.7, 0.89, 0.99, 0.75, 0.82, 0.84, 0.78, 0.95, 1.11, 0.7, 0.69, 0.59, 0.86, 1.28, 0.48, 1.23, 0.95, 1.13, 1.09, 1.13, 1.12, 1.05, 0.89, 1.04, 1.04, 1.05, 1.23, 1.12, 0.86, 0.89, 1.16, 1.25, 1.71, 0.72, 0.92, 0.86, 0.6, 0.92, 1.22, 0.98, 1.01, 0.94, 0.7, 1.04, 1.19, 0.68, 1.33, 1.45, 0.94, 0.77, 0.93, 0.56, 0.93, 0.79, 1.25, 0.74, 0.91, 1.0, 1.36, 0.66, 1.1, 1.06, 1.16, 1.23, 0.91, 0.98, 1.2, 0.73, 0.92, 0.65, 1.25, 1.27, 1.04, 1.42, 1.24, 1.18, 0.86, 0.61, 1.02, 0.6, 1.04, 0.87, 0.88, 1.07, 0.6, 0.8, 0.76, 1.25, 0.57, 0.57, 0.57, 0.96, 1.04, 0.82, 0.91, 1.05, 0.58, 1.06, 0.96, 1.07, 0.67, 1.16, 0.66, 0.56, 0.7, 0.57, 0.87, 1.23, 0.55, 0.93, 0.75, 1.12, 1.03, 1.23, 1.15, 1.19, 1.08, 1.19, 1.1, 0.89, 1.07, 1.07, 1.02, 0.8, 0.89, 1.0, 0.81, 0.57, 0.88, 0.64, 1.38, 0.75], [3.63, 3.07, 2.27, 1.6, 3.17, 1.33, 1.42, 1.82, 3.08, 3.26, 1.68, 2.15, 3.13, 3.64, 1.69, 2.01, 1.75, 3.38, 4.0, 2.47, 2.84, 1.56, 3.3, 2.88, 1.47, 2.87, 2.26, 3.2, 2.78, 2.57, 3.18, 2.85, 2.78, 3.44, 3.59, 1.82, 2.74, 3.1, 3.02, 2.87, 3.14, 3.4, 2.87, 1.75, 3.12, 3.45, 1.48, 3.22, 2.87, 1.59, 3.55, 2.44, 1.56, 3.47, 2.48, 1.75, 2.26, 2.23, 3.31, 2.31, 3.19, 1.58, 3.05, 2.57, 3.0, 1.8, 2.05, 3.17, 3.16, 1.83, 2.87, 3.58, 2.63, 2.82, 3.0, 2.78, 3.0, 3.1, 3.2, 2.0, 1.67, 2.96, 2.77, 2.83, 3.37, 2.69, 3.28, 1.33, 3.58, 1.68, 2.93, 2.11, 3.35, 2.65, 1.62, 3.38, 1.29, 2.73, 1.63, 1.96, 1.64, 3.36, 3.57, 3.0, 3.33, 3.4, 1.55, 2.96, 3.26, 3.21, 1.92, 2.96, 1.36, 1.51, 2.12, 1.33, 3.33, 2.5, 1.62, 3.69, 2.81, 3.82, 3.17, 2.14, 2.9, 2.71, 2.85, 3.13, 2.31, 2.06, 2.75, 2.84, 2.77, 3.39, 3.53, 2.75, 1.82, 1.78, 3.56, 1.74, 3.12, 1.27], [1015.0, 718.0, 480.0, 560.0, 1510.0, 415.0, 530.0, 680.0, 520.0, 1190.0, 615.0, 590.0, 886.0, 380.0, 515.0, 515.0, 520.0, 795.0, 1035.0, 780.0, 352.0, 835.0, 315.0, 1310.0, 480.0, 990.0, 625.0, 830.0, 880.0, 1130.0, 502.0, 1450.0, 342.0, 1065.0, 1035.0, 520.0, 428.0, 1260.0, 312.0, 1285.0, 714.0, 915.0, 407.0, 685.0, 365.0, 1480.0, 725.0, 1195.0, 420.0, 450.0, 1045.0, 415.0, 750.0, 920.0, 392.0, 675.0, 495.0, 355.0, 970.0, 600.0, 385.0, 520.0, 564.0, 580.0, 1235.0, 750.0, 550.0, 510.0, 410.0, 510.0, 1095.0, 1295.0, 937.0, 1680.0, 1035.0, 472.0, 1547.0, 380.0, 1060.0, 855.0, 680.0, 710.0, 660.0, 406.0, 1265.0, 1020.0, 378.0, 425.0, 1290.0, 830.0, 735.0, 570.0, 885.0, 1280.0, 840.0, 438.0, 630.0, 1150.0, 470.0, 660.0, 480.0, 845.0, 672.0, 680.0, 985.0, 1050.0, 640.0, 345.0, 680.0, 625.0, 630.0, 345.0, 560.0, 675.0, 372.0, 550.0, 1080.0, 607.0, 650.0, 465.0, 562.0, 845.0, 1185.0, 428.0, 1320.0, 1285.0, 1045.0, 463.0, 434.0, 520.0, 1060.0, 1270.0, 1285.0, 625.0, 760.0, 680.0, 580.0, 620.0, 1095.0, 740.0, 278.0, 720.0]], DataFrames.Index(Dict(Symbol("Nonflavanoid phenols") => 8,:Proanthcyanins => 9,Symbol("Malic acid") => 2,Symbol("Total phenols") => 6,:Hue => 11,:Magnesium => 5,:Flavanoids => 7,Symbol("Color intensity") => 10,:Proline => 13,:Ash => 3,Symbol("Alcalinity of ash") => 4,Symbol("OD280/OD315 of diluted wines") => 12,:Alcool => 1), [:Alcool, Symbol("Malic acid"), :Ash, Symbol("Alcalinity of ash"), :Magnesium, Symbol("Total phenols"), :Flavanoids, Symbol("Nonflavanoid phenols"), :Proanthcyanins, Symbol("Color intensity"), :Hue, Symbol("OD280/OD315 of diluted wines"), :Proline])), </code></pre>
<p>Let&#39;s train a KNNClassifier with default hyperparameters and get a baseline misclassification rate using 90&#37; of the training data to train the model and the remaining 10&#37; to evaluate it:</p>
<pre><code class=language-julia >opts = (resampling=Holdout(fraction_train=0.9), measure=cross_entropy)
res = evaluate!(knn; opts...)
round(res.measurement[1], sigdigits=3)</code></pre><pre><code class=plaintext >UndefVarError: knn not defined
</code></pre>
<p>Now we do the same with a MultinomialClassifier</p>
<pre><code class=language-julia >res = evaluate!(multi; opts...)
round(res.measurement[1], sigdigits=3)</code></pre><pre><code class=plaintext >UndefVarError: multi not defined
</code></pre>
<p>Both methods seem to offer comparable levels of performance. Let&#39;s check the misclassification over the full training set:</p>
<pre><code class=language-julia >mcr_k = misclassification_rate(predict_mode(knn, Xtrain), ytrain)
mcr_m = misclassification_rate(predict_mode(multi, Xtrain), ytrain)
println(rpad("KNN mcr:", 10), round(mcr_k, sigdigits=3))
println(rpad("MNC mcr:", 10), round(mcr_m, sigdigits=3))</code></pre><pre><code class=plaintext >UndefVarError: knn not defined
</code></pre>
<p>So here we have done no hyperparameter training and already have a misclassification rate below 5&#37;. Clearly the problem is not very difficult.</p>
<h2 id=visualising_the_classes ><a href="#visualising_the_classes">Visualising the classes</a></h2>
<p>One way to get intuition for why the dataset is so easy to classify is to project it onto a 2D space using the PCA and display the two classes to see if they are well separated; we use the arrow-syntax here &#40;if you&#39;re on Julia &lt;&#61; 1.2, use the commented-out lines as you won&#39;t be able to use the arrow-syntax&#41;</p>
<pre><code class=language-julia ># @pipeline PCAPipe(std=Standardizer(), t=PCA(maxoutdim=2))
# pca = machine(PCAPipe(), Xtrain)
# fit!(pca, Xtrain)
# W = transform(pca, Xtrain)

@load PCA

pca = Xc |> Standardizer() |> PCA(maxoutdim=2)
fit!(pca)
W = pca(rows=train);</code></pre><pre><code class=plaintext >UndefVarError: Xc not defined
</code></pre>
<p>Let&#39;s now display this using different colours for the different classes:</p>
<pre><code class=language-julia >x1 = W.x1
x2 = W.x2

mask_1 = ytrain .== 1
mask_2 = ytrain .== 2
mask_3 = ytrain .== 3

figure(figsize=(8, 6))
plot(x1[mask_1], x2[mask_1], linestyle="none", marker="o", color="red")
plot(x1[mask_2], x2[mask_2], linestyle="none", marker="o", color="blue")
plot(x1[mask_3], x2[mask_3], linestyle="none", marker="o", color="magenta")

xlabel("PCA dimension 1", fontsize=14)
ylabel("PCA dimension 2", fontsize=14)
legend(["Class 1", "Class 2", "Class 3"], fontsize=12)
xticks(fontsize=12)
yticks(fontsize=12)</code></pre><pre><code class=plaintext >UndefVarError: W not defined
</code></pre>
<p><span style="color:red;">// Image matching '/assets/end-to-end/wine/code/EX-wine-pca.svg' not found. //</span></p>
<p>On that figure it now becomes quite clear why we managed to achieve such high scores with very simple classifiers. At this point it&#39;s a bit pointless to dig much deaper into parameter tuning etc.</p>
<p>As a last step, we can report performances of the models on the test set which we set aside earlier:</p>
<pre><code class=language-julia >perf_k = misclassification_rate(predict_mode(knn, Xtest), ytest)
perf_m = misclassification_rate(predict_mode(multi, Xtest), ytest)
println(rpad("KNN mcr:", 10), round(perf_k, sigdigits=3))
println(rpad("MNC mcr:", 10), round(perf_m, sigdigits=3))</code></pre><pre><code class=plaintext >UndefVarError: knn not defined
</code></pre>
<p>Pretty good for so little work&#33;</p>
<div class=page-foot >
  <div class=copyright >
    &copy; Thibaut Lienart, Anthony Blaom and collaborators. Last modified: May 24, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div>
      </div> 
  </div> 
  <script src="/DataScienceTutorials.jl/libs/pure/ui.min.js"></script>
  
  
      <script src="/DataScienceTutorials.jl/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>