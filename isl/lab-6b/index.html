<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <!-- Syntax highlighting via Prism, note: restricted langs -->
<link rel="stylesheet" href="/DataScienceTutorials.jl/libs/highlight/github.min.css">
 
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/franklin.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/pure.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/side-menu.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/extra.css">
  <!-- <link rel="icon" href="/DataScienceTutorials.jl/assets/infra/favicon.gif"> -->
   <title>Lab 6b - Ridge and Lasso regression</title>  
  <!-- LUNR -->
  <script src="/DataScienceTutorials.jl/libs/lunr/lunr.min.js"></script>
  <script src="/DataScienceTutorials.jl/libs/lunr/lunr_index.js"></script>
  <script src="/DataScienceTutorials.jl/libs/lunr/lunrclient.min.js"></script>
</head>
<body>
  <div id="layout">
    <!-- Menu toggle / hamburger icon -->
    <a href="#menu" id="menuLink" class="menu-link"><span></span></a>
    <div id="menu">
      <div class="pure-menu">
        <a href="/DataScienceTutorials.jl/" id="menu-logo-link">
          <div class="menu-logo">
            <!-- <img id="menu-logo" alt="MLJ Logo" src="/DataScienceTutorials.jl/assets/infra/MLJLogo2.svg" /> -->
            <p><strong>Data Science Tutorials</strong></p>
          </div>
        </a>
        <form id="lunrSearchForm" name="lunrSearchForm">
          <input class="search-input" name="q" placeholder="Enter search term" type="text">
          <input type="submit" value="Search" formaction="/DataScienceTutorials.jl/search/index.html" style="visibility:hidden">
        </form>
  <!-- LIST OF MENU ITEMS -->
  <ul class="pure-menu-list">
    <li class="pure-menu-item pure-menu-top-item "><a href="/DataScienceTutorials.jl/" class="pure-menu-link"><strong>Home</strong></a></li>

    <!-- DATA BASICS -->
    <li class="pure-menu-sublist-title"><strong>Data basics</strong></li>
    <ul class="pure-menu-sublist">
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/loading/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Loading data</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/dataframe/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data Frames</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/categorical/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/scitype/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Scientific Type</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/processing/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data processing</a></li>
    </ul>

    <!-- GETTING STARTED WITH MLJ -->
    <li class="pure-menu-sublist-title"><strong>Getting started</strong></li>
    <ul class="pure-menu-sublist">
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Choosing a model</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/fit-and-predict/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/model-tuning/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Model tuning</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-3/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/composing-models/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Composing models</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Learning networks</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/stacking/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Stacking</a></li>
    </ul>

    <!-- INTRO TO STATS LEARNING -->
    <li class="pure-menu-sublist-title"><strong>Intro to Stats Learning</strong></li>
    <ul class="pure-menu-sublist" id=isl>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 2</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-3/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 3</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-4/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 4</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-5/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 5</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-6b/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 6b</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-8/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 8</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-9/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 9</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-10/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 10</a></li>
    </ul>

    <!-- END TO END EXAMPLES -->
    <li class="pure-menu-sublist-title"><strong>End to end examples</strong></li>
    <ul class="pure-menu-sublist" id=e2e>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/AMES/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> AMES</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/wine/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Wine</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/crabs-xgb/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/horse/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Horse</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/HouseKingCounty/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> King County Houses</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/airfoil" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Airfoil </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-lgbm" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/glm/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Using GLM.jl </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/powergen/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Power Generation </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-flux" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Boston (Flux) </a></li>
    </ul>
  </ul>
  <!-- END OF LIST OF MENU ITEMS -->
      </div>
    </div>
    <div id="main"> <!-- Closed in foot -->
      

<!-- Content appended here -->
<div class="franklin-content"><h1 id="lab_6b_-_ridge_and_lasso_regression"><a href="#lab_6b_-_ridge_and_lasso_regression">Lab 6b - Ridge and Lasso regression</a></h1>
<em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/notebooks/ISL-lab-6b.ipynb" target="_blank"><em>notebook</em></a>, <em>the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/ISL-lab-6b-raw.jl" target="_blank"><em>raw script</em></a>, <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/ISL-lab-6b.jl" target="_blank"><em>annotated script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class="franklin-toc"><ol><li><a href="#getting_started">Getting started</a><ol><li><a href="#data_preparation">Data preparation</a></li></ol></li><li><a href="#ridge_pipeline">Ridge pipeline</a><ol><li><a href="#baseline">Baseline</a></li><li><a href="#basic_ridge">Basic Ridge</a></li><li><a href="#cross_validating">Cross validating</a></li></ol></li><li><a href="#lasso_pipeline">Lasso pipeline</a></li><li><a href="#elastic_net_pipeline">Elastic net pipeline</a></li></ol></div><pre><code class="language-julia"># In this tutorial, we are exploring the application of Ridge and Lasso</code></pre><pre><code class="plaintext">LassoRegressor(
    lambda = 1.0,
    fit_intercept = true,
    penalize_intercept = false,
    solver = nothing) @897</code></pre>
<p>regression to the Hitters R dataset.</p>
<h2 id="getting_started"><a href="#getting_started">Getting started</a></h2>
<pre><code class="language-julia">using MLJ
import RDatasets: dataset
using PrettyPrinting
import Distributions
const D = Distributions

@load LinearRegressor pkg=MLJLinearModels
@load RidgeRegressor pkg=MLJLinearModels
@load LassoRegressor pkg=MLJLinearModels</code></pre><pre><code class="plaintext">LassoRegressor(
    lambda = 1.0,
    fit_intercept = true,
    penalize_intercept = false,
    solver = nothing) @053</code></pre>
<p>We load the dataset using the <code>dataset</code> function, which takes the Package and dataset names as arguments.</p>
<pre><code class="language-julia">hitters = dataset("ISLR", "Hitters")
@show size(hitters)
names(hitters) |> pprint</code></pre><pre><code class="plaintext">size(hitters) = (322, 20)
["AtBat",
 "Hits",
 "HmRun",
 "Runs",
 "RBI",
 "Walks",
 "Years",
 "CAtBat",
 "CHits",
 "CHmRun",
 "CRuns",
 "CRBI",
 "CWalks",
 "League",
 "Division",
 "PutOuts",
 "Assists",
 "Errors",
 "Salary",
 "NewLeague"]</code></pre>
<p>Let&#39;s unpack the dataset with the <code>unpack</code> function. In this case, the target is <code>Salary</code> &#40;<code>&#61;&#61;&#40;:Salary&#41;</code>&#41; and all other columns are features &#40;<code>col-&gt;true</code>&#41;.</p>
<pre><code class="language-julia">y, X = unpack(hitters, ==(:Salary), col->true);</code></pre>
<p>The target has missing values which we will just ignore. We extract the row indices corresponding to non-missing values of the target. Note the use of the element-wise operator <code>.</code>.</p>
<pre><code class="language-julia">no_miss = .!ismissing.(y);</code></pre>
<p>We collect the non missing values of the target in an Array.</p>
<pre><code class="language-julia"># And keep only the corresponding features values.
y = collect(skipmissing(y))
X = X[no_miss, :]

# Let's now split our dataset into a train and test sets.
train, test = partition(eachindex(y), 0.5, shuffle=true, rng=424);</code></pre>
<p>Let&#39;s have a look at the target.</p>
<pre><code class="language-julia">using PyPlot

figure(figsize=(8,6))
plot(y, ls="none", marker="o")

xticks(fontsize=12); yticks(fontsize=12)
xlabel("Index", fontsize=14), ylabel("Salary", fontsize=14)</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-6b/code/output/ISL-lab-6-g1.svg" alt="Salary">
<p>That looks quite skewed, let&#39;s have a look at a histogram:</p>
<pre><code class="language-julia">figure(figsize=(8,6))
hist(y, bins=50, density=true)

xticks(fontsize=12); yticks(fontsize=12)
xlabel("Salary", fontsize=14); ylabel("Density", fontsize=14)

edfit = D.fit_mle(D.Exponential, y)
xx = range(minimum(y), 2500, length=100)
yy = pdf.(edfit, xx)
plot(xx, yy, lw=3, label="Exponential distribution fit")

legend(fontsize=12)</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-6b/code/output/ISL-lab-6-g2.svg" alt="Distribution of salary">
<h3 id="data_preparation"><a href="#data_preparation">Data preparation</a></h3>
<p>Most features are currently encoded as integers but we will consider them as continuous. To coerce <code>int</code> features to <code>Float</code>, we nest the <code>autotype</code> function in the <code>coerce</code> function. The <code>autotype</code> function returns a dictionary containing scientific types, which is then passed to the <code>coerce</code> function. For more details on the use of <code>autotype</code>, see the <a href="https://alan-turing-institute.github.io/DataScienceTutorials.jl/data/scitype/index.html#autotype">Scientific Types</a></p>
<pre><code class="language-julia">Xc = coerce(X, autotype(X, rules=(:discrete_to_continuous,)))
scitype(Xc)</code></pre><pre><code class="plaintext">Table{Union{AbstractArray{Continuous,1}, AbstractArray{Multiclass{2},1}}}</code></pre>
<p>There&#39;re a few features that are categorical which we&#39;ll one-hot-encode.</p>
<h2 id="ridge_pipeline"><a href="#ridge_pipeline">Ridge pipeline</a></h2>  <h3 id="baseline"><a href="#baseline">Baseline</a></h3>
<p>Let&#39;s first fit a simple pipeline with a standardizer, a one-hot-encoder and a basic linear regression:</p>
<pre><code class="language-julia">model = @pipeline(Standardizer(),
                     OneHotEncoder(),
                     LinearRegressor())

pipe  = machine(model, Xc, y)
fit!(pipe, rows=train)
ŷ = predict(pipe, rows=test)
round(rms(ŷ, y[test])^2, sigdigits=4)</code></pre><pre><code class="plaintext">123500.0</code></pre>
<p>Let&#39;s get a feel for how we&#39;re doing</p>
<pre><code class="language-julia">figure(figsize=(8,6))

res = ŷ .- y[test]
stem(res)

xticks(fontsize=12); yticks(fontsize=12)
xlabel("Index", fontsize=14); ylabel("Residual (ŷ - y)", fontsize=14)

ylim([-1300, 1000])</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-6b/code/output/ISL-lab-6-g3.svg" alt="Residuals">
<pre><code class="language-julia">figure(figsize=(8,6))
hist(res, bins=30, density=true, color="green")

xx = range(-1100, 1100, length=100)
ndfit = D.fit_mle(D.Normal, res)
lfit  = D.fit_mle(D.Laplace, res)

plot(xx, pdf.(ndfit, xx), lw=3, color="orange", label="Normal fit")
plot(xx, pdf.(lfit, xx), lw=3, color="magenta", label="Laplace fit")

legend(fontsize=12)

xticks(fontsize=12); yticks(fontsize=12)
xlabel("Residual (ŷ - y)", fontsize=14); ylabel("Density", fontsize=14)
xlim([-1100, 1100])</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-6b/code/output/ISL-lab-6-g4.svg" alt="Distribution of residuals">
<h3 id="basic_ridge"><a href="#basic_ridge">Basic Ridge</a></h3>
<p>Let&#39;s now swap the linear regressor for a Ridge one without specifying the penalty &#40;<code>1</code> by default&#41;: We modify the supervised model in the pipeline directly.</p>
<pre><code class="language-julia">pipe.model.linear_regressor = RidgeRegressor()
fit!(pipe, rows=train)
ŷ = predict(pipe, rows=test)
round(rms(ŷ, y[test])^2, sigdigits=4)</code></pre><pre><code class="plaintext">109600.0</code></pre>
<p>Ok that&#39;s a bit better but surely we can do better with an appropriate selection of the hyperparameter.</p>
<h3 id="cross_validating"><a href="#cross_validating">Cross validating</a></h3>
<p>What penalty should you use? Let&#39;s do a simple CV to try to find out:</p>
<pre><code class="language-julia">r  = range(model, :(linear_regressor.lambda), lower=1e-2, upper=100_000, scale=:log10)
tm = TunedModel(model=model, ranges=r, tuning=Grid(resolution=50),
                resampling=CV(nfolds=3, rng=4141), measure=rms)
mtm = machine(tm, Xc, y)
fit!(mtm, rows=train)

best_mdl = fitted_params(mtm).best_model
round(best_mdl.linear_regressor.lambda, sigdigits=4)</code></pre><pre><code class="plaintext">5.179</code></pre>
<p>right, and  with that we get:</p>
<pre><code class="language-julia">ŷ = predict(mtm, rows=test)
round(rms(ŷ, y[test])^2, sigdigits=4)</code></pre><pre><code class="plaintext">96690.0</code></pre>
<p>Let&#39;s see:</p>
<pre><code class="language-julia">figure(figsize=(8,6))

res = ŷ .- y[test]
stem(res)

xticks(fontsize=12); yticks(fontsize=12)
xlabel("Index", fontsize=14);
ylabel("Residual (ŷ - y)", fontsize=14)
xlim(1, length(res))

ylim([-1300, 1000])</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-6b/code/output/ISL-lab-6-g5.svg" alt="Ridge residuals">
<p>You can compare that with the residuals obtained earlier.</p>
<h2 id="lasso_pipeline"><a href="#lasso_pipeline">Lasso pipeline</a></h2>
<p>Let&#39;s do the same as above but using a Lasso model and adjusting the range a bit:</p>
<pre><code class="language-julia">mtm.model.model.linear_regressor = LassoRegressor()
mtm.model.range = range(model, :(linear_regressor.lambda), lower=500, upper=100_000, scale=:log10)
fit!(mtm, rows=train)

best_mdl = fitted_params(mtm).best_model
round(best_mdl.linear_regressor.lambda, sigdigits=4)</code></pre><pre><code class="plaintext">2531.0</code></pre>
<p>Ok and let&#39;s see how that does:</p>
<pre><code class="language-julia">ŷ = predict(mtm, rows=test)
round(rms(ŷ, y[test])^2, sigdigits=4)</code></pre><pre><code class="plaintext">98330.0</code></pre>
<p>Pretty good&#33; and the parameters are reasonably sparse as expected:</p>
<pre><code class="language-julia">coefs, intercept = fitted_params(mtm.fitresult).linear_regressor
@show coefs
@show intercept</code></pre><pre><code class="plaintext">coefs = [:AtBat => -0.0, :Hits => 0.0, :HmRun => -0.0, :Runs => 82.33487884749951, :RBI => 0.0, :Walks => 37.88394248222143, :Years => -0.0, :CAtBat => 0.0, :CHits => 152.33401765158266, :CHmRun => 0.0, :CRuns => 0.0, :CRBI => 28.535678620243, :CWalks => -0.0, :League__A => -0.0, :League__N => 0.0, :Division__E => 22.920702987660867, :Division__W => -45.33006538883337, :PutOuts => 64.09339067499211, :Assists => 0.0, :Errors => -0.0, :NewLeague__A => -0.0, :NewLeague__N => 0.0]
intercept = 555.6709756637167
</code></pre>
<p>with around 50&#37; sparsity:</p>
<pre><code class="language-julia">coef_vals = [c[2] for c in coefs]
sum(coef_vals .≈ 0) / length(coefs)</code></pre><pre><code class="plaintext">0.6818181818181818</code></pre>
<p>Let&#39;s visualise this:</p>
<pre><code class="language-julia">figure(figsize=(8,6))
stem(coef_vals)

# name of the features including one-hot-encoded ones
all_names = [:AtBat, :Hits, :HmRun, :Runs, :RBI, :Walks, :Years,
             :CAtBat, :CHits, :CHmRun, :CRuns, :CRBI, :CWalks,
             :League__A, :League__N, :Div_E, :Div_W,
             :PutOuts, :Assists, :Errors, :NewLeague_A, :NewLeague_N]

idxshow = collect(1:length(coef_vals))[abs.(coef_vals) .> 10]
xticks(idxshow .- 1, all_names[idxshow], rotation=45, fontsize=12)
yticks(fontsize=12)
ylabel("Amplitude", fontsize=14)</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-6b/code/output/ISL-lab-6-g6.svg" alt="Lasso coefficients">
<h2 id="elastic_net_pipeline"><a href="#elastic_net_pipeline">Elastic net pipeline</a></h2>
<pre><code class="language-julia">@load ElasticNetRegressor pkg=MLJLinearModels

mtm.model.model.linear_regressor = ElasticNetRegressor()
mtm.model.range = [range(model, :(linear_regressor.lambda), lower=0.1, upper=100, scale=:log10),
                    range(model, :(linear_regressor.gamma),  lower=500, upper=10_000, scale=:log10)]
mtm.model.tuning = Grid(resolution=10)
fit!(mtm, rows=train)

best_mdl = fitted_params(mtm).best_model
@show round(best_mdl.linear_regressor.lambda, sigdigits=4)
@show round(best_mdl.linear_regressor.gamma, sigdigits=4)</code></pre><pre><code class="plaintext">round(best_mdl.linear_regressor.lambda, sigdigits = 4) = 46.42
round(best_mdl.linear_regressor.gamma, sigdigits = 4) = 972.9
</code></pre>
<p>And it&#39;s not too bad in terms of accuracy either</p>
<pre><code class="language-julia">ŷ = predict(mtm, rows=test)
round(rms(ŷ, y[test])^2, sigdigits=4)</code></pre><pre><code class="plaintext">97140.0</code></pre>
<p>But the simple ridge regression seems to work best here.</p>
<div class="page-foot">
  <div class="copyright">
    &copy; Thibaut Lienart, Anthony Blaom and collaborators. Last modified: July 20, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
      </div> <!-- end of id=main -->
  </div> <!-- end of id=layout -->
  <script src="/DataScienceTutorials.jl/libs/pure/ui.min.js"></script>
  
  
      <script src="/DataScienceTutorials.jl/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

  
</body>
</html>
