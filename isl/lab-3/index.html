<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/DataScienceTutorials.jl/libs/highlight/github.min.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/franklin.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/pure.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/side-menu.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/extra.css"> <title>Lab 3 - Linear Regression</title> <script src="/DataScienceTutorials.jl/libs/lunr/lunr.min.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunr_index.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunrclient.min.js"></script> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/DataScienceTutorials.jl/" id=menu-logo-link > <div class=menu-logo > <p><strong>Data Science Tutorials</strong></p> </div> </a> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/DataScienceTutorials.jl/search/index.html" style="visibility:hidden"> </form> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/DataScienceTutorials.jl/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Data basics</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/loading/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Loading data</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/dataframe/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Data Frames</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/categorical/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/scitype/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Scientific Type</a> </ul> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/fit-and-predict/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/model-tuning/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/composing-models/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/stacking/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Stacking</a> </ul> <li class=pure-menu-sublist-title ><strong>Intro to Stats Learning</strong> <ul class=pure-menu-sublist  id=isl> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 2</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 3</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-4/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 4</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-5/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 5</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-6b/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 6b</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-8/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 8</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-9/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 9</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-10/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 10</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/AMES/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/wine/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/crabs-xgb/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/horse/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Horse</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/HouseKingCounty/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> King County Houses</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/airfoil" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Airfoil </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-lgbm" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/glm/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Using GLM.jl </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/powergen/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Power Generation </a> </ul> </ul> </div> </div> <div id=main > <div class=franklin-content ><h1 id=lab_3_-_linear_regression ><a href="#lab_3_-_linear_regression">Lab 3 - Linear Regression</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/notebooks/ISL-lab-3.ipynb" target=_blank ><em>notebook</em></a>, <em>the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/ISL-lab-3-raw.jl" target=_blank ><em>raw script</em></a>, <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/ISL-lab-3.jl" target=_blank ><em>annotated script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=franklin-toc ><ol><li><a href="#simple_linear_regression">Simple linear regression</a><li><a href="#interaction_and_transformation">Interaction and transformation</a></ol></div><h2 id=simple_linear_regression ><a href="#simple_linear_regression">Simple linear regression</a></h2> <code>MLJ</code> essentially serves as a unified path to many existing Julia packages each of which provides their own functionalities and models, with their own conventions.</p> <p>The simple linear regression demonstrates this. Several packages offer it &#40;beyond just using the backslash operator&#41;: here we will use <code>MLJLinearModels</code> but we could also have used <code>GLM</code>, <code>ScikitLearn</code> etc.</p> <p>To load the model from a given package use <code>@load ModelName pkg&#61;PackageName</code> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLJ

<span class=hljs-meta >@load</span> LinearRegressor pkg=MLJLinearModels</code></pre><pre><code class="plaintext hljs">LinearRegressor(
    fit_intercept = true,
    solver = nothing) @ 7…10</code></pre> <p>Note: in order to be able to load this, you <strong>must</strong> have the relevant package in your environment, if you don&#39;t, you can always add it &#40;<code>using Pkg; Pkg.add&#40;&quot;MLJLinearModels&quot;&#41;</code>&#41;.</p> <p>Let&#39;s load the <em>boston</em> data set</p> <pre><code class="julia hljs"><span class=hljs-keyword >import</span> RDatasets: dataset
<span class=hljs-keyword >import</span> DataFrames: describe, select, Not, rename!
boston = dataset(<span class=hljs-string >"MASS"</span>, <span class=hljs-string >"Boston"</span>)
first(boston, <span class=hljs-number >3</span>)</code></pre><pre><code class="plaintext hljs">3×14 DataFrame
│ Row │ Crim    │ Zn      │ Indus   │ Chas  │ NOx     │ Rm      │ Age     │ Dis     │ Rad   │ Tax   │ PTRatio │ Black   │ LStat   │ MedV    │
│     │ Float64 │ Float64 │ Float64 │ Int64 │ Float64 │ Float64 │ Float64 │ Float64 │ Int64 │ Int64 │ Float64 │ Float64 │ Float64 │ Float64 │
├─────┼─────────┼─────────┼─────────┼───────┼─────────┼─────────┼─────────┼─────────┼───────┼───────┼─────────┼─────────┼─────────┼─────────┤
│ 1   │ 0.00632 │ 18.0    │ 2.31    │ 0     │ 0.538   │ 6.575   │ 65.2    │ 4.09    │ 1     │ 296   │ 15.3    │ 396.9   │ 4.98    │ 24.0    │
│ 2   │ 0.02731 │ 0.0     │ 7.07    │ 0     │ 0.469   │ 6.421   │ 78.9    │ 4.9671  │ 2     │ 242   │ 17.8    │ 396.9   │ 9.14    │ 21.6    │
│ 3   │ 0.02729 │ 0.0     │ 7.07    │ 0     │ 0.469   │ 7.185   │ 61.1    │ 4.9671  │ 2     │ 242   │ 17.8    │ 392.83  │ 4.03    │ 34.7    │</code></pre> <p>Let&#39;s get a feel for the data</p> <pre><code class="julia hljs">describe(boston, :mean, :std, :eltype)</code></pre><pre><code class="plaintext hljs">14×4 DataFrame
│ Row │ variable │ mean     │ std      │ eltype   │
│     │ Symbol   │ Float64  │ Float64  │ DataType │
├─────┼──────────┼──────────┼──────────┼──────────┤
│ 1   │ Crim     │ 3.61352  │ 8.60155  │ Float64  │
│ 2   │ Zn       │ 11.3636  │ 23.3225  │ Float64  │
│ 3   │ Indus    │ 11.1368  │ 6.86035  │ Float64  │
│ 4   │ Chas     │ 0.06917  │ 0.253994 │ Int64    │
│ 5   │ NOx      │ 0.554695 │ 0.115878 │ Float64  │
│ 6   │ Rm       │ 6.28463  │ 0.702617 │ Float64  │
│ 7   │ Age      │ 68.5749  │ 28.1489  │ Float64  │
│ 8   │ Dis      │ 3.79504  │ 2.10571  │ Float64  │
│ 9   │ Rad      │ 9.54941  │ 8.70726  │ Int64    │
│ 10  │ Tax      │ 408.237  │ 168.537  │ Int64    │
│ 11  │ PTRatio  │ 18.4555  │ 2.16495  │ Float64  │
│ 12  │ Black    │ 356.674  │ 91.2949  │ Float64  │
│ 13  │ LStat    │ 12.6531  │ 7.14106  │ Float64  │
│ 14  │ MedV     │ 22.5328  │ 9.1971   │ Float64  │</code></pre> <p>So there&#39;s no missing value and most variables are encoded as floating point numbers. In MLJ it&#39;s important to specify the interpretation of the features &#40;should it be considered as a Continuous feature, as a Count, ...?&#41;, see also <a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/#data_and_its_interpretation">this tutorial section</a> on scientific types.</p> <p>Here we will just interpret the integer features as continuous as we will just use a basic linear regression:</p> <pre><code class="julia hljs">data = coerce(boston, autotype(boston, :discrete_to_continuous));</code></pre>
<p>Let&#39;s also extract the target variable &#40;<code>MedV</code>&#41;:</p>
<pre><code class="julia hljs">y = data.MedV
X = select(data, Not(:MedV));</code></pre>
<p>Let&#39;s declare a simple multivariate linear regression model:</p>
<pre><code class="julia hljs">mdl = LinearRegressor()</code></pre><pre><code class="plaintext hljs">LinearRegressor(
    fit_intercept = true,
    solver = nothing) @ 2…48</code></pre>
<p>First let&#39;s do a very simple univariate regression, in order to fit it on the data, we need to wrap it in a <em>machine</em> which, in MLJ, is the composition of a model and data to apply the model on:</p>
<pre><code class="julia hljs">X_uni = select(X, :LStat) <span class=hljs-comment ># only a single feature</span>
mach_uni = machine(mdl, X_uni, y)
fit!(mach_uni)</code></pre><pre><code class="plaintext hljs">Machine{LinearRegressor} @ 4…84
</code></pre>
<p>You can then retrieve the  fitted parameters using <code>fitted_params</code>:</p>
<pre><code class="julia hljs">fp = fitted_params(mach_uni)
<span class=hljs-meta >@show</span> fp.coefs
<span class=hljs-meta >@show</span> fp.intercept</code></pre><pre><code class="plaintext hljs">fp.coefs = [:LStat =&gt; -0.950049353757991]
fp.intercept = 34.553840879383095
</code></pre>
<p>You can also visualise this</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> PyPlot

figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
plot(X.LStat, y, ls=<span class=hljs-string >"none"</span>, marker=<span class=hljs-string >"o"</span>)
Xnew = (LStat = collect(range(extrema(X.LStat)..., length=<span class=hljs-number >100</span>)),)
plot(Xnew.LStat, predict(mach_uni, Xnew))</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-3/code/output/ISL-lab-3-lm1.svg" alt="Univariate regression">
<p>The  multivariate case is very similar</p>
<pre><code class="julia hljs">mach = machine(mdl, X, y)
fit!(mach)

fp = fitted_params(mach)
coefs = fp.coefs
intercept = fp.intercept
<span class=hljs-keyword >for</span> (name, val) <span class=hljs-keyword >in</span> coefs
    println(<span class=hljs-string >"<span class=hljs-subst >$(rpad(name, <span class=hljs-number >8</span>)</span>):  <span class=hljs-subst >$(round(val, sigdigits=<span class=hljs-number >3</span>)</span>)"</span>)
<span class=hljs-keyword >end</span>
println(<span class=hljs-string >"Intercept: <span class=hljs-subst >$(round(intercept, sigdigits=<span class=hljs-number >3</span>)</span>)"</span>)</code></pre><pre><code class="plaintext hljs">Crim    :  -0.108
Zn      :  0.0464
Indus   :  0.0206
Chas    :  2.69
NOx     :  -17.8
Rm      :  3.81
Age     :  0.000692
Dis     :  -1.48
Rad     :  0.306
Tax     :  -0.0123
PTRatio :  -0.953
Black   :  0.00931
LStat   :  -0.525
Intercept: 36.5
</code></pre>
<p>You can use the <code>machine</code> in order to <em>predict</em> values as well and, for instance, compute the root mean squared error:</p>
<pre><code class="julia hljs">ŷ = predict(mach, X)
round(rms(ŷ, y), sigdigits=<span class=hljs-number >4</span>)</code></pre><pre><code class="plaintext hljs">4.679</code></pre>
<p>Let&#39;s see what the residuals look like</p>
<pre><code class="julia hljs">figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
res = ŷ .- y
stem(res)</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-3/code/output/ISL-lab-3-res.svg" alt="Plot of the residuals">
<p>Maybe that a histogram is more appropriate here</p>
<pre><code class="julia hljs">figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
hist(res, density=<span class=hljs-literal >true</span>)</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-3/code/output/ISL-lab-3-res2.svg" alt="Histogram of the residuals">
<h2 id=interaction_and_transformation ><a href="#interaction_and_transformation">Interaction and transformation</a></h2>
<p>Let&#39;s say we want to also consider an interaction term of <code>lstat</code> and <code>age</code> taken together. To do this, just create a new dataframe with an additional column corresponding to the interaction term:</p>
<pre><code class="julia hljs">X2 = hcat(X, X.LStat .* X.Age);</code></pre>
<p>So here we have a DataFrame with one extra column corresponding to the elementwise products between <code>:LStat</code> and <code>Age</code>. DataFrame gives this a default name &#40;<code>:x1</code>&#41; which we can change:</p>
<pre><code class="julia hljs">rename!(X2, :x1 =&gt; :interaction);</code></pre>
<p>Ok cool, now let&#39;s try the linear regression again</p>
<pre><code class="julia hljs">mach = machine(mdl, X2, y)
fit!(mach)
ŷ = predict(mach, X2)
round(rms(ŷ, y), sigdigits=<span class=hljs-number >4</span>)</code></pre><pre><code class="plaintext hljs">4.676</code></pre>
<p>We get slightly better results but nothing spectacular.</p>
<p>Let&#39;s get back to the lab where they consider regressing the target variable on <code>lstat</code> and <code>lstat^2</code>; again, it&#39;s essentially a case of defining the right DataFrame:</p>
<pre><code class="julia hljs">X3 = hcat(X.LStat, X.LStat.^<span class=hljs-number >2</span>)
mach = machine(mdl, X3, y)
fit!(mach)
ŷ = predict(mach, X3)
round(rms(ŷ, y), sigdigits=<span class=hljs-number >4</span>)</code></pre><pre><code class="plaintext hljs">5.507</code></pre>
<p>which again, we can visualise:</p>
<pre><code class="julia hljs">Xnew = (LStat = Xnew.LStat, LStat2 = Xnew.LStat.^<span class=hljs-number >2</span>)

figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
plot(X.LStat, y, ls=<span class=hljs-string >"none"</span>, marker=<span class=hljs-string >"o"</span>)
plot(Xnew.LStat, predict(mach, Xnew))</code></pre>
<img src="/DataScienceTutorials.jl/assets/isl/lab-3/code/output/ISL-lab-3-lreg.svg" alt="Polynomial regression">
<div class=page-foot >
  <div class=copyright >
    &copy; Thibaut Lienart, Anthony Blaom and collaborators. Last modified: May 24, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div>
      </div> 
  </div> 
  <script src="/DataScienceTutorials.jl/libs/pure/ui.min.js"></script>