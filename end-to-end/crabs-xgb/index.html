<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/DataScienceTutorials.jl/libs/highlight/github.min.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/franklin.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/pure.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/side-menu.css"> <link rel=stylesheet  href="/DataScienceTutorials.jl/css/extra.css"> <title>Crabs with XGBoost</title> <script src="/DataScienceTutorials.jl/libs/lunr/lunr.min.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunr_index.js"></script> <script src="/DataScienceTutorials.jl/libs/lunr/lunrclient.min.js"></script> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/DataScienceTutorials.jl/" id=menu-logo-link > <div class=menu-logo > <p><strong>Data Science Tutorials</strong></p> </div> </a> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/DataScienceTutorials.jl/search/index.html" style="visibility:hidden"> </form> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/DataScienceTutorials.jl/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Data basics</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/loading/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Loading data</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/dataframe/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Data Frames</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/categorical/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/scitype/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Scientific Type</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/processing/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Data processing</a> </ul> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/fit-and-predict/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/model-tuning/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/composing-models/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/stacking/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Stacking</a> </ul> <li class=pure-menu-sublist-title ><strong>Intro to Stats Learning</strong> <ul class=pure-menu-sublist  id=isl> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 2</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 3</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-4/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 4</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-5/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 5</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-6b/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 6b</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-8/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 8</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-9/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 9</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-10/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 10</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/AMES/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/wine/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/crabs-xgb/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/horse/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Horse</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/HouseKingCounty/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> King County Houses</a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/airfoil" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Airfoil </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-lgbm" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/glm/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Using GLM.jl </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/powergen/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Power Generation </a> <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-flux" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Boston (Flux) </a> </ul> </ul> </div> </div> <div id=main > <div class=franklin-content ><h1 id=crabs_with_xgboost ><a href="#crabs_with_xgboost">Crabs with XGBoost</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/notebooks/EX-crabs-xgb.ipynb" target=_blank ><em>notebook</em></a>, <em>the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/EX-crabs-xgb-raw.jl" target=_blank ><em>raw script</em></a>, <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/DataScienceTutorials.jl/gh-pages/generated/scripts/EX-crabs-xgb.jl" target=_blank ><em>annotated script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=franklin-toc ><ol><li><a href="#first_steps">First steps</a><li><a href="#xgboost_machine">XGBoost machine</a><ol><li><a href="#more_tuning_1">More tuning &#40;1&#41;</a><li><a href="#more_tuning_2">More tuning &#40;2&#41;</a><li><a href="#more_tuning_3">More tuning &#40;3&#41;</a></ol></ol></div>This example is inspired from <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">this post</a> showing how to use XGBoost.</p> <h2 id=first_steps ><a href="#first_steps">First steps</a></h2> <p>Again, the crabs dataset is so common that there is a simple load function for it:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLJ
<span class=hljs-keyword >using</span> StatsBase
<span class=hljs-keyword >using</span> Random
<span class=hljs-keyword >using</span> PyPlot
<span class=hljs-keyword >using</span> CategoricalArrays
<span class=hljs-keyword >using</span> PrettyPrinting
<span class=hljs-keyword >import</span> DataFrames
<span class=hljs-keyword >using</span> LossFunctions

X, y = <span class=hljs-meta >@load_crabs</span>
X = DataFrames.DataFrame(X)
<span class=hljs-meta >@show</span> size(X)
<span class=hljs-meta >@show</span> y[<span class=hljs-number >1</span>:<span class=hljs-number >3</span>]
first(X, <span class=hljs-number >3</span>) |&gt; pretty</code></pre><pre><code class="plaintext hljs">size(X) = (200, 5)
y[1:3] = CategoricalArrays.CategoricalValue{String,UInt32}[&quot;B&quot;, &quot;B&quot;, &quot;B&quot;]
┌────────────┬────────────┬────────────┬────────────┬────────────┐
│ FL         │ RW         │ CL         │ CW         │ BD         │
│ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │
│ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │
├────────────┼────────────┼────────────┼────────────┼────────────┤
│ 8.1        │ 6.7        │ 16.1       │ 19.0       │ 7.0        │
│ 8.8        │ 7.7        │ 18.1       │ 20.8       │ 7.4        │
│ 9.2        │ 7.8        │ 19.0       │ 22.4       │ 7.7        │
└────────────┴────────────┴────────────┴────────────┴────────────┘
</code></pre> <p>It&#39;s a classification problem with the following classes:</p> <pre><code class="julia hljs">levels(y) |&gt; pprint</code></pre><pre><code class="plaintext hljs">[&quot;B&quot;, &quot;O&quot;]</code></pre>
<p>Note that the dataset is currently sorted by target, let&#39;s shuffle it to avoid the obvious issues this may cause</p>
<pre><code class="julia hljs">Random.seed!(<span class=hljs-number >523</span>)
perm = randperm(length(y))
X = X[perm,:]
y = y[perm];</code></pre>
<p>It&#39;s not a very big dataset so we will likely overfit it badly using something as sophisticated as XGBoost but it will do for a demonstration.</p>
<pre><code class="julia hljs">train, test = partition(eachindex(y), <span class=hljs-number >0.70</span>, shuffle=<span class=hljs-literal >true</span>, rng=<span class=hljs-number >52</span>)
<span class=hljs-meta >@load</span> XGBoostClassifier
xgb_model = XGBoostClassifier()</code></pre><pre><code class="plaintext hljs">XGBoostClassifier(
    num_round = 100,
    booster = &quot;gbtree&quot;,
    disable_default_eval_metric = 0,
    eta = 0.3,
    gamma = 0.0,
    max_depth = 6,
    min_child_weight = 1.0,
    max_delta_step = 0.0,
    subsample = 1.0,
    colsample_bytree = 1.0,
    colsample_bylevel = 1.0,
    lambda = 1.0,
    alpha = 0.0,
    tree_method = &quot;auto&quot;,
    sketch_eps = 0.03,
    scale_pos_weight = 1.0,
    updater = &quot;auto&quot;,
    refresh_leaf = 1,
    process_type = &quot;default&quot;,
    grow_policy = &quot;depthwise&quot;,
    max_leaves = 0,
    max_bin = 256,
    predictor = &quot;cpu_predictor&quot;,
    sample_type = &quot;uniform&quot;,
    normalize_type = &quot;tree&quot;,
    rate_drop = 0.0,
    one_drop = 0,
    skip_drop = 0.0,
    feature_selector = &quot;cyclic&quot;,
    top_k = 0,
    tweedie_variance_power = 1.5,
    objective = &quot;automatic&quot;,
    base_score = 0.5,
    eval_metric = &quot;mlogloss&quot;,
    seed = 0) @084</code></pre>
<p>Let&#39;s check whether the training and  is balanced, <code>StatsBase.countmap</code> is useful for that:</p>
<pre><code class="julia hljs">countmap(y[train]) |&gt; pprint</code></pre><pre><code class="plaintext hljs">Dict(&quot;B&quot; =&gt; 70, &quot;O&quot; =&gt; 70)</code></pre>
<p>which is pretty balanced. You could check the same on the test set and full set and the same comment would still hold.</p>
<h2 id=xgboost_machine ><a href="#xgboost_machine">XGBoost machine</a></h2>
<p>Wrap a machine around an XGBoost model &#40;XGB&#41; and the data:</p>
<pre><code class="julia hljs">xgb  = XGBoostClassifier()
xgbm = machine(xgb, X, y)</code></pre><pre><code class="plaintext hljs">Machine{XGBoostClassifier} @657 trained 0 times.
  args: 
    1:	Source @997 ⏎ `Table{AbstractArray{Continuous,1}}`
    2:	Source @503 ⏎ `AbstractArray{Multiclass{2},1}`
</code></pre>
<p>We will tune it varying the number of rounds used and generate a learning curve</p>
<pre><code class="julia hljs">r = range(xgb, :num_round, lower=<span class=hljs-number >50</span>, upper=<span class=hljs-number >500</span>)
curve = learning_curve!(xgbm, range=r, resolution=<span class=hljs-number >50</span>,
                        measure=HingeLoss())</code></pre><pre><code class="plaintext hljs">(parameter_name = &quot;num_round&quot;,
 parameter_scale = :linear,
 parameter_values = [50, 59, 68, 78, 87, 96, 105, 114, 123, 133, 142, 151, 160, 169, 179, 188, 197, 206, 215, 224, 234, 243, 252, 261, 270, 280, 289, 298, 307, 316, 326, 335, 344, 353, 362, 371, 381, 390, 399, 408, 417, 427, 436, 445, 454, 463, 472, 482, 491, 500],
 measurements = [0.3120960593223572, 0.3002642095088959, 0.28969401121139526, 0.28333112597465515, 0.27584463357925415, 0.2703813910484314, 0.26844921708106995, 0.26478245854377747, 0.26034364104270935, 0.2557550072669983, 0.2547350525856018, 0.2518833875656128, 0.24957433342933655, 0.24690766632556915, 0.24586206674575806, 0.2421659678220749, 0.24172064661979675, 0.23799732327461243, 0.2363770306110382, 0.23531433939933777, 0.23471292853355408, 0.23354791104793549, 0.23355154693126678, 0.23136228322982788, 0.23119983077049255, 0.23020349442958832, 0.22977910935878754, 0.22889092564582825, 0.2243921160697937, 0.22364655137062073, 0.22226248681545258, 0.2218494713306427, 0.22111089527606964, 0.22052700817584991, 0.22054848074913025, 0.21970060467720032, 0.21936337649822235, 0.2194405496120453, 0.21801182627677917, 0.21841095387935638, 0.21867485344409943, 0.21839170157909393, 0.21813097596168518, 0.21723081171512604, 0.21732456982135773, 0.21644990146160126, 0.21593405306339264, 0.21572065353393555, 0.21549715101718903, 0.21505625545978546],)</code></pre>
<p>Let&#39;s have a look</p>
<pre><code class="julia hljs">figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
plot(curve.parameter_values, curve.measurements)
xlabel(<span class=hljs-string >&quot;Number of rounds&quot;</span>, fontsize=<span class=hljs-number >14</span>)
ylabel(<span class=hljs-string >&quot;HingeLoss&quot;</span>, fontsize=<span class=hljs-number >14</span>)
xticks([<span class=hljs-number >10</span>, <span class=hljs-number >100</span>, <span class=hljs-number >200</span>, <span class=hljs-number >500</span>], fontsize=<span class=hljs-number >12</span>)</code></pre>
<img src="/DataScienceTutorials.jl/assets/end-to-end/crabs-xgb/code/output/EX-crabs-xgb-curve1.svg" alt="Cross entropy vs Num Round">
<p>So, in short, using more rounds helps. Let&#39;s arbitrarily fix it to 200.</p>
<pre><code class="julia hljs">xgb.num_round = <span class=hljs-number >200</span>;</code></pre>
<h3 id=more_tuning_1 ><a href="#more_tuning_1">More tuning &#40;1&#41;</a></h3>
<p>Let&#39;s now tune the maximum depth of each tree and the minimum child weight in the boosting.</p>
<pre><code class="julia hljs">r1 = range(xgb, :max_depth, lower=<span class=hljs-number >3</span>, upper=<span class=hljs-number >10</span>)
r2 = range(xgb, :min_child_weight, lower=<span class=hljs-number >0</span>, upper=<span class=hljs-number >5</span>)

tm = TunedModel(model=xgb, tuning=Grid(resolution=<span class=hljs-number >8</span>),
                resampling=CV(rng=<span class=hljs-number >11</span>), ranges=[r1,r2],
                measure=cross_entropy)
mtm = machine(tm, X, y)
fit!(mtm, rows=train)</code></pre><pre><code class="plaintext hljs">Machine{ProbabilisticTunedModel{Grid,…}} @505 trained 1 time.
  args: 
    1:	Source @143 ⏎ `Table{AbstractArray{Continuous,1}}`
    2:	Source @373 ⏎ `AbstractArray{Multiclass{2},1}`
</code></pre>
<p>Great, as always we can investigate the tuning by using <code>report</code> and can, for instance, plot a heatmap of the measurements:</p>
<pre><code class="julia hljs">r = report(mtm)

res = r.plotting

md = res.parameter_values[:,<span class=hljs-number >1</span>]
mcw = res.parameter_values[:,<span class=hljs-number >2</span>]

figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
tricontourf(md, mcw, res.measurements)

xlabel(<span class=hljs-string >&quot;Maximum tree depth&quot;</span>, fontsize=<span class=hljs-number >14</span>)
ylabel(<span class=hljs-string >&quot;Minimum child weight&quot;</span>, fontsize=<span class=hljs-number >14</span>)
xticks(<span class=hljs-number >3</span>:<span class=hljs-number >2</span>:<span class=hljs-number >10</span>, fontsize=<span class=hljs-number >12</span>)
yticks(fontsize=<span class=hljs-number >12</span>)</code></pre>
<img src="/DataScienceTutorials.jl/assets/end-to-end/crabs-xgb/code/output/EX-crabs-xgb-heatmap.svg" alt="Hyperparameter heatmap">
<p>Let&#39;s extract the optimal model and inspect its parameters:</p>
<pre><code class="julia hljs">xgb = fitted_params(mtm).best_model
<span class=hljs-meta >@show</span> xgb.max_depth
<span class=hljs-meta >@show</span> xgb.min_child_weight</code></pre><pre><code class="plaintext hljs">xgb.max_depth = 3
xgb.min_child_weight = 2.857142857142857
</code></pre>
<h3 id=more_tuning_2 ><a href="#more_tuning_2">More tuning &#40;2&#41;</a></h3>
<p>Let&#39;s examine the effect of <code>gamma</code>:</p>
<pre><code class="julia hljs">xgbm = machine(xgb, X, y)
r = range(xgb, :gamma, lower=<span class=hljs-number >0</span>, upper=<span class=hljs-number >10</span>)
curve = learning_curve!(xgbm, range=r, resolution=<span class=hljs-number >30</span>,
                        measure=cross_entropy);</code></pre>
<p>actually it doesn&#39;t look like it&#39;s changing much...:</p>
<pre><code class="julia hljs"><span class=hljs-meta >@show</span> round(minimum(curve.measurements), sigdigits=<span class=hljs-number >3</span>)
<span class=hljs-meta >@show</span> round(maximum(curve.measurements), sigdigits=<span class=hljs-number >3</span>)</code></pre><pre><code class="plaintext hljs">round(minimum(curve.measurements), sigdigits = 3) = 0.211
round(maximum(curve.measurements), sigdigits = 3) = 0.464
</code></pre>
<h3 id=more_tuning_3 ><a href="#more_tuning_3">More tuning &#40;3&#41;</a></h3>
<p>Let&#39;s examine the effect of <code>subsample</code> and <code>colsample_bytree</code>:</p>
<pre><code class="julia hljs">r1 = range(xgb, :subsample, lower=<span class=hljs-number >0.6</span>, upper=<span class=hljs-number >1.0</span>)
r2 = range(xgb, :colsample_bytree, lower=<span class=hljs-number >0.6</span>, upper=<span class=hljs-number >1.0</span>)
tm = TunedModel(model=xgb, tuning=Grid(resolution=<span class=hljs-number >8</span>),
                resampling=CV(rng=<span class=hljs-number >234</span>), ranges=[r1,r2],
                measure=cross_entropy)
mtm = machine(tm, X, y)
fit!(mtm, rows=train)</code></pre><pre><code class="plaintext hljs">Machine{ProbabilisticTunedModel{Grid,…}} @620 trained 1 time.
  args: 
    1:	Source @949 ⏎ `Table{AbstractArray{Continuous,1}}`
    2:	Source @151 ⏎ `AbstractArray{Multiclass{2},1}`
</code></pre>
<p>and the usual procedure to visualise it:</p>
<pre><code class="julia hljs">r = report(mtm)

res = r.plotting

ss = res.parameter_values[:,<span class=hljs-number >1</span>]
cbt = res.parameter_values[:,<span class=hljs-number >2</span>]

figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
tricontourf(ss, cbt, res.measurements)

xlabel(<span class=hljs-string >&quot;Sub sample&quot;</span>, fontsize=<span class=hljs-number >14</span>)
ylabel(<span class=hljs-string >&quot;Col sample by tree&quot;</span>, fontsize=<span class=hljs-number >14</span>)
xticks(fontsize=<span class=hljs-number >12</span>)
yticks(fontsize=<span class=hljs-number >12</span>)</code></pre>
<img src="/DataScienceTutorials.jl/assets/end-to-end/crabs-xgb/code/output/EX-crabs-xgb-heatmap2.svg" alt="Hyperparameter heatmap">
<p>Let&#39;s retrieve the best models:</p>
<pre><code class="julia hljs">xgb = fitted_params(mtm).best_model
<span class=hljs-meta >@show</span> xgb.subsample
<span class=hljs-meta >@show</span> xgb.colsample_bytree</code></pre><pre><code class="plaintext hljs">xgb.subsample = 0.6
xgb.colsample_bytree = 0.9428571428571428
</code></pre>
<p>We could continue with more fine tuning but given how small the dataset is, it doesn&#39;t make much sense. How does it fare on the test set?</p>
<pre><code class="julia hljs">ŷ = predict_mode(mtm, rows=test)
round(accuracy(ŷ, y[test]), sigdigits=<span class=hljs-number >3</span>)</code></pre><div class=page-foot >
  <div class=copyright >
    &copy; Thibaut Lienart, Anthony Blaom and collaborators. Last modified: June 22, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div>
      </div> 
  </div> 
  <script src="/DataScienceTutorials.jl/libs/pure/ui.min.js"></script>