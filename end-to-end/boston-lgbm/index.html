<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/MLJTutorials/libs/highlight/github.min.css"> <link rel=stylesheet  href="/MLJTutorials/css/franklin.css"> <link rel=stylesheet  href="/MLJTutorials/css/pure.css"> <link rel=stylesheet  href="/MLJTutorials/css/side-menu.css"> <link rel=stylesheet  href="/MLJTutorials/css/extra.css"> <title>Boston with LightGBM</title> <script src="/MLJTutorials/libs/lunr/lunr.min.js"></script> <script src="/MLJTutorials/libs/lunr/lunr_index.js"></script> <script src="/MLJTutorials/libs/lunr/lunrclient.min.js"></script> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/MLJTutorials/" id=menu-logo-link > <div class=menu-logo > <img id=menu-logo  alt="MLJ Logo" src="/MLJTutorials/assets/infra/MLJLogo2.svg" /> <p><strong>MLJ Tutorials</strong></p> </div> </a> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/MLJTutorials/search/index.html" style="visibility:hidden"> </form> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/MLJTutorials/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Data basics</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/MLJTutorials/data/loading/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Loading data</a> <li class="pure-menu-item "><a href="/MLJTutorials/data/dataframe/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Data Frames</a> <li class="pure-menu-item "><a href="/MLJTutorials/data/categorical/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a> <li class="pure-menu-item "><a href="/MLJTutorials/data/scitype/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Scientific Type</a> </ul> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/MLJTutorials/getting-started/choosing-a-model/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/MLJTutorials/getting-started/fit-and-predict/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/MLJTutorials/getting-started/model-tuning/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item "><a href="/MLJTutorials/getting-started/ensembles/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/MLJTutorials/getting-started/ensembles-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/MLJTutorials/getting-started/ensembles-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a> <li class="pure-menu-item "><a href="/MLJTutorials/getting-started/composing-models/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item "><a href="/MLJTutorials/getting-started/learning-networks/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> <li class="pure-menu-item "><a href="/MLJTutorials/getting-started/learning-networks-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a> <li class="pure-menu-item "><a href="/MLJTutorials/getting-started/stacking/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Stacking</a> </ul> <li class=pure-menu-sublist-title ><strong>Intro to Stats Learning</strong> <ul class=pure-menu-sublist  id=isl> <li class="pure-menu-item "><a href="/MLJTutorials/isl/lab-2/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 2</a> <li class="pure-menu-item "><a href="/MLJTutorials/isl/lab-3/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 3</a> <li class="pure-menu-item "><a href="/MLJTutorials/isl/lab-4/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 4</a> <li class="pure-menu-item "><a href="/MLJTutorials/isl/lab-5/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 5</a> <li class="pure-menu-item "><a href="/MLJTutorials/isl/lab-6b/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 6b</a> <li class="pure-menu-item "><a href="/MLJTutorials/isl/lab-8/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 8</a> <li class="pure-menu-item "><a href="/MLJTutorials/isl/lab-9/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 9</a> <li class="pure-menu-item "><a href="/MLJTutorials/isl/lab-10/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 10</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item "><a href="/MLJTutorials/end-to-end/AMES/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/MLJTutorials/end-to-end/wine/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item "><a href="/MLJTutorials/end-to-end/crabs-xgb/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> <li class="pure-menu-item "><a href="/MLJTutorials/end-to-end/horse/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Horse</a> <li class="pure-menu-item "><a href="/MLJTutorials/end-to-end/HouseKingCounty/" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> King County Houses</a> <li class="pure-menu-item "><a href="/MLJTutorials/end-to-end/airfoil" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Airfoil </a> <li class="pure-menu-item "><a href="/MLJTutorials/end-to-end/boston-lgbm" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a> </ul> </ul> </div> </div> <div id=main > <div class=franklin-content > <h1 id=boston_with_lightgbm ><a href="#boston_with_lightgbm">Boston with LightGBM</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/generated/notebooks/EX-boston-lgbm.ipynb" target=_blank ><em>notebook</em></a>, <em>the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/generated/scripts/EX-boston-lgbm-raw.jl" target=_blank ><em>raw script</em></a>, <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/generated/scripts/EX-boston-lgbm.jl" target=_blank ><em>annotated script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=franklin-toc ><ol><li><a href="#getting_started">Getting started</a></ol></div><strong>Main author</strong>: Yaqub Alwan &#40;IQVIA&#41;.</p> <h2 id=getting_started ><a href="#getting_started">Getting started</a></h2> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLJ, PrettyPrinting, DataFrames, Statistics
<span class=hljs-keyword >using</span> PyPlot, Random

Random.seed!(<span class=hljs-number >1212</span>)
<span class=hljs-meta >@load</span> LGBMRegressor</code></pre><pre><code class="plaintext hljs">LGBMRegressor(
    num_iterations = 10,
    learning_rate = 0.1,
    num_leaves = 31,
    max_depth = -1,
    tree_learner = "serial",
    histogram_pool_size = -1.0,
    min_data_in_leaf = 20,
    min_sum_hessian_in_leaf = 0.001,
    lambda_l1 = 0.0,
    lambda_l2 = 0.0,
    min_gain_to_split = 0.0,
    feature_fraction = 1.0,
    feature_fraction_seed = 2,
    bagging_fraction = 1.0,
    bagging_freq = 0,
    bagging_seed = 3,
    early_stopping_round = 0,
    max_bin = 255,
    init_score = "",
    objective = "regression",
    categorical_feature = Int64[],
    data_random_seed = 1,
    is_sparse = true,
    is_unbalance = false,
    metric = ["l2"],
    metric_freq = 1,
    is_training_metric = false,
    ndcg_at = [1, 2, 3, 4, 5],
    num_machines = 1,
    num_threads = 0,
    local_listen_port = 12400,
    time_out = 120,
    machine_list_file = "",
    save_binary = false,
    device_type = "cpu") @ 1…42</code></pre> <p>Let us try LightGBM out by doing a regression task on the Boston house prices dataset. This is a commonly used dataset so there is a loader built into MLJ.</p> <p>Here, the objective is to show how LightGBM can do better than a Linear Regressor with minimal effort.</p> <p>We start out by taking a quick peek at the data itself and its statistical properties.</p> <pre><code class="julia hljs">features, targets = <span class=hljs-meta >@load_boston</span>
features = DataFrame(features)
<span class=hljs-meta >@show</span> size(features)
<span class=hljs-meta >@show</span> targets[<span class=hljs-number >1</span>:<span class=hljs-number >3</span>]
first(features, <span class=hljs-number >3</span>) |&gt; pretty</code></pre><pre><code class="plaintext hljs">size(features) = (506, 12)
targets[1:3] = [24.0, 21.6, 34.7]
┌────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┐
│ Crim       │ Zn         │ Indus      │ NOx        │ Rm         │ Age        │ Dis        │ Rad        │ Tax        │ PTRatio    │ Black      │ LStat      │
│ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │
│ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │
├────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤
│ 0.00632    │ 18.0       │ 2.31       │ 0.538      │ 6.575      │ 65.2       │ 4.09       │ 1.0        │ 296.0      │ 15.3       │ 396.9      │ 4.98       │
│ 0.02731    │ 0.0        │ 7.07       │ 0.469      │ 6.421      │ 78.9       │ 4.9671     │ 2.0        │ 242.0      │ 17.8       │ 396.9      │ 9.14       │
│ 0.02729    │ 0.0        │ 7.07       │ 0.469      │ 7.185      │ 61.1       │ 4.9671     │ 2.0        │ 242.0      │ 17.8       │ 392.83     │ 4.03       │
└────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┘
</code></pre> <p>We can also describe the dataframe</p> <pre><code class="julia hljs">describe(features)</code></pre><pre><code class="plaintext hljs">12×8 DataFrames.DataFrame
│ Row │ variable │ mean     │ min     │ median  │ max     │ nunique │ nmissing │ eltype   │
│     │ Symbol   │ Float64  │ Float64 │ Float64 │ Float64 │ Nothing │ Nothing  │ DataType │
├─────┼──────────┼──────────┼─────────┼─────────┼─────────┼─────────┼──────────┼──────────┤
│ 1   │ Crim     │ 3.61352  │ 0.00632 │ 0.25651 │ 88.9762 │         │          │ Float64  │
│ 2   │ Zn       │ 11.3636  │ 0.0     │ 0.0     │ 100.0   │         │          │ Float64  │
│ 3   │ Indus    │ 11.1368  │ 0.46    │ 9.69    │ 27.74   │         │          │ Float64  │
│ 4   │ NOx      │ 0.554695 │ 0.385   │ 0.538   │ 0.871   │         │          │ Float64  │
│ 5   │ Rm       │ 6.28463  │ 3.561   │ 6.2085  │ 8.78    │         │          │ Float64  │
│ 6   │ Age      │ 68.5749  │ 2.9     │ 77.5    │ 100.0   │         │          │ Float64  │
│ 7   │ Dis      │ 3.79504  │ 1.1296  │ 3.20745 │ 12.1265 │         │          │ Float64  │
│ 8   │ Rad      │ 9.54941  │ 1.0     │ 5.0     │ 24.0    │         │          │ Float64  │
│ 9   │ Tax      │ 408.237  │ 187.0   │ 330.0   │ 711.0   │         │          │ Float64  │
│ 10  │ PTRatio  │ 18.4555  │ 12.6    │ 19.05   │ 22.0    │         │          │ Float64  │
│ 11  │ Black    │ 356.674  │ 0.32    │ 391.44  │ 396.9   │         │          │ Float64  │
│ 12  │ LStat    │ 12.6531  │ 1.73    │ 11.36   │ 37.97   │         │          │ Float64  │</code></pre> <p>Do the usual train/test partitioning. This is important so we can estimate generalisation.</p> <pre><code class="julia hljs">train, test = partition(eachindex(targets), <span class=hljs-number >0.70</span>, shuffle=<span class=hljs-literal >true</span>, rng=<span class=hljs-number >52</span>)</code></pre><pre><code class="plaintext hljs">([358, 422, 334, 476, 1, 441, 12, 115, 240, 104, 208, 158, 46, 504, 462, 101, 157, 92, 287, 360, 385, 330, 475, 465, 117, 300, 246, 230, 105, 38, 436, 481, 424, 44, 73, 296, 61, 244, 371, 14, 195, 444, 489, 235, 143, 428, 172, 66, 318, 323, 232, 74, 338, 77, 57, 23, 357, 437, 401, 127, 397, 356, 404, 136, 260, 4, 327, 121, 432, 445, 43, 19, 304, 468, 141, 47, 280, 85, 342, 440, 51, 169, 67, 168, 231, 361, 126, 54, 396, 190, 270, 164, 409, 176, 383, 352, 184, 322, 156, 416, 398, 197, 329, 220, 377, 60, 71, 494, 266, 491, 479, 130, 369, 109, 53, 214, 179, 380, 39, 119, 233, 316, 469, 213, 114, 457, 211, 152, 408, 324, 155, 319, 171, 276, 50, 102, 482, 82, 139, 420, 15, 206, 151, 486, 410, 209, 203, 364, 473, 10, 34, 282, 120, 285, 227, 68, 317, 98, 7, 459, 100, 133, 478, 439, 186, 97, 177, 159, 18, 228, 466, 362, 320, 99, 267, 212, 484, 40, 153, 279, 337, 339, 281, 249, 359, 349, 302, 224, 25, 325, 488, 69, 76, 265, 429, 268, 91, 255, 333, 123, 111, 415, 321, 33, 226, 256, 106, 129, 183, 307, 165, 95, 471, 196, 435, 229, 70, 348, 273, 137, 373, 26, 90, 506, 28, 303, 161, 449, 311, 447, 204, 414, 116, 378, 326, 480, 63, 382, 312, 306, 501, 8, 41, 247, 288, 393, 163, 388, 328, 310, 6, 474, 89, 375, 167, 16, 505, 201, 79, 443, 346, 49, 202, 347, 110, 374, 35, 405, 425, 309, 258, 187, 341, 86, 216, 24, 343, 138, 94, 248, 314, 455, 308, 88, 294, 419, 78, 81, 293, 215, 406, 427, 407, 417, 376, 194, 490, 344, 118, 27, 472, 103, 182, 42, 198, 36, 386, 236, 87, 200, 289, 52, 413, 456, 336, 400, 144, 83, 389, 237, 502, 412, 181, 162, 134, 191, 430, 219, 9, 331, 292, 173, 438, 243, 446, 125, 188, 252, 262, 58, 205, 175, 477, 301, 250, 497, 345, 132, 291, 277, 257, 379, 218, 166], [225, 189, 245, 418, 295, 135, 463, 487, 37, 207, 332, 434, 210, 283, 391, 21, 297, 59, 17, 238, 193, 387, 241, 275, 448, 217, 62, 458, 298, 452, 146, 150, 22, 470, 45, 503, 11, 426, 363, 467, 128, 498, 32, 154, 461, 56, 423, 160, 402, 251, 3, 131, 199, 464, 495, 353, 254, 64, 234, 96, 263, 284, 442, 372, 399, 313, 365, 500, 80, 454, 122, 5, 367, 113, 20, 223, 315, 29, 384, 72, 272, 499, 421, 394, 286, 174, 261, 453, 450, 112, 366, 269, 274, 93, 13, 185, 492, 148, 354, 278, 2, 305, 259, 239, 124, 335, 392, 75, 142, 108, 170, 140, 149, 350, 180, 460, 192, 340, 290, 451, 264, 431, 395, 485, 351, 381, 271, 145, 178, 55, 496, 411, 493, 370, 390, 107, 403, 65, 31, 222, 221, 299, 355, 483, 30, 433, 84, 242, 368, 147, 48, 253])</code></pre>
<p>Let us investigation some of the commonly tweaked LightGBM parameters. We start with looking at a learning curve for number of boostings.</p>
<pre><code class="julia hljs">lgb = LGBMRegressor() <span class=hljs-comment >#initialised a model with default params</span>
lgbm = machine(lgb, features[train, :], targets[train, <span class=hljs-number >1</span>])
boostrange = range(lgb, :num_iterations, lower=<span class=hljs-number >2</span>, upper=<span class=hljs-number >500</span>)
curve = learning_curve!(lgbm, resampling=CV(nfolds=<span class=hljs-number >5</span>),
                        range=boostrange, resolution=<span class=hljs-number >100</span>,
                        measure=rms)


figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
plot(curve.parameter_values, curve.measurements)
xlabel(<span class=hljs-string >"Number of rounds"</span>, fontsize=<span class=hljs-number >14</span>)
ylabel(<span class=hljs-string >"RMSE"</span>, fontsize=<span class=hljs-number >14</span>)</code></pre>
<img src="/MLJTutorials/assets/end-to-end/boston-lgbm/code/output/lgbm_hp1.svg" alt="">
<p>It looks like that we don&#39;t need to go much past 100 boosts</p>
<p>Since LightGBM is a gradient based learning method, we also have a learning rate parameter which controls the size of gradient updates. Let us look at a learning curve for this parameter too</p>
<pre><code class="julia hljs">lgb = LGBMRegressor() <span class=hljs-comment >#initialised a model with default params</span>
lgbm = machine(lgb, features[train, :], targets[train, <span class=hljs-number >1</span>])
learning_range = range(lgb, :learning_rate, lower=<span class=hljs-number >1e-3</span>, upper=<span class=hljs-number >1</span>, scale=:log)
curve = learning_curve!(lgbm, resampling=CV(nfolds=<span class=hljs-number >5</span>),
                        range=learning_range, resolution=<span class=hljs-number >100</span>,
                        measure=rms)


figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
plot(curve.parameter_values, curve.measurements)
yscale(<span class=hljs-string >"log"</span>)
xlabel(<span class=hljs-string >"Learning rate"</span>, fontsize=<span class=hljs-number >14</span>)
ylabel(<span class=hljs-string >"RMSE (log scale)"</span>, fontsize=<span class=hljs-number >14</span>)</code></pre>
<img src="/MLJTutorials/assets/end-to-end/boston-lgbm/code/output/lgbm_hp2.svg" alt="">
<p>It seems like near 0.5 is a reasonable place. Bearing in mind that for lower values of learning rate we possibly require more boosting in order to converge, so the default value of 100 might not be sufficient for convergence. We leave this as an exercise to the reader. We can still try to tune this parameter, however.</p>
<p>Finally let us check number of datapoints required to produce a leaf in an individual tree. This parameter controls the complexity of individual learner trees, and too low a value might lead to overfitting.</p>
<pre><code class="julia hljs">lgb = LGBMRegressor() <span class=hljs-comment >#initialised a model with default params</span>
lgbm = machine(lgb, features[train, :], targets[train, <span class=hljs-number >1</span>])</code></pre><pre><code class="plaintext hljs">Machine{LGBMRegressor} @ 1…50
</code></pre>
<p>dataset is small enough and the lower and upper sets the tree to have certain number of leaves</p>
<pre><code class="julia hljs">leaf_range = range(lgb, :min_data_in_leaf, lower=<span class=hljs-number >1</span>, upper=<span class=hljs-number >50</span>)


curve = learning_curve!(lgbm, resampling=CV(nfolds=<span class=hljs-number >5</span>),
                        range=leaf_range, resolution=<span class=hljs-number >50</span>,
                        measure=rms)

figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
plot(curve.parameter_values, curve.measurements)
xlabel(<span class=hljs-string >"Min data in leaf"</span>, fontsize=<span class=hljs-number >14</span>)
ylabel(<span class=hljs-string >"RMSE"</span>, fontsize=<span class=hljs-number >14</span>)</code></pre>
<img src="/MLJTutorials/assets/end-to-end/boston-lgbm/code/output/lgbm_hp3.svg" alt="">
<p>It does not seem like there is a huge risk for overfitting, and lower is better for this parameter.</p>
<p>Using the learning curves above we can select some small-ish ranges to jointly search for the best combinations of these parameters via cross validation.</p>
<pre><code class="julia hljs">r1 = range(lgb, :num_iterations, lower=<span class=hljs-number >50</span>, upper=<span class=hljs-number >100</span>)
r2 = range(lgb, :min_data_in_leaf, lower=<span class=hljs-number >2</span>, upper=<span class=hljs-number >10</span>)
r3 = range(lgb, :learning_rate, lower=<span class=hljs-number >1e-1</span>, upper=<span class=hljs-number >1e0</span>)
tm = TunedModel(model=lgb, tuning=Grid(resolution=<span class=hljs-number >5</span>),
                resampling=CV(rng=<span class=hljs-number >123</span>), ranges=[r1,r2,r3],
                measure=rms)
mtm = machine(tm, features, targets)
fit!(mtm, rows=train)</code></pre><pre><code class="plaintext hljs">Machine{DeterministicTunedModel{Grid,…}} @ 1…84
</code></pre>
<p>Lets see what the cross validation best model parameters turned out to be?</p>
<pre><code class="julia hljs">best_model = fitted_params(mtm).best_model
<span class=hljs-meta >@show</span> best_model.learning_rate
<span class=hljs-meta >@show</span> best_model.min_data_in_leaf
<span class=hljs-meta >@show</span> best_model.num_iterations</code></pre><pre><code class="plaintext hljs">best_model.learning_rate = 0.1
best_model.min_data_in_leaf = 6
best_model.num_iterations = 75
</code></pre>
<p>Great, and now let&#39;s predict using the held out data.</p>
<pre><code class="julia hljs">predictions = predict(mtm, rows=test)
rms_score = round(rms(predictions, targets[test, <span class=hljs-number >1</span>]), sigdigits=<span class=hljs-number >4</span>)

<span class=hljs-meta >@show</span> rms_score</code></pre><pre><code class="plaintext hljs">rms_score = 3.831
</code></pre>
<div class=page-foot >
  <div class=copyright >
    &copy; Anthony Blaom, Thibaut Lienart and collaborators. Last modified: April 17, 2020. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>

</div>

      </div> 
  </div> 
  <script src="/MLJTutorials/libs/pure/ui.min.js"></script>