<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <!-- Syntax highlighting via Prism, note: restricted langs -->
<link rel="stylesheet" href="/DataScienceTutorials.jl/libs/highlight/github.min.css">
 
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/franklin.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/pure.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/side-menu.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/extra.css">
  <!-- <link rel="icon" href="/DataScienceTutorials.jl/assets/infra/favicon.gif"> -->
   <title>Breast Cancer Wisconsin&#40;Diagnostic&#41;</title>  
  <!-- LUNR -->
  <script src="/DataScienceTutorials.jl/libs/lunr/lunr.min.js"></script>
  <script src="/DataScienceTutorials.jl/libs/lunr/lunr_index.js"></script>
  <script src="/DataScienceTutorials.jl/libs/lunr/lunrclient.min.js"></script>
</head>
<body>
  <div id="layout">
    <!-- Menu toggle / hamburger icon -->
    <a href="#menu" id="menuLink" class="menu-link"><span></span></a>
    <div id="menu">
      <div class="pure-menu">
        <a href="/DataScienceTutorials.jl/" id="menu-logo-link">
          <div class="menu-logo">
            <!-- <img id="menu-logo" alt="MLJ Logo" src="/DataScienceTutorials.jl/assets/infra/MLJLogo2.svg" /> -->
            <p><strong>Data Science Tutorials</strong></p>
          </div>
        </a>
        <form id="lunrSearchForm" name="lunrSearchForm">
          <input class="search-input" name="q" placeholder="Enter search term" type="text">
          <input type="submit" value="Search" formaction="/DataScienceTutorials.jl/search/index.html" style="visibility:hidden">
        </form>
  <!-- LIST OF MENU ITEMS -->
  <ul class="pure-menu-list">
    <li class="pure-menu-item pure-menu-top-item "><a href="/DataScienceTutorials.jl/" class="pure-menu-link"><strong>Home</strong></a></li>

    <!-- DATA BASICS -->
    <li class="pure-menu-sublist-title"><strong>Data basics</strong></li>
    <ul class="pure-menu-sublist">
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/loading/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Loading data</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/dataframe/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data Frames</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/categorical/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/scitype/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Scientific Type</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/processing/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data processing</a></li>
    </ul>

    <!-- GETTING STARTED WITH MLJ -->
    <li class="pure-menu-sublist-title"><strong>Getting started</strong></li>
    <ul class="pure-menu-sublist">
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Choosing a model</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/fit-and-predict/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/model-tuning/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Model tuning</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-3/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/composing-models/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Composing models</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Learning networks</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/stacking/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Stacking</a></li>
    </ul>

    <!-- INTRO TO STATS LEARNING -->
    <li class="pure-menu-sublist-title"><strong>Intro to Stats Learning</strong></li>
    <ul class="pure-menu-sublist" id=isl>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 2</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-3/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 3</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-4/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 4</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-5/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 5</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-6b/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 6b</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-8/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 8</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-9/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 9</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-10/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 10</a></li>
    </ul>

    <!-- END TO END EXAMPLES -->
    <li class="pure-menu-sublist-title"><strong>End to end examples</strong></li>
    <ul class="pure-menu-sublist" id=e2e>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/AMES/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> AMES</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/wine/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Wine</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/crabs-xgb/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/horse/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Horse</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/HouseKingCounty/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> King County Houses</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/airfoil" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Airfoil </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-lgbm" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a></li>
      <!-- <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/glm/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Using GLM.jl </a></li> -->
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/powergen/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Power Generation </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-flux" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Boston (Flux) </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/breastcancer" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Breast Cancer</a></li>
    </ul>
  </ul>
  <!-- END OF LIST OF MENU ITEMS -->
      </div>
    </div>
    <div id="main"> <!-- Closed in foot -->
      

<!-- Content appended here -->
<div class="franklin-content"><h1 id="breast_cancer_wisconsindiagnostic"><a href="#breast_cancer_wisconsindiagnostic" class="header-anchor">Breast Cancer Wisconsin&#40;Diagnostic&#41;</a></h1>
<em>Download the 
  <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-breastcancer/tutorial.ipynb" target="_blank"><em>notebook</em></a>
  , the 
  <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-breastcancer/tutorial.jl" target="_blank"><em>annotated script</em></a>
   or the 
  <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-breastcancer/tutorial-raw.jl" target="_blank"><em>raw script</em></a>
   for this tutorial &#40;right-click on the relevant link and save-as&#41;. These rely on <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-breastcancer/Project.toml">this Project.toml</a> and <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-breastcancer/Manifest.toml">this Manifest.toml</a>.</em> <br/>   <em>You can also download the whole <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-breastcancer.tar.gz">project folder</a>.</em></p>
<p><em>If you have questions or suggestions about this tutorial, please open an issue <a href="https://github.com/JuliaAI/DataScienceTutorials.jl/issues/new">here</a>.</em></p>
<p><div class="franklin-toc"><ol><li><a href="#introduction">Introduction</a></li><li><a href="#loading_the_relevant_packages">Loading the relevant packages</a></li><li><a href="#downloading_and_loading_the_data">Downloading and loading the data</a></li><li><a href="#exploring_the_obtained_data">Exploring the obtained data</a><ol><li><a href="#inspecting_the_class_variable">Inspecting the class variable</a></li><li><a href="#inspecting_the_feature_set">Inspecting the feature set</a></li></ol></li><li><a href="#unpacking_the_values">Unpacking the values</a></li><li><a href="#standardizing_the_feature_set">Standardizing the &quot;feature set&quot;</a></li><li><a href="#train-test_split">Train-test split</a></li><li><a href="#model_compatibility">Model compatibility</a></li><li><a href="#analyzing_the_performance_of_different_models">Analyzing the performance of different models</a><ol><li><a href="#creating_various_empty_vectors_for_our_analysis">Creating various empty vectors for our analysis</a></li><li><a href="#collecting_data_for_analysis">Collecting data for analysis</a></li><li><a href="#analyzing_models">Analyzing models</a></li></ol></li></ol></div>
<h2 id="introduction"><a href="#introduction" class="header-anchor">Introduction</a></h2>
<p>This tutorial covers the concepts of iterative model selection on the popular <a href="https://archive.ics.uci.edu/ml/datasets/Breast&#43;Cancer&#43;Wisconsin&#43;&#40;Diagnostic&#41;">&quot;Breast Cancer Wisconsin &#40;Diagnostic&#41; Data Set&quot;</a> from the UCI archives. The tutorial also covers basic data preprocessing and usage of MLJ Scientific Types.</p>
<h2 id="loading_the_relevant_packages"><a href="#loading_the_relevant_packages" class="header-anchor">Loading the relevant packages</a></h2>
<p>For a guide to package intsllation in Julia please refer this <a href="https://docs.julialang.org/en/v1/stdlib/Pkg/">link</a> taken directly from Juliav1 documentation</p>
<pre><code class="language-julia">using UrlDownload
using DataFrames
using PrettyPrinting
using PyPlot
using MLJ</code></pre>
<p>Inititalizing a global random seed which we&#39;ll use throughout the code to maintain consistency in results</p>
<pre><code class="language-julia">RANDOM_SEED &#61; 42;</code></pre>
<h2 id="downloading_and_loading_the_data"><a href="#downloading_and_loading_the_data" class="header-anchor">Downloading and loading the data</a></h2>
<p>Using the package UrlDownload.jl, we can capture the data from the given link using the below commands</p>
<pre><code class="language-julia">url &#61; &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data&quot;;
feature_names &#61; &#91;&quot;ID&quot;, &quot;Class&quot;, &quot;mean radius&quot;, &quot;mean texture&quot;, &quot;mean perimeter&quot;, &quot;mean area&quot;, &quot;mean smoothness&quot;, &quot;mean compactness&quot;, &quot;mean concavity&quot;, &quot;mean concave points&quot;, &quot;mean symmetry&quot;, &quot;mean fractal dimension&quot;, &quot;radius error&quot;, &quot;texture error&quot;, &quot;perimeter error&quot;, &quot;area error&quot;, &quot;smoothness error&quot;, &quot;compactness error&quot;, &quot;concavity error&quot;, &quot;concave points error&quot;, &quot;symmetry error&quot;, &quot;fractal dimension error&quot;, &quot;worst radius&quot;, &quot;worst texture&quot;, &quot;worst perimeter&quot;, &quot;worst area&quot;, &quot;worst smoothness&quot;, &quot;worst compactness&quot;, &quot;worst concavity&quot;, &quot;worst concave points&quot;, &quot;worst symmetry&quot;, &quot;worst fractal dimension&quot;&#93;
data &#61; urldownload&#40;url, true, format &#61; :CSV, header &#61; feature_names&#41;;</code></pre>
<h2 id="exploring_the_obtained_data"><a href="#exploring_the_obtained_data" class="header-anchor">Exploring the obtained data</a></h2>
<h3 id="inspecting_the_class_variable"><a href="#inspecting_the_class_variable" class="header-anchor">Inspecting the class variable</a></h3>
<pre><code class="language-julia">figure&#40;figsize&#61;&#40;8, 6&#41;&#41;
hist&#40;data.Class&#41;
xlabel&#40;&quot;Classes&quot;&#41;
ylabel&#40;&quot;Number of samples&quot;&#41;</code></pre>
<img src="/DataScienceTutorials.jl/assets/end-to-end/breastcancer/code/output/Target_class.svg" alt="Distribution of target classes">
<h3 id="inspecting_the_feature_set"><a href="#inspecting_the_feature_set" class="header-anchor">Inspecting the feature set</a></h3>
<pre><code class="language-julia">df &#61; DataFrame&#40;data&#41;&#91;:, 2:end&#93;;</code></pre>
<p>Printing the 1st 10 rows so as to get a visual idea about the type of data we&#39;re dealing with</p>
<pre><code class="language-julia">pprint&#40;first&#40;df,10&#41;&#41;</code></pre><pre><code class="plaintext code-output">10×31 DataFrame
 Row │ Class    mean radius  mean texture  mean perimeter  mean area  mean smoothness  mean compactness  mean concavity  mean concave points  mean symmetry  mean fractal dimension  radius error  texture error  perimeter error  area error  smoothness error  compactness error  concavity error  concave points error  symmetry error  fractal dimension error  worst radius  worst texture  worst perimeter  worst area  worst smoothness  worst compactness  worst concavity  worst concave points  worst symmetry  worst fractal dimension
     │ String1  Float64      Float64       Float64         Float64    Float64          Float64           Float64         Float64              Float64        Float64                 Float64       Float64        Float64          Float64     Float64           Float64            Float64          Float64               Float64         Float64                  Float64       Float64        Float64          Float64     Float64           Float64            Float64          Float64               Float64         Float64
─────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ M              17.99         10.38          122.8      1001.0          0.1184            0.2776          0.3001               0.1471          0.2419                 0.07871        1.095          0.9053            8.589      153.4           0.006399            0.04904          0.05373               0.01587         0.03003                 0.006193         25.38          17.33           184.6       2019.0            0.1622             0.6656           0.7119                0.2654          0.4601                  0.1189
   2 │ M              20.57         17.77          132.9      1326.0          0.08474           0.07864         0.0869               0.07017         0.1812                 0.05667        0.5435         0.7339            3.398       74.08          0.005225            0.01308          0.0186                0.0134          0.01389                 0.003532         24.99          23.41           158.8       1956.0            0.1238             0.1866           0.2416                0.186           0.275                   0.08902
   3 │ M              19.69         21.25          130.0      1203.0          0.1096            0.1599          0.1974               0.1279          0.2069                 0.05999        0.7456         0.7869            4.585       94.03          0.00615             0.04006          0.03832               0.02058         0.0225                  0.004571         23.57          25.53           152.5       1709.0            0.1444             0.4245           0.4504                0.243           0.3613                  0.08758
   4 │ M              11.42         20.38           77.58      386.1          0.1425            0.2839          0.2414               0.1052          0.2597                 0.09744        0.4956         1.156             3.445       27.23          0.00911             0.07458          0.05661               0.01867         0.05963                 0.009208         14.91          26.5             98.87       567.7            0.2098             0.8663           0.6869                0.2575          0.6638                  0.173
   5 │ M              20.29         14.34          135.1      1297.0          0.1003            0.1328          0.198                0.1043          0.1809                 0.05883        0.7572         0.7813            5.438       94.44          0.01149             0.02461          0.05688               0.01885         0.01756                 0.005115         22.54          16.67           152.2       1575.0            0.1374             0.205            0.4                   0.1625          0.2364                  0.07678
   6 │ M              12.45         15.7            82.57      477.1          0.1278            0.17            0.1578               0.08089         0.2087                 0.07613        0.3345         0.8902            2.217       27.19          0.00751             0.03345          0.03672               0.01137         0.02165                 0.005082         15.47          23.75           103.4        741.6            0.1791             0.5249           0.5355                0.1741          0.3985                  0.1244
   7 │ M              18.25         19.98          119.6      1040.0          0.09463           0.109           0.1127               0.074           0.1794                 0.05742        0.4467         0.7732            3.18        53.91          0.004314            0.01382          0.02254               0.01039         0.01369                 0.002179         22.88          27.66           153.2       1606.0            0.1442             0.2576           0.3784                0.1932          0.3063                  0.08368
   8 │ M              13.71         20.83           90.2       577.9          0.1189            0.1645          0.09366              0.05985         0.2196                 0.07451        0.5835         1.377             3.856       50.96          0.008805            0.03029          0.02488               0.01448         0.01486                 0.005412         17.06          28.14           110.6        897.0            0.1654             0.3682           0.2678                0.1556          0.3196                  0.1151
   9 │ M              13.0          21.82           87.5       519.8          0.1273            0.1932          0.1859               0.09353         0.235                  0.07389        0.3063         1.002             2.406       24.32          0.005731            0.03502          0.03553               0.01226         0.02143                 0.003749         15.49          30.73           106.2        739.3            0.1703             0.5401           0.539                 0.206           0.4378                  0.1072
  10 │ M              12.46         24.04           83.97      475.9          0.1186            0.2396          0.2273               0.08543         0.203                  0.08243        0.2976         1.599             2.039       23.94          0.007149            0.07217          0.07743               0.01432         0.01789                 0.01008          15.09          40.68            97.65       711.4            0.1853             1.058            1.105                 0.221           0.4366                  0.2075</code></pre>
<p>For checking the statistical attributes of each inividual feature, we can use the <strong>decsribe&#40;&#41;</strong> method</p>
<pre><code class="language-julia">pprint&#40;describe&#40;df&#41;&#41;</code></pre><pre><code class="plaintext code-output">31×7 DataFrame
 Row │ variable                 mean        min        median    max      nmissing  eltype
     │ Symbol                   Union…      Any        Union…    Any      Int64     DataType
─────┼───────────────────────────────────────────────────────────────────────────────────────
   1 │ Class                                B                    M               0  String1
   2 │ mean radius              14.1273     6.981      13.37     28.11           0  Float64
   3 │ mean texture             19.2896     9.71       18.84     39.28           0  Float64
   4 │ mean perimeter           91.969      43.79      86.24     188.5           0  Float64
   5 │ mean area                654.889     143.5      551.1     2501.0          0  Float64
   6 │ mean smoothness          0.0963603   0.05263    0.09587   0.1634          0  Float64
   7 │ mean compactness         0.104341    0.01938    0.09263   0.3454          0  Float64
   8 │ mean concavity           0.0887993   0.0        0.06154   0.4268          0  Float64
   9 │ mean concave points      0.0489191   0.0        0.0335    0.2012          0  Float64
  10 │ mean symmetry            0.181162    0.106      0.1792    0.304           0  Float64
  11 │ mean fractal dimension   0.0627976   0.04996    0.06154   0.09744         0  Float64
  12 │ radius error             0.405172    0.1115     0.3242    2.873           0  Float64
  13 │ texture error            1.21685     0.3602     1.108     4.885           0  Float64
  14 │ perimeter error          2.86606     0.757      2.287     21.98           0  Float64
  15 │ area error               40.3371     6.802      24.53     542.2           0  Float64
  16 │ smoothness error         0.00704098  0.001713   0.00638   0.03113         0  Float64
  17 │ compactness error        0.0254781   0.002252   0.02045   0.1354          0  Float64
  18 │ concavity error          0.0318937   0.0        0.02589   0.396           0  Float64
  19 │ concave points error     0.0117961   0.0        0.01093   0.05279         0  Float64
  20 │ symmetry error           0.0205423   0.007882   0.01873   0.07895         0  Float64
  21 │ fractal dimension error  0.0037949   0.0008948  0.003187  0.02984         0  Float64
  22 │ worst radius             16.2692     7.93       14.97     36.04           0  Float64
  23 │ worst texture            25.6772     12.02      25.41     49.54           0  Float64
  24 │ worst perimeter          107.261     50.41      97.66     251.2           0  Float64
  25 │ worst area               880.583     185.2      686.5     4254.0          0  Float64
  26 │ worst smoothness         0.132369    0.07117    0.1313    0.2226          0  Float64
  27 │ worst compactness        0.254265    0.02729    0.2119    1.058           0  Float64
  28 │ worst concavity          0.272188    0.0        0.2267    1.252           0  Float64
  29 │ worst concave points     0.114606    0.0        0.09993   0.291           0  Float64
  30 │ worst symmetry           0.290076    0.1565     0.2822    0.6638          0  Float64
  31 │ worst fractal dimension  0.0839458   0.05504    0.08004   0.2075          0  Float64</code></pre>
<p>As we can see the feature set consists of varying features that have different ranges and quantiles. This can cause trouble for the optimization techniques and might cause convergence issues. We can use a feature scaling technique like <strong>Standardizer&#40;&#41;</strong> to handle this.</p>
<p>But first, let&#39;s handle the <a href="https://alan-turing-institute.github.io/ScientificTypes.jl/dev/">scientific types</a> of all the features. We can use the schema&#40;&#41; method from MLJ.jl package to do this</p>
<pre><code class="language-julia">pprint&#40;schema&#40;df&#41;&#41;</code></pre><pre><code class="plaintext code-output">ScientificTypes.Schema{(:Class, Symbol("mean radius"), Symbol("mean texture"), Symbol("mean perimeter"), Symbol("mean area"), Symbol("mean smoothness"), Symbol("mean compactness"), Symbol("mean concavity"), Symbol("mean concave points"), Symbol("mean symmetry"), Symbol("mean fractal dimension"), Symbol("radius error"), Symbol("texture error"), Symbol("perimeter error"), Symbol("area error"), Symbol("smoothness error"), Symbol("compactness error"), Symbol("concavity error"), Symbol("concave points error"), Symbol("symmetry error"), Symbol("fractal dimension error"), Symbol("worst radius"), Symbol("worst texture"), Symbol("worst perimeter"), Symbol("worst area"), Symbol("worst smoothness"), Symbol("worst compactness"), Symbol("worst concavity"), Symbol("worst concave points"), Symbol("worst symmetry"), Symbol("worst fractal dimension")), Tuple{ScientificTypesBase.Textual, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous, ScientificTypesBase.Continuous}, Tuple{InlineStrings.String1, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64}}(nothing, nothing, nothing)</code></pre>
<p>As the target variable is &#39;Textual&#39; in nature, we&#39;ll have to change it to a more appropriate scientific type. Using the <strong>coerce&#40;&#41;</strong> method, let&#39;s change it to an OrderedFactor.</p>
<pre><code class="language-julia">coerce&#33;&#40;df, :Class &#61;&gt; OrderedFactor&#123;2&#125;&#41;;</code></pre>
<h2 id="unpacking_the_values"><a href="#unpacking_the_values" class="header-anchor">Unpacking the values</a></h2>
<p>Now that our data is fully processed, we can separate the target variable &#39;y&#39; from the feature set &#39;X&#39; using the <strong>unpack&#40;&#41;</strong> method.</p>
<pre><code class="language-julia">y, X &#61; unpack&#40;df, &#61;&#61;&#40;:Class&#41;,name-&gt;true, rng &#61; RANDOM_SEED&#41;;</code></pre>
<h2 id="standardizing_the_feature_set"><a href="#standardizing_the_feature_set" class="header-anchor">Standardizing the &quot;feature set&quot;</a></h2>
<p>Now that our feature set is separated from the target variable, we can use the <strong>Standardizer&#40;&#41;</strong> worklow to obtain to standadrize our feature set &#39;X&#39;.</p>
<pre><code class="language-julia">transformer_instance &#61; Standardizer&#40;&#41;
transformer_model &#61; machine&#40;transformer_instance, X&#41;
fit&#33;&#40;transformer_model&#41;
X &#61; MLJ.transform&#40;transformer_model, X&#41;;</code></pre>
<h2 id="train-test_split"><a href="#train-test_split" class="header-anchor">Train-test split</a></h2>
<p>After feature scaling, our data is ready to put into a Machine Learning model for classification&#33; Using 80&#37; of data for training, we can perform a train-test split using the <strong>partition&#40;&#41;</strong> method.</p>
<pre><code class="language-julia">train, test &#61; partition&#40;eachindex&#40;y&#41;, 0.8, shuffle&#61;true, rng&#61;RANDOM_SEED&#41;;</code></pre>
<h2 id="model_compatibility"><a href="#model_compatibility" class="header-anchor">Model compatibility</a></h2>
<p>Now that we have separate training and testing set, let&#39;s see the models compatible with our data&#33;</p>
<pre><code class="language-julia">for m in models&#40;matching&#40;X, y&#41;&#41;
    println&#40;&quot;Model name &#61; &quot;,m.name,&quot;, &quot;,&quot;Prediction type &#61; &quot;,m.prediction_type,&quot;, &quot;,&quot;Package name &#61; &quot;,m.package_name&#41;;
end</code></pre><pre><code class="plaintext code-output">Model name = AdaBoostClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = AdaBoostStumpClassifier, Prediction type = probabilistic, Package name = DecisionTree
Model name = BaggingClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = BayesianLDA, Prediction type = probabilistic, Package name = MultivariateStats
Model name = BayesianLDA, Prediction type = probabilistic, Package name = ScikitLearn
Model name = BayesianQDA, Prediction type = probabilistic, Package name = ScikitLearn
Model name = BayesianSubspaceLDA, Prediction type = probabilistic, Package name = MultivariateStats
Model name = ConstantClassifier, Prediction type = probabilistic, Package name = MLJModels
Model name = DSADDetector, Prediction type = unknown, Package name = OutlierDetectionNetworks
Model name = DecisionTreeClassifier, Prediction type = probabilistic, Package name = BetaML
Model name = DecisionTreeClassifier, Prediction type = probabilistic, Package name = DecisionTree
Model name = DeterministicConstantClassifier, Prediction type = deterministic, Package name = MLJModels
Model name = DummyClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = ESADDetector, Prediction type = unknown, Package name = OutlierDetectionNetworks
Model name = EvoTreeClassifier, Prediction type = probabilistic, Package name = EvoTrees
Model name = ExtraTreesClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = GaussianNBClassifier, Prediction type = probabilistic, Package name = NaiveBayes
Model name = GaussianNBClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = GaussianProcessClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = GradientBoostingClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = KNNClassifier, Prediction type = probabilistic, Package name = NearestNeighborModels
Model name = KNeighborsClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = KernelPerceptronClassifier, Prediction type = probabilistic, Package name = BetaML
Model name = LDA, Prediction type = probabilistic, Package name = MultivariateStats
Model name = LGBMClassifier, Prediction type = probabilistic, Package name = LightGBM
Model name = LinearBinaryClassifier, Prediction type = probabilistic, Package name = GLM
Model name = LinearSVC, Prediction type = deterministic, Package name = LIBSVM
Model name = LogisticCVClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = LogisticClassifier, Prediction type = probabilistic, Package name = MLJLinearModels
Model name = LogisticClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = MultinomialClassifier, Prediction type = probabilistic, Package name = MLJLinearModels
Model name = NeuralNetworkClassifier, Prediction type = probabilistic, Package name = MLJFlux
Model name = NuSVC, Prediction type = deterministic, Package name = LIBSVM
Model name = PassiveAggressiveClassifier, Prediction type = deterministic, Package name = ScikitLearn
Model name = PegasosClassifier, Prediction type = probabilistic, Package name = BetaML
Model name = PerceptronClassifier, Prediction type = probabilistic, Package name = BetaML
Model name = PerceptronClassifier, Prediction type = deterministic, Package name = ScikitLearn
Model name = ProbabilisticSGDClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = RandomForestClassifier, Prediction type = probabilistic, Package name = BetaML
Model name = RandomForestClassifier, Prediction type = probabilistic, Package name = DecisionTree
Model name = RandomForestClassifier, Prediction type = probabilistic, Package name = ScikitLearn
Model name = RidgeCVClassifier, Prediction type = deterministic, Package name = ScikitLearn
Model name = RidgeClassifier, Prediction type = deterministic, Package name = ScikitLearn
Model name = SGDClassifier, Prediction type = deterministic, Package name = ScikitLearn
Model name = SVC, Prediction type = deterministic, Package name = LIBSVM
Model name = SVMClassifier, Prediction type = deterministic, Package name = ScikitLearn
Model name = SVMLinearClassifier, Prediction type = deterministic, Package name = ScikitLearn
Model name = SVMNuClassifier, Prediction type = deterministic, Package name = ScikitLearn
Model name = SubspaceLDA, Prediction type = probabilistic, Package name = MultivariateStats
Model name = XGBoostClassifier, Prediction type = probabilistic, Package name = XGBoost
</code></pre>
<h2 id="analyzing_the_performance_of_different_models"><a href="#analyzing_the_performance_of_different_models" class="header-anchor">Analyzing the performance of different models</a></h2>
<p>Thats a lot of models for our data&#33; To narrow it down, lets analyze the performance of &quot;probabilistic classifiers&quot; from the &quot;ScikitLearn&quot; package.</p>
<h3 id="creating_various_empty_vectors_for_our_analysis"><a href="#creating_various_empty_vectors_for_our_analysis" class="header-anchor">Creating various empty vectors for our analysis</a></h3>
<ul>
<li><p><strong>model_names</strong> captures the names of the models being iterated</p>
</li>
<li><p><strong>loss_acc captures</strong> the value of the model accuracy on the test set</p>
</li>
<li><p><strong>loss_ce captures</strong> the values of the Cross-entropy loss on the test set</p>
</li>
<li><p><strong>loss_f1</strong> captures the values of F1-Score on the test set</p>
</li>
</ul>
<pre><code class="language-julia">model_names&#61;Vector&#123;String&#125;&#40;&#41;;
loss_acc&#61;&#91;&#93;;
loss_ce&#61;&#91;&#93;;
loss_f1&#61;&#91;&#93;;</code></pre>
<h3 id="collecting_data_for_analysis"><a href="#collecting_data_for_analysis" class="header-anchor">Collecting data for analysis</a></h3>
<pre><code class="language-julia">figure&#40;figsize&#61;&#40;8, 6&#41;&#41;
for m in models&#40;matching&#40;X, y&#41;&#41;
    if m.prediction_type&#61;&#61;Symbol&#40;&quot;probabilistic&quot;&#41; &amp;&amp; m.package_name&#61;&#61;&quot;ScikitLearn&quot; &amp;&amp; m.name&#33;&#61;&quot;LogisticCVClassifier&quot;
        #Excluding LogisticCVClassfiier as we can infer similar baseline results from the LogisticClassifier

        #Capturing the model and loading it using the @load utility
        model_name&#61;m.name
        package_name&#61;m.package_name
        eval&#40;:&#40;clf &#61; @load &#36;model_name pkg&#61;&#36;package_name verbosity&#61;1&#41;&#41;

        #Fitting the captured model onto the training set
        clf_machine &#61; machine&#40;clf&#40;&#41;, X, y&#41;
        fit&#33;&#40;clf_machine, rows&#61;train&#41;

        #Getting the predictions onto the test set
        y_pred &#61; MLJ.predict&#40;clf_machine, rows&#61;test&#41;;

        #Plotting the ROC-AUC curve for each model being iterated
        fprs, tprs, thresholds &#61; roc&#40;y_pred, y&#91;test&#93;&#41;
        plot&#40;fprs, tprs,label&#61;model_name&#41;;

        #Obtaining different evaluation metrics
        ce_loss &#61; mean&#40;cross_entropy&#40;y_pred,y&#91;test&#93;&#41;&#41;
        acc &#61; accuracy&#40;mode.&#40;y_pred&#41;, y&#91;test&#93;&#41;
        f1_score &#61; f1score&#40;mode.&#40;y_pred&#41;, y&#91;test&#93;&#41;

        #Adding the different obtained values of the evaluation metrics to the respective vectors
        push&#33;&#40;model_names, m.name&#41;
        append&#33;&#40;loss_acc, acc&#41;
        append&#33;&#40;loss_ce, ce_loss&#41;
        append&#33;&#40;loss_f1, f1_score&#41;
    end
end

#Adding labels and legend to the ROC-AUC curve
xlabel&#40;&quot;False Positive Rate&quot;&#41;
ylabel&#40;&quot;True Positive Rate&quot;&#41;
legend&#40;loc&#61;&quot;best&quot;, fontsize&#61;&quot;xx-small&quot;&#41;
title&#40;&quot;ROC curve&quot;&#41;</code></pre><pre><code class="plaintext code-output">import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
import MLJScikitLearnInterface ✔
</code></pre>
<img src="/DataScienceTutorials.jl/assets/end-to-end/breastcancer/code/output/breastcancer_auc_curve.svg" alt="ROC-AUC Curve">
<h3 id="analyzing_models"><a href="#analyzing_models" class="header-anchor">Analyzing models</a></h3>
<p>Let&#39;s collect the data in form a dataframe for a more precise analysis</p>
<pre><code class="language-julia">model_info&#61;DataFrame&#40;ModelName&#61;model_names,Accuracy&#61;loss_acc,CrossEntropyLoss&#61;loss_ce,F1Score&#61;loss_f1&#41;;</code></pre>
<p>Now, let&#39;s sort the data on basis of the Cross-entropy loss</p>
<pre><code class="language-julia">pprint&#40;sort&#33;&#40;model_info,&#91;:CrossEntropyLoss&#93;&#41;&#41;;</code></pre><pre><code class="plaintext code-output">13×4 DataFrame
 Row │ ModelName                   Accuracy  CrossEntropyLoss  F1Score
     │ String                      Any       Any               Any
─────┼──────────────────────────────────────────────────────────────────
   1 │ LogisticClassifier          0.973684  0.13142           0.962025
   2 │ BayesianLDA                 0.95614   0.145701          0.935065
   3 │ ExtraTreesClassifier        0.947368  0.15706           0.923077
   4 │ RandomForestClassifier      0.938596  0.171584          0.911392
   5 │ GradientBoostingClassifier  0.938596  0.236792          0.909091
   6 │ AdaBoostClassifier          0.95614   0.3495            0.936709
   7 │ KNeighborsClassifier        0.938596  0.432447          0.911392
   8 │ BaggingClassifier           0.929825  0.473429          0.9
   9 │ ProbabilisticSGDClassifier  0.938596  0.501896          0.915663
  10 │ GaussianProcessClassifier   0.938596  0.597197          0.909091
  11 │ GaussianNBClassifier        0.903509  0.929409          0.864198
  12 │ BayesianQDA                 0.903509  1.1303            0.857143
  13 │ DummyClassifier             0.552632  16.1248           0.385542</code></pre>
<p>It seems like a simple LogisticClassifier works really well with this dataset&#33;</p>
<h1 id="conclusion"><a href="#conclusion" class="header-anchor">Conclusion</a></h1>
<p>This article covered iterative feature selection on the Breast cancer classification dataset. In this tutorial, we only analyzed the <strong>ScikitLearn</strong> models so as to keep the flow of the content precise, but the same workflow can be applied to any compatible model in the <strong>MLJ</strong> family.</p>

<div class="page-foot">
  <div class="copyright">
    &copy; Thibaut Lienart, Anthony Blaom, Sebastian Vollmer and collaborators. Last modified: August 09, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
      </div> <!-- end of id=main -->
  </div> <!-- end of id=layout -->
  <script src="/DataScienceTutorials.jl/libs/pure/ui.min.js"></script>
  
  
      <script src="/DataScienceTutorials.jl/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

  
</body>
</html>
