<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <!-- Syntax highlighting via Prism, note: restricted langs -->
<link rel="stylesheet" href="/DataScienceTutorials.jl/libs/highlight/github.min.css">
 
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/franklin.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/pure.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/side-menu.css">
  <link rel="stylesheet" href="/DataScienceTutorials.jl/css/extra.css">
  <!-- <link rel="icon" href="/DataScienceTutorials.jl/assets/infra/favicon.gif"> -->
   <title>MLJ for Data Scientists in Two Hours</title>  
  <!-- LUNR -->
  <script src="/DataScienceTutorials.jl/libs/lunr/lunr.min.js"></script>
  <script src="/DataScienceTutorials.jl/libs/lunr/lunr_index.js"></script>
  <script src="/DataScienceTutorials.jl/libs/lunr/lunrclient.min.js"></script>
</head>
<body>
  <div id="layout">
    <!-- Menu toggle / hamburger icon -->
    <a href="#menu" id="menuLink" class="menu-link"><span></span></a>
    <div id="menu">
      <div class="pure-menu">
        <a href="/DataScienceTutorials.jl/" id="menu-logo-link">
          <div class="menu-logo">
            <!-- <img id="menu-logo" alt="MLJ Logo" src="/DataScienceTutorials.jl/assets/infra/MLJLogo2.svg" /> -->
            <p><strong>Data Science Tutorials</strong></p>
          </div>
        </a>
        <form id="lunrSearchForm" name="lunrSearchForm">
          <input class="search-input" name="q" placeholder="Enter search term" type="text">
          <input type="submit" value="Search" formaction="/DataScienceTutorials.jl/search/index.html" style="visibility:hidden">
        </form>
  <!-- LIST OF MENU ITEMS -->
  <ul class="pure-menu-list">
    <li class="pure-menu-item pure-menu-top-item "><a href="/DataScienceTutorials.jl/" class="pure-menu-link"><strong>Home</strong></a></li>

    <!-- DATA BASICS -->
    <li class="pure-menu-sublist-title"><strong>Data basics</strong></li>
    <ul class="pure-menu-sublist">
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/loading/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Loading data</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/dataframe/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data Frames</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/categorical/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Categorical Arrays</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/scitype/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Scientific Type</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/data/processing/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Data processing</a></li>
    </ul>

    <!-- GETTING STARTED WITH MLJ -->
    <li class="pure-menu-sublist-title"><strong>Getting started</strong></li>
    <ul class="pure-menu-sublist">
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/choosing-a-model/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Choosing a model</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/fit-and-predict/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/model-tuning/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Model tuning</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/ensembles-3/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Ensembles (3)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/composing-models/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Composing models</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Learning networks</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/learning-networks-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/getting-started/stacking/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Stacking</a></li>
    </ul>

    <!-- INTRO TO STATS LEARNING -->
    <li class="pure-menu-sublist-title"><strong>Intro to Stats Learning</strong></li>
    <ul class="pure-menu-sublist" id=isl>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-2/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 2</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-3/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 3</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-4/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 4</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-5/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 5</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-6b/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 6b</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-8/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 8</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-9/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 9</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/isl/lab-10/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Lab 10</a></li>
    </ul>

    <!-- END TO END EXAMPLES -->
    <li class="pure-menu-sublist-title"><strong>End to end examples</strong></li>
    <ul class="pure-menu-sublist" id=e2e>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/telco/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span>Telco Churn</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/AMES/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> AMES</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/wine/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Wine</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/crabs-xgb/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/horse/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Horse</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/HouseKingCounty/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> King County Houses</a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/airfoil" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Airfoil </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-lgbm" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Boston (lgbm) </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/glm/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Using GLM.jl </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/powergen/" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Power Generation </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/boston-flux" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Boston (Flux) </a></li>
      <li class="pure-menu-item "><a href="/DataScienceTutorials.jl/end-to-end/breastcancer" class="pure-menu-link"><span style="padding-right:0.5rem;">•</span> Breast Cancer</a></li>
    </ul>
  </ul>
  <!-- END OF LIST OF MENU ITEMS -->
      </div>
    </div>
    <div id="main"> <!-- Closed in foot -->
      

<!-- Content appended here -->
<div class="franklin-content"><h1 id="mlj_for_data_scientists_in_two_hours"><a href="#mlj_for_data_scientists_in_two_hours" class="header-anchor">MLJ for Data Scientists in Two Hours</a></h1>
<p>An end-to-end example using the <strong>Telco Churn</strong> dataset</p>
<em>Download the 
  <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-telco/tutorial.ipynb" target="_blank"><em>notebook</em></a>
  , the 
  <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-telco/tutorial.jl" target="_blank"><em>annotated script</em></a>
   or the 
  <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-telco/tutorial-raw.jl" target="_blank"><em>raw script</em></a>
   for this tutorial &#40;right-click on the relevant link and save-as&#41;. These rely on <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-telco/Project.toml">this Project.toml</a> and <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-telco/Manifest.toml">this Manifest.toml</a>.</em> <br/>   <em>You can also download the whole <a href="https://raw.githubusercontent.com/juliaai/DataScienceTutorials.jl/gh-pages/__generated/EX-telco.tar.gz">project folder</a>.</em></p>
<p><em>If you have questions or suggestions about this tutorial, please open an issue <a href="https://github.com/JuliaAI/DataScienceTutorials.jl/issues/new">here</a>.</em></p>
<p><div class="franklin-toc"><ol><li><a href="#summary_of_methods_and_types_introduced">Summary of methods and types introduced</a></li><li><a href="#warm_up_building_a_model_for_the_iris_dataset">Warm up: Building a model for the iris dataset</a></li><li><a href="#getting_the_telco_data">Getting the Telco data</a></li><li><a href="#type_coercion">Type coercion</a></li><li><a href="#preparing_a_holdout_set_for_final_testing">Preparing a holdout set for final testing</a></li><li><a href="#splitting_data_into_target_and_features">Splitting data into target and features</a></li><li><a href="#loading_a_model_and_checking_type_requirements">Loading a model and checking type requirements</a></li><li><a href="#building_a_model_pipeline_to_incorporate_feature_encoding">Building a model pipeline to incorporate feature encoding</a></li><li><a href="#evaluating_the_pipeline_models_performance">Evaluating the pipeline model&#39;s performance</a><ol><li><a href="#evaluating_by_hand_with_a_holdout_set">Evaluating by hand &#40;with a holdout set&#41;</a></li><li><a href="#automated_performance_evaluation_more_typical_workflow">Automated performance evaluation &#40;more typical workflow&#41;</a></li></ol></li><li><a href="#filtering_out_unimportant_features">Filtering out unimportant features</a></li><li><a href="#wrapping_our_iterative_model_in_control_strategies">Wrapping our iterative model in control strategies</a></li><li><a href="#hyper-parameter_optimization_model_tuning">Hyper-parameter optimization &#40;model tuning&#41;</a></li><li><a href="#saving_our_model">Saving our model</a></li><li><a href="#final_performance_estimate">Final performance estimate</a></li><li><a href="#testing_the_final_model">Testing the final model</a></li></ol></div>
<p>An application of the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/">MLJ toolbox</a> to the Telco Customer Churn dataset, aimed at practicing data scientists new to MLJ &#40;Machine Learning in Julia&#41;. This tutorial does not cover exploratory data analysis.</p>
<p>MLJ is a <em>multi-paradigm</em> machine learning toolbox &#40;i.e., not just deep-learning&#41;.</p>
<p>For other MLJ learning resources see the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/learning_mlj/">Learning MLJ</a> section of the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/">manual</a>.</p>
<p><strong>Topics covered</strong>: Grabbing and preparing a dataset, basic fit/predict workflow, constructing a pipeline to include data pre-processing, estimating performance metrics, ROC curves, confusion matrices, feature importance, basic feature selection, controlling iterative models, hyper-parameter optimization &#40;tuning&#41;.</p>
<p><strong>Prerequisites for this tutorial.</strong> Previous experience building, evaluating, and optimizing machine learning models using scikit-learn, caret, MLR, weka, or similar tool. No previous experience with MLJ. Only fairly basic familiarity with Julia is required. Uses <a href="https://dataframes.juliadata.org/stable/">DataFrames.jl</a> but in a minimal way &#40;<a href="https://ahsmart.com/pub/data-wrangling-with-data-frames-jl-cheat-sheet/index.html">this cheatsheet</a> may help&#41;.</p>
<p><strong>Time.</strong> Between two and three hours, first time through.</p>
<h2 id="summary_of_methods_and_types_introduced"><a href="#summary_of_methods_and_types_introduced" class="header-anchor">Summary of methods and types introduced</a></h2>
<table><tr><th align="left">code</th><th align="left">purpose</th></tr><tr><td align="left"><code>OpenML.load&#40;id&#41;</code></td><td align="left">grab a dataset from <a href="https://www.openml.org">OpenML.org</a></td></tr><tr><td align="left"><code>scitype&#40;X&#41;</code></td><td align="left">inspect the scientific type &#40;scitype&#41; of object <code>X</code></td></tr><tr><td align="left"><code>schema&#40;X&#41;</code></td><td align="left">inspect the column scitypes &#40;scientific types&#41; of a table <code>X</code></td></tr><tr><td align="left"><code>coerce&#40;X, ...&#41;</code></td><td align="left">fix column encodings to get appropriate scitypes</td></tr><tr><td align="left"><code>partition&#40;data, frac1, frac2, ...; rng&#61;...&#41;</code></td><td align="left">vertically split <code>data</code>, which can be a table, vector or matrix</td></tr><tr><td align="left"><code>unpack&#40;table, f1, f2, ...&#41;</code></td><td align="left">horizontally split <code>table</code> based on conditions <code>f1</code>, <code>f2</code>, ..., applied to column names</td></tr><tr><td align="left"><code>@load ModelType pkg&#61;...</code></td><td align="left">load code defining a model type</td></tr><tr><td align="left"><code>input_scitype&#40;model&#41;</code></td><td align="left">inspect the scitype that a model requires for features &#40;inputs&#41;</td></tr><tr><td align="left"><code>target_scitype&#40;model&#41;</code></td><td align="left">inspect the scitype that a model requires for the target &#40;labels&#41;</td></tr><tr><td align="left"><code>ContinuousEncoder</code></td><td align="left">built-in model type for re-encoding all features as <code>Continuous</code></td></tr><tr><td align="left"><code>model1 ∣&gt; model2 ∣&gt; ...</code></td><td align="left">combine multiple models into a pipeline</td></tr><tr><td align="left"><code>measures&#40;&quot;under curve&quot;&#41;</code></td><td align="left">list all measures &#40;metrics&#41; with string &quot;under curve&quot; in documentation</td></tr><tr><td align="left"><code>accuracy&#40;yhat, y&#41;</code></td><td align="left">compute accuracy of predictions <code>yhat</code> against ground truth observations <code>y</code></td></tr><tr><td align="left"><code>auc&#40;yhat, y&#41;, brier_loss&#40;yhat, y&#41;</code></td><td align="left">evaluate two probabilistic measures &#40;<code>yhat</code> a vector of probability distributions&#41;</td></tr><tr><td align="left"><code>machine&#40;model, X, y&#41;</code></td><td align="left">bind <code>model</code> to training data <code>X</code> &#40;features&#41; and <code>y</code> &#40;target&#41;</td></tr><tr><td align="left"><code>fit&#33;&#40;mach, rows&#61;...&#41;</code></td><td align="left">train machine using specified rows &#40;observation indices&#41;</td></tr><tr><td align="left"><code>predict&#40;mach, rows&#61;...&#41;</code>,</td><td align="left">make in-sample model predictions given specified rows</td></tr><tr><td align="left"><code>predict&#40;mach, Xnew&#41;</code></td><td align="left">make predictions given new features <code>Xnew</code></td></tr><tr><td align="left"><code>fitted_params&#40;mach&#41;</code></td><td align="left">inspect learned parameters</td></tr><tr><td align="left"><code>report&#40;mach&#41;</code></td><td align="left">inspect other outcomes of training</td></tr><tr><td align="left"><code>confmat&#40;yhat, y&#41;</code></td><td align="left">confusion matrix for predictions <code>yhat</code> and ground truth <code>y</code></td></tr><tr><td align="left"><code>roc&#40;yhat, y&#41;</code></td><td align="left">compute points on the receiver-operator Characteristic</td></tr><tr><td align="left"><code>StratifiedCV&#40;nfolds&#61;6&#41;</code></td><td align="left">6-fold stratified cross-validation resampling strategy</td></tr><tr><td align="left"><code>Holdout&#40;fraction_train&#61;0.7&#41;</code></td><td align="left">holdout resampling strategy</td></tr><tr><td align="left"><code>evaluate&#40;model, X, y; resampling&#61;..., options...&#41;</code></td><td align="left">estimate performance metrics for <code>model</code> using the data <code>X</code>, <code>y</code></td></tr><tr><td align="left"><code>FeatureSelector&#40;&#41;</code></td><td align="left">transformer for selecting features</td></tr><tr><td align="left"><code>Step&#40;3&#41;</code></td><td align="left">iteration control for stepping 3 iterations</td></tr><tr><td align="left"><code>NumberSinceBest&#40;6&#41;</code>, <code>TimeLimit&#40;60/5&#41;, InvalidValue&#40;&#41;</code></td><td align="left">stopping criterion iteration controls</td></tr><tr><td align="left"><code>IteratedModel&#40;model&#61;..., controls&#61;..., options...&#41;</code></td><td align="left">wrap an iterative <code>model</code> in controls</td></tr><tr><td align="left"><code>range&#40;model,  :some_hyperparam, lower&#61;..., upper&#61;...&#41;</code></td><td align="left">define a numeric range</td></tr><tr><td align="left"><code>RandomSearch&#40;&#41;</code></td><td align="left">random search tuning strategy</td></tr><tr><td align="left"><code>TunedModel&#40;model&#61;..., tuning&#61;..., options...&#41;</code></td><td align="left">wrap the supervised <code>model</code> in specified <code>tuning</code> strategy</td></tr></table>
<h2 id="warm_up_building_a_model_for_the_iris_dataset"><a href="#warm_up_building_a_model_for_the_iris_dataset" class="header-anchor">Warm up: Building a model for the iris dataset</a></h2>
<p>Before turning to the Telco Customer Churn dataset, we very quickly build a predictive model for Fisher&#39;s well-known iris data set, as way of introducing the main actors in any MLJ workflow. Details that you don&#39;t fully grasp should become clearer in the Telco study.</p>
<p>This section is a condensed adaption of the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/#Fit-and-predict">Getting Started example</a> in the MLJ documentation.</p>
<p>First, using the built-in iris dataset, we load and inspect the features <code>X_iris</code> &#40;a table&#41; and target variable <code>y_iris</code> &#40;a vector&#41;:</p>
<pre><code class="language-julia">using MLJ</code></pre>
<pre><code class="language-julia">X_iris, y_iris &#61; @load_iris;
schema&#40;X_iris&#41;</code></pre><pre><code class="plaintext code-output">┌──────────────┬────────────┬─────────┐
│ names        │ scitypes   │ types   │
├──────────────┼────────────┼─────────┤
│ sepal_length │ Continuous │ Float64 │
│ sepal_width  │ Continuous │ Float64 │
│ petal_length │ Continuous │ Float64 │
│ petal_width  │ Continuous │ Float64 │
└──────────────┴────────────┴─────────┘
</code></pre>
<pre><code class="language-julia">y_iris&#91;1:4&#93;</code></pre><pre><code class="plaintext code-output">4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:
 "setosa"
 "setosa"
 "setosa"
 "setosa"</code></pre>
<pre><code class="language-julia">levels&#40;y_iris&#41;</code></pre><pre><code class="plaintext code-output">3-element Vector{String}:
 "setosa"
 "versicolor"
 "virginica"</code></pre>
<p>We load a decision tree model, from the package DecisionTree.jl:</p>
<pre><code class="language-julia">DecisionTree &#61; @load DecisionTreeClassifier pkg&#61;DecisionTree # model type
model &#61; DecisionTree&#40;min_samples_split&#61;5&#41;                    # model instance</code></pre><pre><code class="plaintext code-output">import MLJDecisionTreeInterface ✔
DecisionTreeClassifier(
  max_depth = -1, 
  min_samples_leaf = 1, 
  min_samples_split = 5, 
  min_purity_increase = 0.0, 
  n_subfeatures = 0, 
  post_prune = false, 
  merge_purity_threshold = 1.0, 
  display_depth = 5, 
  rng = Random._GLOBAL_RNG())</code></pre>
<p>In MLJ, a <em>model</em> is just a container for hyper-parameters of some learning algorithm. It does not store learned parameters.</p>
<p>Next, we bind the model together with the available data in what&#39;s called a <em>machine</em>:</p>
<pre><code class="language-julia">mach &#61; machine&#40;model, X_iris, y_iris&#41;</code></pre><pre><code class="plaintext code-output">Machine trained 0 times; caches data
  model: DecisionTreeClassifier(max_depth = -1, …)
  args: 
    1:	Source @858 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`
    2:	Source @919 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`
</code></pre>
<p>A machine is essentially just a model &#40;ie, hyper-parameters&#41; plus data, but it additionally stores <em>learned parameters</em> &#40;the tree&#41; once it is trained on some view of the data:</p>
<pre><code class="language-julia">train_rows &#61; vcat&#40;1:60, 91:150&#41;; # some row indices &#40;observations are rows not columns&#41;
fit&#33;&#40;mach, rows&#61;train_rows&#41;
fitted_params&#40;mach&#41;</code></pre><pre><code class="plaintext code-output">(tree = Decision Tree
Leaves: 5
Depth:  3,
 encoding = Dict{CategoricalArrays.CategoricalValue{String, UInt32}, UInt32}("virginica" => 0x00000003, "setosa" => 0x00000001, "versicolor" => 0x00000002),
 features = [:sepal_length, :sepal_width, :petal_length, :petal_width],)</code></pre>
<p>A machine stores some other information enabling <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/machines/#Warm-restarts">warm restart</a> for some models, but we won&#39;t go into that here. You are allowed to access and mutate the <code>model</code> parameter:</p>
<pre><code class="language-julia">mach.model.min_samples_split  &#61; 10
fit&#33;&#40;mach, rows&#61;train_rows&#41; # re-train with new hyper-parameter</code></pre><pre><code class="plaintext code-output">Machine trained 2 times; caches data
  model: DecisionTreeClassifier(max_depth = -1, …)
  args: 
    1:	Source @858 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`
    2:	Source @919 ⏎ `AbstractVector{ScientificTypesBase.Multiclass{3}}`
</code></pre>
<p>Now we can make predictions on some other view of the data, as in</p>
<pre><code class="language-julia">predict&#40;mach, rows&#61;71:73&#41;</code></pre><pre><code class="plaintext code-output">3-element CategoricalDistributions.UnivariateFiniteVector{ScientificTypesBase.Multiclass{3}, String, UInt32, Float64}:
 UnivariateFinite{ScientificTypesBase.Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)
 UnivariateFinite{ScientificTypesBase.Multiclass{3}}(setosa=>0.0, versicolor=>1.0, virginica=>0.0)
 UnivariateFinite{ScientificTypesBase.Multiclass{3}}(setosa=>0.0, versicolor=>0.25, virginica=>0.75)</code></pre>
<p>or on completely new data, as in</p>
<pre><code class="language-julia">Xnew &#61; &#40;sepal_length &#61; &#91;5.1, 6.3&#93;,
        sepal_width &#61; &#91;3.0, 2.5&#93;,
        petal_length &#61; &#91;1.4, 4.9&#93;,
        petal_width &#61; &#91;0.3, 1.5&#93;&#41;
yhat &#61; predict&#40;mach, Xnew&#41;</code></pre><pre><code class="plaintext code-output">2-element CategoricalDistributions.UnivariateFiniteVector{ScientificTypesBase.Multiclass{3}, String, UInt32, Float64}:
 UnivariateFinite{ScientificTypesBase.Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)
 UnivariateFinite{ScientificTypesBase.Multiclass{3}}(setosa=>0.0, versicolor=>0.25, virginica=>0.75)</code></pre>
<p>These are probabilistic predictions which can be manipulated using a widely adopted interface defined in the Distributions.jl package. For example, we can get raw probabilities like this:</p>
<pre><code class="language-julia">pdf.&#40;yhat, &quot;virginica&quot;&#41;</code></pre><pre><code class="plaintext code-output">2-element Vector{Float64}:
 0.0
 0.75</code></pre>
<p>A single prediction is displayed like this:</p>
<pre><code class="language-julia">yhat&#91;2&#93;</code></pre><pre><code class="plaintext code-output">              UnivariateFinite{ScientificTypesBase.Multiclass{3}} 
              ┌                                        ┐ 
       setosa ┤ 0.0                                      
   versicolor ┤■■■■■■■■■■■ 0.25                          
    virginica ┤■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■ 0.75   
              └                                        ┘ </code></pre>
<p>We now turn to the Telco dataset.</p>
<h2 id="getting_the_telco_data"><a href="#getting_the_telco_data" class="header-anchor">Getting the Telco data</a></h2>
<pre><code class="language-julia">import DataFrames</code></pre>
<pre><code class="language-julia">data &#61; OpenML.load&#40;42178&#41; # data set from OpenML.org
df0 &#61; DataFrames.DataFrame&#40;data&#41;
first&#40;df0, 4&#41;</code></pre><pre><code class="plaintext code-output">4×21 DataFrame
 Row │ customerID  gender  SeniorCitizen  Partner  Dependents  tenure   PhoneService  MultipleLines     InternetService  OnlineSecurity  OnlineBackup  DeviceProtection  TechSupport  StreamingTV  StreamingMovies  Contract        PaperlessBilling  PaymentMethod              MonthlyCharges  TotalCharges  Churn
     │ String      String  Float64        String   String      Float64  String        String            String           String          String        String            String       String       String           String          String            String                     Float64         String        String
─────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ 7590-VHVEG  Female            0.0  Yes      No              1.0  No            No phone service  DSL              No              Yes           No                No           No           No               Month-to-month  Yes               Electronic check                    29.85  29.85         No
   2 │ 5575-GNVDE  Male              0.0  No       No             34.0  Yes           No                DSL              Yes             No            Yes               No           No           No               One year        No                Mailed check                        56.95  1889.5        No
   3 │ 3668-QPYBK  Male              0.0  No       No              2.0  Yes           No                DSL              Yes             Yes           No                No           No           No               Month-to-month  Yes               Mailed check                        53.85  108.15        Yes
   4 │ 7795-CFOCW  Male              0.0  No       No             45.0  No            No phone service  DSL              Yes             No            Yes               Yes          No           No               One year        No                Bank transfer (automatic)           42.3   1840.75       No</code></pre>
<p>The object of this tutorial is to build and evaluate supervised learning models to predict the <code>Churn</code> variable, a binary variable measuring customer retention, based on other variables that are relevant.</p>
<p>In the table, observations correspond to rows, and features to columns, which is the convention for representing all two-dimensional data in MLJ.</p>
<h2 id="type_coercion"><a href="#type_coercion" class="header-anchor">Type coercion</a></h2>
<p><em>Introduces:</em> <code>scitype</code>, <code>schema</code>, <code>coerce</code></p>
<p>A <a href="https://juliaai.github.io/ScientificTypes.jl/dev/">&quot;scientific type&quot;</a> or <em>scitype</em> indicates how MLJ will <em>interpret</em> data. For example, <code>typeof&#40;3.14&#41; &#61;&#61; Float64</code>, while <code>scitype&#40;3.14&#41; &#61;&#61; Continuous</code> and also <code>scitype&#40;3.14f0&#41; &#61;&#61; Continuous</code>. In MLJ, model data requirements are articulated using scitypes.</p>
<p>Here are common &quot;scalar&quot; scitypes:</p>
<img src="/DataScienceTutorials.jl/assets/end-to-end/telco/scitypes.svg" alt="scalar scitypesb">
<p>There are also container scitypes. For example, the scitype of any <code>N</code>-dimensional array is <code>AbstractArray&#123;S, N&#125;</code>, where <code>S</code> is the scitype of the elements:</p>
<pre><code class="language-julia">scitype&#40;&#91;&quot;cat&quot;, &quot;mouse&quot;, &quot;dog&quot;&#93;&#41;</code></pre><pre><code class="plaintext code-output">AbstractVector{Textual} (alias for AbstractArray{ScientificTypesBase.Textual, 1})</code></pre>
<p>The <code>schema</code> operator summarizes the column scitypes of a table:</p>
<pre><code class="language-julia">schema&#40;df0&#41; |&gt; DataFrames.DataFrame  # converted to DataFrame for better display</code></pre><pre><code class="plaintext code-output">21×3 DataFrame
 Row │ names             scitypes    types
     │ Symbol            DataType    DataType
─────┼────────────────────────────────────────
   1 │ customerID        Textual     String
   2 │ gender            Textual     String
   3 │ SeniorCitizen     Continuous  Float64
   4 │ Partner           Textual     String
   5 │ Dependents        Textual     String
   6 │ tenure            Continuous  Float64
   7 │ PhoneService      Textual     String
   8 │ MultipleLines     Textual     String
   9 │ InternetService   Textual     String
  10 │ OnlineSecurity    Textual     String
  11 │ OnlineBackup      Textual     String
  12 │ DeviceProtection  Textual     String
  13 │ TechSupport       Textual     String
  14 │ StreamingTV       Textual     String
  15 │ StreamingMovies   Textual     String
  16 │ Contract          Textual     String
  17 │ PaperlessBilling  Textual     String
  18 │ PaymentMethod     Textual     String
  19 │ MonthlyCharges    Continuous  Float64
  20 │ TotalCharges      Textual     String
  21 │ Churn             Textual     String</code></pre>
<p>All of the fields being interpreted as <code>Textual</code> are really something else, either <code>Multiclass</code> or, in the case of <code>TotalCharges</code>, <code>Continuous</code>. In fact, <code>TotalCharges</code> is mostly floats wrapped as strings. However, it needs special treatment because some elements consist of a single space, &quot; &quot;, which we&#39;ll treat as &quot;0.0&quot;.</p>
<pre><code class="language-julia">fix_blanks&#40;v&#41; &#61; map&#40;v&#41; do x
    if x &#61;&#61; &quot; &quot;
        return &quot;0.0&quot;
    else
        return x
    end
end

df0.TotalCharges &#61; fix_blanks&#40;df0.TotalCharges&#41;;</code></pre>
<p>Coercing the <code>TotalCharges</code> type to ensure a <code>Continuous</code> scitype:</p>
<pre><code class="language-julia">coerce&#33;&#40;df0, :TotalCharges &#61;&gt; Continuous&#41;;</code></pre>
<p>Coercing all remaining <code>Textual</code> data to <code>Multiclass</code>:</p>
<pre><code class="language-julia">coerce&#33;&#40;df0, Textual &#61;&gt; Multiclass&#41;;</code></pre>
<p>Finally, we&#39;ll coerce our target variable <code>Churn</code> to be <code>OrderedFactor</code>, rather than <code>Multiclass</code>, to enable a reliable interpretation of metrics like &quot;true positive rate&quot;.  By convention, the first class is the negative one:</p>
<pre><code class="language-julia">coerce&#33;&#40;df0, :Churn &#61;&gt; OrderedFactor&#41;
levels&#40;df0.Churn&#41; # to check order</code></pre><pre><code class="plaintext code-output">2-element Vector{String}:
 "No"
 "Yes"</code></pre>
<p>Re-inspecting the scitypes:</p>
<pre><code class="language-julia">schema&#40;df0&#41; |&gt; DataFrames.DataFrame</code></pre><pre><code class="plaintext code-output">21×3 DataFrame
 Row │ names             scitypes          types
     │ Symbol            DataType          DataType
─────┼──────────────────────────────────────────────────────────────────────
   1 │ customerID        Multiclass{7043}  CategoricalValue{String, UInt32}
   2 │ gender            Multiclass{2}     CategoricalValue{String, UInt32}
   3 │ SeniorCitizen     Continuous        Float64
   4 │ Partner           Multiclass{2}     CategoricalValue{String, UInt32}
   5 │ Dependents        Multiclass{2}     CategoricalValue{String, UInt32}
   6 │ tenure            Continuous        Float64
   7 │ PhoneService      Multiclass{2}     CategoricalValue{String, UInt32}
   8 │ MultipleLines     Multiclass{3}     CategoricalValue{String, UInt32}
   9 │ InternetService   Multiclass{3}     CategoricalValue{String, UInt32}
  10 │ OnlineSecurity    Multiclass{3}     CategoricalValue{String, UInt32}
  11 │ OnlineBackup      Multiclass{3}     CategoricalValue{String, UInt32}
  12 │ DeviceProtection  Multiclass{3}     CategoricalValue{String, UInt32}
  13 │ TechSupport       Multiclass{3}     CategoricalValue{String, UInt32}
  14 │ StreamingTV       Multiclass{3}     CategoricalValue{String, UInt32}
  15 │ StreamingMovies   Multiclass{3}     CategoricalValue{String, UInt32}
  16 │ Contract          Multiclass{3}     CategoricalValue{String, UInt32}
  17 │ PaperlessBilling  Multiclass{2}     CategoricalValue{String, UInt32}
  18 │ PaymentMethod     Multiclass{4}     CategoricalValue{String, UInt32}
  19 │ MonthlyCharges    Continuous        Float64
  20 │ TotalCharges      Continuous        Float64
  21 │ Churn             OrderedFactor{2}  CategoricalValue{String, UInt32}</code></pre>
<h2 id="preparing_a_holdout_set_for_final_testing"><a href="#preparing_a_holdout_set_for_final_testing" class="header-anchor">Preparing a holdout set for final testing</a></h2>
<p><em>Introduces:</em> <code>partition</code></p>
<p>To reduce training times for the purposes of this tutorial, we&#39;re going to dump 90&#37; of observations &#40;after shuffling&#41; and split off 30&#37; of the remainder for use as a lock-and-throw-away-the-key holdout set:</p>
<pre><code class="language-julia">df, df_test, df_dumped &#61; partition&#40;df0, 0.07, 0.03, # in ratios 7:3:90
                                   stratify&#61;df0.Churn,
                                   rng&#61;123&#41;;</code></pre>
<p>The reader interested in including all data can instead do <code>df, df_test &#61; partition&#40;df0, 0.7, rng&#61;123&#41;</code>.</p>
<h2 id="splitting_data_into_target_and_features"><a href="#splitting_data_into_target_and_features" class="header-anchor">Splitting data into target and features</a></h2>
<p><em>Introduces:</em> <code>unpack</code></p>
<p>In the following call, the column with name <code>Churn</code> is copied over to a vector <code>y</code>, and every remaining column, except <code>customerID</code> &#40;which contains no useful information&#41; goes into a table <code>X</code>. Here <code>Churn</code> is the target variable for which we seek predictions, given new versions of the features <code>X</code>.</p>
<pre><code class="language-julia">y, X &#61; unpack&#40;df, &#61;&#61;&#40;:Churn&#41;, &#33;&#61;&#40;:customerID&#41;&#41;;
schema&#40;X&#41;.names</code></pre><pre><code class="plaintext code-output">(:gender, :SeniorCitizen, :Partner, :Dependents, :tenure, :PhoneService, :MultipleLines, :InternetService, :OnlineSecurity, :OnlineBackup, :DeviceProtection, :TechSupport, :StreamingTV, :StreamingMovies, :Contract, :PaperlessBilling, :PaymentMethod, :MonthlyCharges, :TotalCharges)</code></pre>
<pre><code class="language-julia">intersect&#40;&#91;:Churn, :customerID&#93;, schema&#40;X&#41;.names&#41;</code></pre><pre><code class="plaintext code-output">Symbol[]</code></pre>
<p>We&#39;ll do the same for the holdout data:</p>
<pre><code class="language-julia">ytest, Xtest &#61; unpack&#40;df_test, &#61;&#61;&#40;:Churn&#41;, &#33;&#61;&#40;:customerID&#41;&#41;;</code></pre>
<h2 id="loading_a_model_and_checking_type_requirements"><a href="#loading_a_model_and_checking_type_requirements" class="header-anchor">Loading a model and checking type requirements</a></h2>
<p><em>Introduces:</em> <code>@load</code>, <code>input_scitype</code>, <code>target_scitype</code></p>
<p>For tools helping us to identify suitable models, see the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/model_search/#model_search">Model Search</a> section of the manual. We will build a gradient tree-boosting model, a popular first choice for structured data like we have here. Model code is contained in a third-party package called <a href="https://github.com/Evovest/EvoTrees.jl">EvoTrees.jl</a> which is loaded as follows:</p>
<pre><code class="language-julia">Booster &#61; @load EvoTreeClassifier pkg&#61;EvoTrees</code></pre><pre><code class="plaintext code-output">import EvoTrees ✔
EvoTrees.EvoTreeClassifier</code></pre>
<p>Recall that a <em>model</em> is just a container for some algorithm&#39;s hyper-parameters. Let&#39;s create a <code>Booster</code> with default values for the hyper-parameters:</p>
<pre><code class="language-julia">booster &#61; Booster&#40;&#41;</code></pre><pre><code class="plaintext code-output">EvoTreeClassifier(
  loss = EvoTrees.Softmax(), 
  nrounds = 10, 
  λ = 0.0, 
  γ = 0.0, 
  η = 0.1, 
  max_depth = 5, 
  min_weight = 1.0, 
  rowsample = 1.0, 
  colsample = 1.0, 
  nbins = 64, 
  α = 0.5, 
  metric = :mlogloss, 
  rng = Random.MersenneTwister(123), 
  device = "cpu")</code></pre>
<p>This model is appropriate for the kind of target variable we have because of the following passing test:</p>
<pre><code class="language-julia">scitype&#40;y&#41; &lt;: target_scitype&#40;booster&#41;</code></pre><pre><code class="plaintext code-output">true</code></pre>
<p>However, our features <code>X</code> cannot be directly used with <code>booster</code>:</p>
<pre><code class="language-julia">scitype&#40;X&#41; &lt;: input_scitype&#40;booster&#41;</code></pre><pre><code class="plaintext code-output">false</code></pre>
<p>As it turns out, this is because <code>booster</code>, like the majority of MLJ supervised models, expects the features to be <code>Continuous</code>. &#40;With some experience, this can be gleaned from <code>input_scitype&#40;booster&#41;</code>.&#41; So we need categorical feature encoding, discussed next.</p>
<h2 id="building_a_model_pipeline_to_incorporate_feature_encoding"><a href="#building_a_model_pipeline_to_incorporate_feature_encoding" class="header-anchor">Building a model pipeline to incorporate feature encoding</a></h2>
<p><em>Introduces:</em> <code>ContinuousEncoder</code>, pipeline operator <code>|&gt;</code></p>
<p>The built-in <code>ContinuousEncoder</code> model transforms an arbitrary table to a table whose features are all <code>Continuous</code> &#40;dropping any fields it does not know how to encode&#41;. In particular, all <code>Multiclass</code> features are one-hot encoded.</p>
<p>A <em>pipeline</em> is a stand-alone model that internally combines one or more models in a linear &#40;non-branching&#41; pipeline. Here&#39;s a pipeline that adds the <code>ContinuousEncoder</code> as a pre-processor to the gradient tree-boosting model above:</p>
<pre><code class="language-julia">pipe &#61; ContinuousEncoder&#40;&#41; |&gt; booster</code></pre><pre><code class="plaintext code-output">ProbabilisticPipeline(
  continuous_encoder = ContinuousEncoder(
        drop_last = false, 
        one_hot_ordered_factors = false), 
  evo_tree_classifier = EvoTreeClassifier(
        loss = EvoTrees.Softmax(), 
        nrounds = 10, 
        λ = 0.0, 
        γ = 0.0, 
        η = 0.1, 
        max_depth = 5, 
        min_weight = 1.0, 
        rowsample = 1.0, 
        colsample = 1.0, 
        nbins = 64, 
        α = 0.5, 
        metric = :mlogloss, 
        rng = Random.MersenneTwister(123), 
        device = "cpu"), 
  cache = true)</code></pre>
<p>Note that the component models appear as hyper-parameters of <code>pipe</code>. Pipelines are an implementation of a more general <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/composing_models/#Composing-Models">model composition</a> interface provided by MLJ that advanced users may want to learn about.</p>
<p>From the above display, we see that component model hyper-parameters are now <em>nested</em>, but they are still accessible &#40;important in hyper-parameter optimization&#41;:</p>
<pre><code class="language-julia">pipe.evo_tree_classifier.max_depth</code></pre><pre><code class="plaintext code-output">5</code></pre>
<h2 id="evaluating_the_pipeline_models_performance"><a href="#evaluating_the_pipeline_models_performance" class="header-anchor">Evaluating the pipeline model&#39;s performance</a></h2>
<p><em>Introduces:</em> <code>measures</code> &#40;function&#41;, <strong>measures:</strong> <code>brier_loss</code>, <code>auc</code>, <code>accuracy</code>; <code>machine</code>, <code>fit&#33;</code>, <code>predict</code>, <code>fitted_params</code>, <code>report</code>, <code>roc</code>, <strong>resampling strategy</strong> <code>StratifiedCV</code>, <code>evaluate</code>, <code>FeatureSelector</code></p>
<p>Without touching our test set <code>Xtest</code>, <code>ytest</code>, we will estimate the performance of our pipeline model, with default hyper-parameters, in two different ways:</p>
<p><strong>Evaluating by hand.</strong> First, we&#39;ll do this &quot;by hand&quot; using the <code>fit&#33;</code> and <code>predict</code> workflow illustrated for the iris data set above, using a holdout resampling strategy. At the same time we&#39;ll see how to generate a <strong>confusion matrix</strong>, <strong>ROC curve</strong>, and inspect <strong>feature importances</strong>.</p>
<p><strong>Automated performance evaluation.</strong> Next we&#39;ll apply the more typical and convenient <code>evaluate</code> workflow, but using <code>StratifiedCV</code> &#40;stratified cross-validation&#41; which is more informative.</p>
<p>In any case, we need to choose some measures &#40;metrics&#41; to quantify the performance of our model. For a complete list of measures, one does <code>measures&#40;&#41;</code>. Or we also can do:</p>
<pre><code class="language-julia">measures&#40;&quot;Brier&quot;&#41;</code></pre><pre><code class="plaintext code-output">2-element Vector{NamedTuple{(:name, :instances, :human_name, :target_scitype, :supports_weights, :supports_class_weights, :prediction_type, :orientation, :reports_each_observation, :aggregation, :is_feature_dependent, :docstring, :distribution_type)}}:
 (name = BrierLoss, instances = [brier_loss], ...)
 (name = BrierScore, instances = [brier_score], ...)</code></pre>
<p>We will be primarily using <code>brier_loss</code>, but also <code>auc</code> &#40;area under the ROC curve&#41; and <code>accuracy</code>.</p>
<h3 id="evaluating_by_hand_with_a_holdout_set"><a href="#evaluating_by_hand_with_a_holdout_set" class="header-anchor">Evaluating by hand &#40;with a holdout set&#41;</a></h3>
<p>Our pipeline model can be trained just like the decision tree model we built for the iris data set. Binding all non-test data to the pipeline model:</p>
<pre><code class="language-julia">mach_pipe &#61; machine&#40;pipe, X, y&#41;</code></pre><pre><code class="plaintext code-output">Machine trained 0 times; caches data
  model: ProbabilisticPipeline(continuous_encoder = ContinuousEncoder(drop_last = false, …), …)
  args: 
    1:	Source @885 ⏎ `ScientificTypesBase.Table{Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Multiclass{2}}, AbstractVector{ScientificTypesBase.Multiclass{4}}, AbstractVector{ScientificTypesBase.Multiclass{3}}}}`
    2:	Source @785 ⏎ `AbstractVector{ScientificTypesBase.OrderedFactor{2}}`
</code></pre>
<p>We already encountered the <code>partition</code> method above. Here we apply it to row indices, instead of data containers, as <code>fit&#33;</code> and <code>predict</code> only need a <em>view</em> of the data to work.</p>
<pre><code class="language-julia">train, validation &#61; partition&#40;1:length&#40;y&#41;, 0.7&#41;
fit&#33;&#40;mach_pipe, rows&#61;train&#41;</code></pre><pre><code class="plaintext code-output">Machine trained 1 time; caches data
  model: ProbabilisticPipeline(continuous_encoder = ContinuousEncoder(drop_last = false, …), …)
  args: 
    1:	Source @885 ⏎ `ScientificTypesBase.Table{Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Multiclass{2}}, AbstractVector{ScientificTypesBase.Multiclass{4}}, AbstractVector{ScientificTypesBase.Multiclass{3}}}}`
    2:	Source @785 ⏎ `AbstractVector{ScientificTypesBase.OrderedFactor{2}}`
</code></pre>
<p>We note in passing that we can access two kinds of information from a trained machine:</p>
<ul>
<li><p>The <strong>learned parameters</strong> &#40;eg, coefficients of a linear model&#41;: We use <code>fitted_params&#40;mach_pipe&#41;</code></p>
</li>
<li><p>Other <strong>by-products of training</strong> &#40;eg, feature importances&#41;: We use <code>report&#40;mach_pipe&#41;</code></p>
</li>
</ul>
<pre><code class="language-julia">fp &#61; fitted_params&#40;mach_pipe&#41;;
keys&#40;fp&#41;</code></pre><pre><code class="plaintext code-output">(:evo_tree_classifier, :continuous_encoder, :machines, :fitted_params_given_machine)</code></pre>
<p>For example, we can check that the encoder did not actually drop any features:</p>
<pre><code class="language-julia">Set&#40;fp.continuous_encoder.features_to_keep&#41; &#61;&#61; Set&#40;schema&#40;X&#41;.names&#41;</code></pre><pre><code class="plaintext code-output">true</code></pre>
<p>And, from the report, extract feature importances:</p>
<pre><code class="language-julia">rpt &#61; report&#40;mach_pipe&#41;
keys&#40;rpt.evo_tree_classifier&#41;</code></pre><pre><code class="plaintext code-output">(:feature_importances,)</code></pre>
<pre><code class="language-julia">fi &#61; rpt.evo_tree_classifier.feature_importances
feature_importance_table &#61;
    &#40;feature&#61;Symbol.&#40;first.&#40;fi&#41;&#41;, importance&#61;last.&#40;fi&#41;&#41; |&gt; DataFrames.DataFrame</code></pre><pre><code class="plaintext code-output">45×2 DataFrame
 Row │ feature                            importance
     │ Symbol                             Float64
─────┼────────────────────────────────────────────────
   1 │ tenure                             0.367021
   2 │ MonthlyCharges                     0.167477
   3 │ Contract__Month-to-month           0.16637
   4 │ TotalCharges                       0.0608734
   5 │ PaymentMethod__Bank transfer (au…  0.0381171
   6 │ Dependents__No                     0.0273874
   7 │ SeniorCitizen                      0.0262011
   8 │ PaperlessBilling__No               0.024691
   9 │ DeviceProtection__No               0.0234396
  10 │ StreamingMovies__Yes               0.0182277
  11 │ gender__Female                     0.0133198
  12 │ OnlineBackup__Yes                  0.010152
  13 │ MultipleLines__No                  0.0093157
  14 │ StreamingTV__No                    0.00695159
  15 │ DeviceProtection__Yes              0.00653241
  16 │ PaperlessBilling__Yes              0.00581502
  17 │ TechSupport__No                    0.00460489
  18 │ PaymentMethod__Credit card (auto…  0.00369404
  19 │ StreamingMovies__No                0.00352602
  20 │ Contract__One year                 0.00339499
  21 │ Contract__Two year                 0.00316092
  22 │ Partner__Yes                       0.00292232
  23 │ OnlineBackup__No                   0.0029127
  24 │ Dependents__Yes                    0.00225914
  25 │ Partner__No                        0.00145106
  26 │ gender__Male                       0.000181241
  27 │ PaymentMethod__Mailed check        1.17948e-6
  28 │ OnlineSecurity__No internet serv…  0.0
  29 │ OnlineSecurity__No                 0.0
  30 │ PaymentMethod__Electronic check    0.0
  31 │ OnlineSecurity__Yes                0.0
  32 │ InternetService__Fiber optic       0.0
  33 │ TechSupport__Yes                   0.0
  34 │ OnlineBackup__No internet service  0.0
  35 │ InternetService__No                0.0
  36 │ StreamingTV__Yes                   0.0
  37 │ PhoneService__No                   0.0
  38 │ PhoneService__Yes                  0.0
  39 │ DeviceProtection__No internet se…  0.0
  40 │ StreamingTV__No internet service   0.0
  41 │ StreamingMovies__No internet ser…  0.0
  42 │ TechSupport__No internet service   0.0
  43 │ MultipleLines__Yes                 0.0
  44 │ MultipleLines__No phone service    0.0
  45 │ InternetService__DSL               0.0</code></pre>
<p>For models not reporting feature importances, we recommend the <a href="https://expandingman.gitlab.io/Shapley.jl/">Shapley.jl</a> package.</p>
<p>Returning to predictions and evaluations of our measures:</p>
<pre><code class="language-julia">ŷ &#61; predict&#40;mach_pipe, rows&#61;validation&#41;;
print&#40;
    &quot;Measurements:\n&quot;,
    &quot;  brier loss: &quot;, brier_loss&#40;ŷ, y&#91;validation&#93;&#41; |&gt; mean, &quot;\n&quot;,
    &quot;  auc:        &quot;, auc&#40;ŷ, y&#91;validation&#93;&#41;,                &quot;\n&quot;,
    &quot;  accuracy:   &quot;, accuracy&#40;mode.&#40;ŷ&#41;, y&#91;validation&#93;&#41;
&#41;</code></pre><pre><code class="plaintext code-output">Measurements:
  brier loss: 0.27683139589958233
  auc:        0.8171277997364954
  accuracy:   0.7972972972972973</code></pre>
<p>Note that we need <code>mode</code> in the last case because <code>accuracy</code> expects point predictions, not probabilistic ones. &#40;One can alternatively use <code>predict_mode</code> to generate the predictions.&#41;</p>
<p>While we&#39;re here, lets also generate a <strong>confusion matrix</strong> and <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">receiver-operator characteristic</a> &#40;ROC&#41;:</p>
<pre><code class="language-julia">confmat&#40;mode.&#40;ŷ&#41;, y&#91;validation&#93;&#41;</code></pre><pre><code class="plaintext code-output">              ┌───────────────────────────┐
              │       Ground Truth        │
┌─────────────┼─────────────┬─────────────┤
│  Predicted  │     No      │     Yes     │
├─────────────┼─────────────┼─────────────┤
│     No      │     101     │     16      │
├─────────────┼─────────────┼─────────────┤
│     Yes     │     14      │     17      │
└─────────────┴─────────────┴─────────────┘
</code></pre>
<p>Note: Importing the plotting package and calling the plotting functions for the first time can take a minute or so.</p>
<pre><code class="language-julia">using Plots</code></pre>
<pre><code class="language-julia">roc_curve &#61; roc&#40;ŷ, y&#91;validation&#93;&#41;
plt &#61; scatter&#40;roc_curve, legend&#61;false&#41;
plot&#33;&#40;plt, xlab&#61;&quot;false positive rate&quot;, ylab&#61;&quot;true positive rate&quot;&#41;
plot&#33;&#40;&#91;0, 1&#93;, &#91;0, 1&#93;, linewidth&#61;2, linestyle&#61;:dash, color&#61;:black&#41;</code></pre>
<img src="/DataScienceTutorials.jl/assets/end-to-end/telco/code/output/EX-telco-roc.svg" alt="">
<h3 id="automated_performance_evaluation_more_typical_workflow"><a href="#automated_performance_evaluation_more_typical_workflow" class="header-anchor">Automated performance evaluation &#40;more typical workflow&#41;</a></h3>
<p>We can also get performance estimates with a single call to the <code>evaluate</code> function, which also allows for more complicated resampling - in this case stratified cross-validation. To make this more comprehensive, we set <code>repeats&#61;3</code> below to make our cross-validation &quot;Monte Carlo&quot; &#40;3 random size-6 partitions of the observation space, for a total of 18 folds&#41; and set <code>acceleration&#61;CPUThreads&#40;&#41;</code> to parallelize the computation.</p>
<p>We choose a <code>StratifiedCV</code> resampling strategy; the complete list of options is <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/#Built-in-resampling-strategies">here</a>.</p>
<pre><code class="language-julia">e_pipe &#61; evaluate&#40;pipe, X, y,
                  resampling&#61;StratifiedCV&#40;nfolds&#61;6, rng&#61;123&#41;,
                  measures&#61;&#91;brier_loss, auc, accuracy&#93;,
                  repeats&#61;3,
                  acceleration&#61;CPUThreads&#40;&#41;&#41;</code></pre><pre><code class="plaintext code-output">PerformanceEvaluation object with these fields:
  measure, measurement, operation, per_fold,
  per_observation, fitted_params_per_fold,
  report_per_fold, train_test_rows
Extract:
┌──────────────────┬─────────────┬──────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ measure          │ measurement │ operation    │ per_fold                                                                                                                       │
├──────────────────┼─────────────┼──────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ BrierLoss()      │ 0.313       │ predict      │ [0.311, 0.344, 0.291, 0.348, 0.357, 0.295, 0.285, 0.313, 0.324, 0.296, 0.284, 0.343, 0.316, 0.268, 0.306, 0.369, 0.287, 0.298] │
│ AreaUnderCurve() │ 0.788       │ predict      │ [0.778, 0.743, 0.802, 0.774, 0.724, 0.808, 0.832, 0.788, 0.743, 0.82, 0.862, 0.692, 0.761, 0.845, 0.827, 0.734, 0.833, 0.824]  │
│ Accuracy()       │ 0.783       │ predict_mode │ [0.771, 0.78, 0.78, 0.78, 0.732, 0.817, 0.795, 0.78, 0.78, 0.793, 0.805, 0.768, 0.783, 0.841, 0.78, 0.744, 0.793, 0.768]       │
└──────────────────┴─────────────┴──────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
</code></pre>
<p>&#40;There is also a version of <code>evaluate</code> for machines. Query the <code>evaluate</code> and <code>evaluate&#33;</code> doc-strings to learn more about these functions and what the <code>PerformanceEvaluation</code> object <code>e_pipe</code> records.&#41;</p>
<p>While <a href="https://arxiv.org/abs/2104.00673">less than ideal</a>, let&#39;s adopt the common practice of using the standard error of a cross-validation score as an estimate of the uncertainty of a performance measure&#39;s expected value. Here&#39;s a utility function to calculate 95&#37; confidence intervals for our performance estimates based on this practice, and it&#39;s application to the current evaluation:</p>
<pre><code class="language-julia">using Measurements</code></pre>
<pre><code class="language-julia">function confidence_intervals&#40;e&#41;
    factor &#61; 2.0 # to get level of 95&#37;
    measure &#61; e.measure
    nfolds &#61; length&#40;e.per_fold&#91;1&#93;&#41;
    measurement &#61; &#91;e.measurement&#91;j&#93; ± factor*std&#40;e.per_fold&#91;j&#93;&#41;/sqrt&#40;nfolds - 1&#41;
                   for j in eachindex&#40;measure&#41;&#93;
    table &#61; &#40;measure&#61;measure, measurement&#61;measurement&#41;
    return DataFrames.DataFrame&#40;table&#41;
end

confidence_intervals_basic_model &#61; confidence_intervals&#40;e_pipe&#41;</code></pre><pre><code class="plaintext code-output">3×2 DataFrame
 Row │ measure           measurement
     │ Measure           Measuremen…
─────┼───────────────────────────────
   1 │ BrierLoss()       0.313±0.014
   2 │ AreaUnderCurve()  0.788±0.023
   3 │ Accuracy()        0.783±0.012</code></pre>
<h2 id="filtering_out_unimportant_features"><a href="#filtering_out_unimportant_features" class="header-anchor">Filtering out unimportant features</a></h2>
<p><em>Introduces:</em> <code>FeatureSelector</code></p>
<p>Before continuing, we&#39;ll modify our pipeline to drop those features with low feature importance, to speed up later optimization:</p>
<pre><code class="language-julia">unimportant_features &#61; filter&#40;:importance &#61;&gt; &lt;&#40;0.005&#41;, feature_importance_table&#41;.feature

pipe2 &#61; ContinuousEncoder&#40;&#41; |&gt;
    FeatureSelector&#40;features&#61;unimportant_features, ignore&#61;true&#41; |&gt; booster</code></pre><pre><code class="plaintext code-output">ProbabilisticPipeline(
  continuous_encoder = ContinuousEncoder(
        drop_last = false, 
        one_hot_ordered_factors = false), 
  feature_selector = FeatureSelector(
        features = [:TechSupport__No, Symbol("PaymentMethod__Credit card (automatic)"), :StreamingMovies__No, Symbol("Contract__One year"), Symbol("Contract__Two year"), :Partner__Yes, :OnlineBackup__No, :Dependents__Yes, :Partner__No, :gender__Male, Symbol("PaymentMethod__Mailed check"), Symbol("OnlineSecurity__No internet service"), :OnlineSecurity__No, Symbol("PaymentMethod__Electronic check"), :OnlineSecurity__Yes, Symbol("InternetService__Fiber optic"), :TechSupport__Yes, Symbol("OnlineBackup__No internet service"), :InternetService__No, :StreamingTV__Yes, :PhoneService__No, :PhoneService__Yes, Symbol("DeviceProtection__No internet service"), Symbol("StreamingTV__No internet service"), Symbol("StreamingMovies__No internet service"), Symbol("TechSupport__No internet service"), :MultipleLines__Yes, Symbol("MultipleLines__No phone service"), :InternetService__DSL], 
        ignore = true), 
  evo_tree_classifier = EvoTreeClassifier(
        loss = EvoTrees.Softmax(), 
        nrounds = 10, 
        λ = 0.0, 
        γ = 0.0, 
        η = 0.1, 
        max_depth = 5, 
        min_weight = 1.0, 
        rowsample = 1.0, 
        colsample = 1.0, 
        nbins = 64, 
        α = 0.5, 
        metric = :mlogloss, 
        rng = Random.MersenneTwister(123, (0, 86172, 85170, 780)), 
        device = "cpu"), 
  cache = true)</code></pre>
<h2 id="wrapping_our_iterative_model_in_control_strategies"><a href="#wrapping_our_iterative_model_in_control_strategies" class="header-anchor">Wrapping our iterative model in control strategies</a></h2>
<p><em>Introduces:</em> <strong>control strategies:</strong> <code>Step</code>, <code>NumberSinceBest</code>, <code>TimeLimit</code>, <code>InvalidValue</code>, <strong>model wrapper</strong> <code>IteratedModel</code>, <strong>resampling strategy:</strong> <code>Holdout</code></p>
<p>We want to optimize the hyper-parameters of our model. Since our model is iterative, these parameters include the &#40;nested&#41; iteration parameter <code>pipe.evo_tree_classifier.nrounds</code>. Sometimes this parameter is optimized first, fixed, and then maybe optimized again after the other parameters. Here we take a more principled approach, <strong>wrapping our model in a control strategy</strong> that makes it &quot;self-iterating&quot;. The strategy applies a stopping criterion to <em>out-of-sample</em> estimates of the model performance, constructed using an internally constructed holdout set. In this way, we avoid some data hygiene issues, and, when we subsequently optimize other parameters, we will always being using an optimal number of iterations.</p>
<p>Note that this approach can be applied to any iterative MLJ model, eg, the neural network models provided by <a href="https://github.com/FluxML/MLJFlux.jl">MLJFlux.jl</a>.</p>
<p>First, we select appropriate controls from <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/#Controls-provided">this list</a>:</p>
<pre><code class="language-julia">controls &#61; &#91;
    Step&#40;1&#41;,              # to increment iteration parameter &#40;&#96;pipe.nrounds&#96;&#41;
    NumberSinceBest&#40;4&#41;,   # main stopping criterion
    TimeLimit&#40;2/3600&#41;,    # never train more than 2 sec
    InvalidValue&#40;&#41;        # stop if NaN or ±Inf encountered
&#93;</code></pre><pre><code class="plaintext code-output">4-element Vector{Any}:
 IterationControl.Step(1)
 EarlyStopping.NumberSinceBest(4)
 EarlyStopping.TimeLimit(Dates.Millisecond(2000))
 EarlyStopping.InvalidValue()</code></pre>
<p>Now we wrap our pipeline model using the <code>IteratedModel</code> wrapper, being sure to specify the <code>measure</code> on which internal estimates of the out-of-sample performance will be based:</p>
<pre><code class="language-julia">iterated_pipe &#61; IteratedModel&#40;model&#61;pipe2,
                              controls&#61;controls,
                              measure&#61;brier_loss,
                              resampling&#61;Holdout&#40;fraction_train&#61;0.7&#41;&#41;</code></pre><pre><code class="plaintext code-output">ProbabilisticIteratedModel(
  model = ProbabilisticPipeline(
        continuous_encoder = ContinuousEncoder(drop_last = false, …), 
        feature_selector = FeatureSelector(features = [:TechSupport__No, Symbol("PaymentMethod__Credit card (automatic)"), :StreamingMovies__No, Symbol("Contract__One year"), Symbol("Contract__Two year"), :Partner__Yes, :OnlineBackup__No, :Dependents__Yes, :Partner__No, :gender__Male, Symbol("PaymentMethod__Mailed check"), Symbol("OnlineSecurity__No internet service"), :OnlineSecurity__No, Symbol("PaymentMethod__Electronic check"), :OnlineSecurity__Yes, Symbol("InternetService__Fiber optic"), :TechSupport__Yes, Symbol("OnlineBackup__No internet service"), :InternetService__No, :StreamingTV__Yes, :PhoneService__No, :PhoneService__Yes, Symbol("DeviceProtection__No internet service"), Symbol("StreamingTV__No internet service"), Symbol("StreamingMovies__No internet service"), Symbol("TechSupport__No internet service"), :MultipleLines__Yes, Symbol("MultipleLines__No phone service"), :InternetService__DSL], …), 
        evo_tree_classifier = EvoTreeClassifier(loss = EvoTrees.Softmax(), …), 
        cache = true), 
  controls = Any[IterationControl.Step(1), EarlyStopping.NumberSinceBest(4), EarlyStopping.TimeLimit(Dates.Millisecond(2000)), EarlyStopping.InvalidValue()], 
  resampling = Holdout(
        fraction_train = 0.7, 
        shuffle = false, 
        rng = Random._GLOBAL_RNG()), 
  measure = BrierLoss(), 
  weights = nothing, 
  class_weights = nothing, 
  operation = MLJModelInterface.predict, 
  retrain = false, 
  check_measure = true, 
  iteration_parameter = nothing, 
  cache = true)</code></pre>
<p>We&#39;ve set <code>resampling&#61;Holdout&#40;fraction_train&#61;0.7&#41;</code> to arrange that data attached to our model should be internally split into a train set &#40;70&#37;&#41; and a holdout set &#40;30&#37;&#41; for determining the out-of-sample estimate of the Brier loss.</p>
<p>For demonstration purposes, let&#39;s bind <code>iterated_model</code> to all data not in our don&#39;t-touch holdout set, and train on all of that data:</p>
<pre><code class="language-julia">mach_iterated_pipe &#61; machine&#40;iterated_pipe, X, y&#41;
fit&#33;&#40;mach_iterated_pipe&#41;;</code></pre>
<p>To recap, internally this training is split into two separate steps:</p>
<ul>
<li><p>A controlled iteration step, training on the holdout set, with the total number of iterations determined by the specified stopping criteria &#40;based on the out-of-sample performance estimates&#41;</p>
</li>
<li><p>A final step that trains the atomic model on <em>all</em> available data using the number of iterations determined in the first step. Calling <code>predict</code> on <code>mach_iterated_pipe</code> means using the learned parameters of the second step.</p>
</li>
</ul>
<h2 id="hyper-parameter_optimization_model_tuning"><a href="#hyper-parameter_optimization_model_tuning" class="header-anchor">Hyper-parameter optimization &#40;model tuning&#41;</a></h2>
<p><em>Introduces:</em> <code>range</code>, <strong>model wrapper</strong> <code>TunedModel</code>, <code>RandomSearch</code></p>
<p>We now turn to hyper-parameter optimization. A tool not discussed here is the <code>learning_curve</code> function, which can be useful when wanting to visualize the effect of changes to a <em>single</em> hyper-parameter &#40;which could be an iteration parameter&#41;. See, for example, <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/learning_curves/">this section of the manual</a> or <a href="https://github.com/ablaom/MLJTutorial.jl/blob/dev/notebooks/04_tuning/notebook.ipynb">this tutorial</a>.</p>
<p>Fine tuning the hyper-parameters of a gradient booster can be somewhat involved. Here we settle for simultaneously optimizing two key parameters: <code>max_depth</code> and <code>η</code> &#40;learning_rate&#41;.</p>
<p>Like iteration control, <strong>model optimization in MLJ is implemented as a model wrapper</strong>, called <code>TunedModel</code>. After wrapping a model in a tuning strategy and binding the wrapped model to data in a machine called <code>mach</code>, calling <code>fit&#33;&#40;mach&#41;</code> instigates a search for optimal model hyperparameters, within a specified range, and then uses all supplied data to train the best model. To predict using that model, one then calls <code>predict&#40;mach, Xnew&#41;</code>. In this way the wrapped model may be viewed as a &quot;self-tuning&quot; version of the unwrapped model. That is, wrapping the model simply transforms certain hyper-parameters into learned parameters &#40;just as <code>IteratedModel</code> does for an iteration parameter&#41;.</p>
<p>To start with, we define ranges for the parameters of interest. Since these parameters are nested, let&#39;s force a display of our model to a larger depth:</p>
<pre><code class="language-julia">show&#40;iterated_pipe, 2&#41;</code></pre><pre><code class="plaintext code-output">ProbabilisticIteratedModel(
  model = ProbabilisticPipeline(
        continuous_encoder = ContinuousEncoder(
              drop_last = false, 
              one_hot_ordered_factors = false), 
        feature_selector = FeatureSelector(
              features = [:TechSupport__No, Symbol("PaymentMethod__Credit card (automatic)"), :StreamingMovies__No, Symbol("Contract__One year"), Symbol("Contract__Two year"), :Partner__Yes, :OnlineBackup__No, :Dependents__Yes, :Partner__No, :gender__Male, Symbol("PaymentMethod__Mailed check"), Symbol("OnlineSecurity__No internet service"), :OnlineSecurity__No, Symbol("PaymentMethod__Electronic check"), :OnlineSecurity__Yes, Symbol("InternetService__Fiber optic"), :TechSupport__Yes, Symbol("OnlineBackup__No internet service"), :InternetService__No, :StreamingTV__Yes, :PhoneService__No, :PhoneService__Yes, Symbol("DeviceProtection__No internet service"), Symbol("StreamingTV__No internet service"), Symbol("StreamingMovies__No internet service"), Symbol("TechSupport__No internet service"), :MultipleLines__Yes, Symbol("MultipleLines__No phone service"), :InternetService__DSL], 
              ignore = true), 
        evo_tree_classifier = EvoTreeClassifier(
              loss = EvoTrees.Softmax(), 
              nrounds = 10, 
              λ = 0.0, 
              γ = 0.0, 
              η = 0.1, 
              max_depth = 5, 
              min_weight = 1.0, 
              rowsample = 1.0, 
              colsample = 1.0, 
              nbins = 64, 
              α = 0.5, 
              metric = :mlogloss, 
              rng = Random.MersenneTwister(123, (0, 86172, 85170, 780)), 
              device = "cpu"), 
        cache = true), 
  controls = Any[IterationControl.Step(1), EarlyStopping.NumberSinceBest(4), EarlyStopping.TimeLimit(Dates.Millisecond(2000)), EarlyStopping.InvalidValue()], 
  resampling = Holdout(
        fraction_train = 0.7, 
        shuffle = false, 
        rng = Random._GLOBAL_RNG()), 
  measure = BrierLoss(), 
  weights = nothing, 
  class_weights = nothing, 
  operation = MLJModelInterface.predict, 
  retrain = false, 
  check_measure = true, 
  iteration_parameter = nothing, 
  cache = true)</code></pre>
<pre><code class="language-julia">p1 &#61; :&#40;model.evo_tree_classifier.η&#41;
p2 &#61; :&#40;model.evo_tree_classifier.max_depth&#41;

r1 &#61; range&#40;iterated_pipe, p1, lower&#61;-2, upper&#61;-0.5, scale&#61;x-&gt;10^x&#41;
r2 &#61; range&#40;iterated_pipe, p2, lower&#61;2, upper&#61;6&#41;</code></pre><pre><code class="plaintext code-output">NumericRange(2 ≤ model.evo_tree_classifier.max_depth ≤ 6; origin=4.0, unit=2.0)</code></pre>
<p>Nominal ranges are defined by specifying <code>values</code> instead of <code>lower</code> and <code>upper</code>.</p>
<p>Next, we choose an optimization strategy from <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/tuning_models/#Tuning-Models">this list</a>:</p>
<pre><code class="language-julia">tuning &#61; RandomSearch&#40;rng&#61;123&#41;</code></pre><pre><code class="plaintext code-output">RandomSearch(
  bounded = Distributions.Uniform, 
  positive_unbounded = Distributions.Gamma, 
  other = Distributions.Normal, 
  rng = Random.MersenneTwister(123))</code></pre>
<p>Then we wrap the model, specifying a <code>resampling</code> strategy and a <code>measure</code>, as we did for <code>IteratedModel</code>.  In fact, we can include a battery of <code>measures</code>; by default, optimization is with respect to performance estimates based on the first measure, but estimates for all measures can be accessed from the model&#39;s <code>report</code>.</p>
<p>The keyword <code>n</code> specifies the total number of models &#40;sets of hyper-parameters&#41; to evaluate.</p>
<pre><code class="language-julia">tuned_iterated_pipe &#61; TunedModel&#40;model&#61;iterated_pipe,
                                 range&#61;&#91;r1, r2&#93;,
                                 tuning&#61;tuning,
                                 measures&#61;&#91;brier_loss, auc, accuracy&#93;,
                                 resampling&#61;StratifiedCV&#40;nfolds&#61;6, rng&#61;123&#41;,
                                 acceleration&#61;CPUThreads&#40;&#41;,
                                 n&#61;40&#41;</code></pre><pre><code class="plaintext code-output">ProbabilisticTunedModel(
  model = ProbabilisticIteratedModel(
        model = ProbabilisticPipeline(continuous_encoder = ContinuousEncoder(drop_last = false, …), …), 
        controls = Any[IterationControl.Step(1), EarlyStopping.NumberSinceBest(4), EarlyStopping.TimeLimit(Dates.Millisecond(2000)), EarlyStopping.InvalidValue()], 
        resampling = Holdout(fraction_train = 0.7, …), 
        measure = BrierLoss(), 
        weights = nothing, 
        class_weights = nothing, 
        operation = MLJModelInterface.predict, 
        retrain = false, 
        check_measure = true, 
        iteration_parameter = nothing, 
        cache = true), 
  tuning = RandomSearch(
        bounded = Distributions.Uniform, 
        positive_unbounded = Distributions.Gamma, 
        other = Distributions.Normal, 
        rng = Random.MersenneTwister(123)), 
  resampling = StratifiedCV(
        nfolds = 6, 
        shuffle = true, 
        rng = Random.MersenneTwister(123)), 
  measure = MLJBase.Measure[BrierLoss(), AreaUnderCurve(), Accuracy()], 
  weights = nothing, 
  operation = nothing, 
  range = MLJBase.NumericRange{T, MLJBase.Bounded} where T[transformed NumericRange(-2.0 ≤ model.evo_tree_classifier.η ≤ -0.5; origin=-1.25, unit=0.75), NumericRange(2 ≤ model.evo_tree_classifier.max_depth ≤ 6; origin=4.0, unit=2.0)], 
  selection_heuristic = MLJTuning.NaiveSelection(nothing), 
  train_best = true, 
  repeats = 1, 
  n = 40, 
  acceleration = ComputationalResources.CPUThreads{Int64}(1), 
  acceleration_resampling = ComputationalResources.CPU1{Nothing}(nothing), 
  check_measure = true, 
  cache = true)</code></pre>
<p>To save time, we skip the <code>repeats</code> here.</p>
<p>Binding our final model to data and training:</p>
<pre><code class="language-julia">mach_tuned_iterated_pipe &#61; machine&#40;tuned_iterated_pipe, X, y&#41;
fit&#33;&#40;mach_tuned_iterated_pipe&#41;</code></pre><pre><code class="plaintext code-output">Machine trained 1 time; caches data
  model: ProbabilisticTunedModel(model = ProbabilisticIteratedModel(model = ProbabilisticPipeline(continuous_encoder = ContinuousEncoder(drop_last = false, …), …), …), …)
  args: 
    1:	Source @007 ⏎ `ScientificTypesBase.Table{Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Multiclass{2}}, AbstractVector{ScientificTypesBase.Multiclass{4}}, AbstractVector{ScientificTypesBase.Multiclass{3}}}}`
    2:	Source @099 ⏎ `AbstractVector{ScientificTypesBase.OrderedFactor{2}}`
</code></pre>
<p>As explained above, the training we have just performed was split internally into two separate steps:</p>
<ul>
<li><p>A step to determine the parameter values that optimize the aggregated cross-validation scores</p>
</li>
<li><p>A final step that trains the optimal model on <em>all</em> available data. Future predictions <code>predict&#40;mach_tuned_iterated_pipe, ...&#41;</code> are based on this final training step.</p>
</li>
</ul>
<p>From <code>report&#40;mach_tuned_iterated_pipe&#41;</code> we can extract details about the optimization procedure. For example:</p>
<pre><code class="language-julia">rpt2 &#61; report&#40;mach_tuned_iterated_pipe&#41;;
best_booster &#61; rpt2.best_model.model.evo_tree_classifier</code></pre><pre><code class="plaintext code-output">EvoTreeClassifier(
  loss = EvoTrees.Softmax(), 
  nrounds = 10, 
  λ = 0.0, 
  γ = 0.0, 
  η = 0.14682597477521467, 
  max_depth = 2, 
  min_weight = 1.0, 
  rowsample = 1.0, 
  colsample = 1.0, 
  nbins = 64, 
  α = 0.5, 
  metric = :mlogloss, 
  rng = Random.MersenneTwister(123, (0, 86172, 85170, 780)), 
  device = "cpu")</code></pre>
<pre><code class="language-julia">print&#40;
    &quot;Optimal hyper-parameters: \n&quot;,
    &quot;  max_depth: &quot;, best_booster.max_depth, &quot;\n&quot;,
    &quot;  η:         &quot;, best_booster.η
&#41;</code></pre><pre><code class="plaintext code-output">Optimal hyper-parameters: 
  max_depth: 2
  η:         0.14682597477521467</code></pre>
<p>Using the <code>confidence_intervals</code> function we defined earlier:</p>
<pre><code class="language-julia">e_best &#61; rpt2.best_history_entry
confidence_intervals&#40;e_best&#41;</code></pre><pre><code class="plaintext code-output">3×2 DataFrame
 Row │ measure           measurement
     │ Measure           Measuremen…
─────┼───────────────────────────────
   1 │ BrierLoss()       0.294±0.031
   2 │ AreaUnderCurve()  0.817±0.051
   3 │ Accuracy()        0.801±0.023</code></pre>
<p>Digging a little deeper, we can learn what stopping criterion was applied in the case of the optimal model, and how many iterations were required:</p>
<pre><code class="language-julia">rpt2.best_report.controls |&gt; collect</code></pre><pre><code class="plaintext code-output">4-element Vector{Tuple{Any, NamedTuple}}:
 (IterationControl.Step(1), (new_iterations = 26,))
 (EarlyStopping.NumberSinceBest(4), (done = true, log = "Stop triggered by EarlyStopping.NumberSinceBest(4) stopping criterion. "))
 (EarlyStopping.TimeLimit(Dates.Millisecond(2000)), (done = false, log = ""))
 (EarlyStopping.InvalidValue(), (done = false, log = ""))</code></pre>
<p>Finally, we can visualize the optimization results:</p>
<pre><code class="language-julia">plot&#40;mach_tuned_iterated_pipe, size&#61;&#40;600,450&#41;&#41;</code></pre>
<img src="/DataScienceTutorials.jl/assets/end-to-end/telco/code/output/EX-telco-tuning.svg" alt="">
<h2 id="saving_our_model"><a href="#saving_our_model" class="header-anchor">Saving our model</a></h2>
<p><em>Introduces:</em> <code>MLJ.save</code></p>
<p>Here&#39;s how to serialize our final, trained self-iterating, self-tuning pipeline machine using Julia&#39;s native serializer &#40;see <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/machines/#Saving-machines">the manual</a> for more options&#41;:</p>
<pre><code class="language-julia">MLJ.save&#40;&quot;tuned_iterated_pipe.jls&quot;, mach_tuned_iterated_pipe&#41;</code></pre>
<p>We&#39;ll deserialize this in &quot;Testing the final model&quot; below.</p>
<h2 id="final_performance_estimate"><a href="#final_performance_estimate" class="header-anchor">Final performance estimate</a></h2>
<p>Finally, to get an even more accurate estimate of performance, we can evaluate our model using stratified cross-validation and all the data attached to our machine. Because this evaluation implies <a href="https://mlr.mlr-org.com/articles/tutorial/nested_resampling.html">nested resampling</a>, this computation takes quite a bit longer than the previous one &#40;which is being repeated six times, using 5/6th of the data each time&#41;:</p>
<pre><code class="language-julia">e_tuned_iterated_pipe &#61; evaluate&#40;tuned_iterated_pipe, X, y,
                                 resampling&#61;StratifiedCV&#40;nfolds&#61;6, rng&#61;123&#41;,
                                 measures&#61;&#91;brier_loss, auc, accuracy&#93;&#41;</code></pre><pre><code class="plaintext code-output">PerformanceEvaluation object with these fields:
  measure, measurement, operation, per_fold,
  per_observation, fitted_params_per_fold,
  report_per_fold, train_test_rows
Extract:
┌──────────────────┬─────────────┬──────────────┬────────────────────────────────────────────┐
│ measure          │ measurement │ operation    │ per_fold                                   │
├──────────────────┼─────────────┼──────────────┼────────────────────────────────────────────┤
│ BrierLoss()      │ 0.293       │ predict      │ [0.297, 0.334, 0.265, 0.278, 0.322, 0.26]  │
│ AreaUnderCurve() │ 0.816       │ predict      │ [0.804, 0.749, 0.823, 0.858, 0.772, 0.892] │
│ Accuracy()       │ 0.801       │ predict_mode │ [0.807, 0.793, 0.841, 0.793, 0.768, 0.805] │
└──────────────────┴─────────────┴──────────────┴────────────────────────────────────────────┘
</code></pre>
<pre><code class="language-julia">confidence_intervals&#40;e_tuned_iterated_pipe&#41;</code></pre><pre><code class="plaintext code-output">3×2 DataFrame
 Row │ measure           measurement
     │ Measure           Measuremen…
─────┼───────────────────────────────
   1 │ BrierLoss()       0.293±0.027
   2 │ AreaUnderCurve()  0.816±0.048
   3 │ Accuracy()        0.801±0.022</code></pre>
<p>For comparison, here are the confidence intervals for the basic pipeline model &#40;no feature selection and default hyperparameters&#41;:</p>
<pre><code class="language-julia">confidence_intervals_basic_model</code></pre><pre><code class="plaintext code-output">3×2 DataFrame
 Row │ measure           measurement
     │ Measure           Measuremen…
─────┼───────────────────────────────
   1 │ BrierLoss()       0.313±0.014
   2 │ AreaUnderCurve()  0.788±0.023
   3 │ Accuracy()        0.783±0.012</code></pre>
<p>As each pair of intervals overlap, it&#39;s doubtful the small changes here can be assigned statistical significance. Default <code>booster</code> hyper-parameters do a pretty good job.</p>
<h2 id="testing_the_final_model"><a href="#testing_the_final_model" class="header-anchor">Testing the final model</a></h2>
<p>We now determine the performance of our model on our lock-and-throw-away-the-key holdout set. To demonstrate deserialization, we&#39;ll pretend we&#39;re in a new Julia session &#40;but have called import/using on the same packages&#41;. Then the following should suffice to recover our model trained under &quot;Hyper-parameter optimization&quot; above:</p>
<pre><code class="language-julia">mach_restored &#61; machine&#40;&quot;tuned_iterated_pipe.jls&quot;&#41;</code></pre><pre><code class="plaintext code-output">Machine trained 1 time; caches data
  model: ProbabilisticTunedModel(model = ProbabilisticIteratedModel(model = ProbabilisticPipeline(continuous_encoder = ContinuousEncoder(drop_last = false, …), …), …), …)
  args: 
</code></pre>
<p>We compute predictions on the holdout set:</p>
<pre><code class="language-julia">ŷ_tuned &#61; predict&#40;mach_restored, Xtest&#41;;
ŷ_tuned&#91;1&#93;</code></pre><pre><code class="plaintext code-output">       UnivariateFinite{ScientificTypesBase.Multiclass{2}} 
       ┌                                        ┐ 
    No ┤■■■■■■■■■■■■■■■■■ 0.4644309597323305      
   Yes ┤■■■■■■■■■■■■■■■■■■■■ 0.5355690402676695   
       └                                        ┘ </code></pre>
<p>And can compute the final performance measures:</p>
<pre><code class="language-julia">print&#40;
    &quot;Tuned model measurements on test:\n&quot;,
    &quot;  brier loss: &quot;, brier_loss&#40;ŷ_tuned, ytest&#41; |&gt; mean, &quot;\n&quot;,
    &quot;  auc:        &quot;, auc&#40;ŷ_tuned, ytest&#41;,                &quot;\n&quot;,
    &quot;  accuracy:   &quot;, accuracy&#40;mode.&#40;ŷ_tuned&#41;, ytest&#41;
&#41;</code></pre><pre><code class="plaintext code-output">Tuned model measurements on test:
  brier loss: 0.2725784645587789
  auc:        0.8470046082949308
  accuracy:   0.8056872037914692</code></pre>
<p>For comparison, here&#39;s the performance for the basic pipeline model</p>
<pre><code class="language-julia">mach_basic &#61; machine&#40;pipe, X, y&#41;
fit&#33;&#40;mach_basic, verbosity&#61;0&#41;

ŷ_basic &#61; predict&#40;mach_basic, Xtest&#41;;

print&#40;
    &quot;Basic model measurements on test set:\n&quot;,
    &quot;  brier loss: &quot;, brier_loss&#40;ŷ_basic, ytest&#41; |&gt; mean, &quot;\n&quot;,
    &quot;  auc:        &quot;, auc&#40;ŷ_basic, ytest&#41;,                &quot;\n&quot;,
    &quot;  accuracy:   &quot;, accuracy&#40;mode.&#40;ŷ_basic&#41;, ytest&#41;
&#41;</code></pre><pre><code class="plaintext code-output">Basic model measurements on test set:
  brier loss: 0.3059458610206901
  auc:        0.8120967741935484
  accuracy:   0.7725118483412322</code></pre>

<div class="page-foot">
  <div class="copyright">
    &copy; Thibaut Lienart, Anthony Blaom, Sebastian Vollmer and collaborators. Last modified: May 13, 2022. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
      </div> <!-- end of id=main -->
  </div> <!-- end of id=layout -->
  <script src="/DataScienceTutorials.jl/libs/pure/ui.min.js"></script>
  
  
      <script src="/DataScienceTutorials.jl/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

  
</body>
</html>
