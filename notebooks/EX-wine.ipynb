{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"MLJTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Initial data processing\n",
    "\n",
    "In this example, we consider the [UCI \"wine\" dataset](https://archive.ics.uci.edu/ml/datasets/wine)\n",
    "\n",
    "> These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
    "\n",
    "### Getting the data\n",
    "Let's download the data thanks to the [HTTP.jl](HTTP.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\")) package and load it into a DataFrame via the [CSV.jl](https://github.com/JuliaData/CSV.jl) package:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using HTTP, CSV, MLJ, StatsBase, PyPlot\n",
    "req = HTTP.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\")\n",
    "data = CSV.read(req.body,\n",
    "                header=[\"Class\", \"Alcool\", \"Malic acid\", \"Ash\",\n",
    "                        \"Alcalinity of ash\", \"Magnesium\", \"Total phenols\",\n",
    "                        \"Flavanoids\", \"Nonflavanoid phenols\", \"Proanthcyanins\",\n",
    "                        \"Color intensity\", \"Hue\", \"OD280/OD315 of diluted wines\",\n",
    "                        \"Proline\"])\n",
    "# the target is the Class column, everything else is a feature\n",
    "y, X = unpack(data, ==(:Class), colname->true);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Setting the scientific type\n",
    "\n",
    "Let's explore the scientific type attributed by default to the target and the features"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "scitype_union(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "this should be changed as it should be considered as an ordered factor"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "yc = coerce(y, OrderedFactor);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's now consider the features"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for col in names(X)\n",
    "    println(rpad(col, 30), scitype_union(X[:, col]))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Most values are considered as Continuous as they're encoded as floating point.\n",
    "Note also that there are no missing value (otherwise one of the scientific type would have been a `Union{Missing,*}`).\n",
    "Let's have a look at the `Proline` one to see what it looks like"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X[1:5, :Proline]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "It can likely be interpreted as a Continuous as well (it would be better to know precisely what it is but for now let's just go with the hunch).\n",
    "We'll do the same with `:Magnesium`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xc = coerce(X, :Proline=>Continuous, :Magnesium=>Continuous);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Finally, let's have a quick look at the mean and standard deviation of each feature to get a feel for their amplitude:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for col in names(Xc)\n",
    "    x = Xc[:, col]\n",
    "    μ = round(mean(x), sigdigits=2)\n",
    "    σ = round(std(x), sigdigits=2)\n",
    "    println(rpad(col, 30), lpad(μ, 5), \"; \" , lpad(σ, 5))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Right so it varies a fair bit which would invite to standardise the data.\n",
    "\n",
    "**Note**: to complete such a first step, one could explore histograms of the various features for instance, check that there is enough variation among the continuous features and that there does not seem to be problems in the encoding, we cut this out to shorten the tutorial. We could also have checked that the data was balanced.\n",
    "\n",
    "## Getting a baseline\n",
    "\n",
    "It's a multiclass classification problem with continuous inputs so a sensible start is  to test two very simple classifiers to get a baseline.\n",
    "We'll train a KNN classifier and a multinomial classifier (logistic regression)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load KNNClassifier pkg=\"NearestNeighbors\"\n",
    "@load MultinomialClassifier pkg=\"MLJLinearModels\";"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "First let's standardise the data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "stand = machine(Standardizer(), Xc)\n",
    "fit!(stand)\n",
    "Xcs = transform(stand, Xc);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's also set aside 20% of the data for eventual testing."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train, test = partition(eachindex(yc), 0.8, shuffle=true, rng=111);\n",
    "Xtrain = selectrows(Xcs, train)\n",
    "Xtest = selectrows(Xcs, test)\n",
    "ytrain = selectrows(yc, train)\n",
    "ytest = selectrows(yc, test);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's train a KNNClassifier with default hyperparameters and get a baseline misclassification rate using 90% of the training data to train the model and the remaining 10% to evaluate it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "knn = machine(KNNClassifier(), Xtrain, ytrain)\n",
    "opts = (resampling=Holdout(fraction_train=0.9), measure=cross_entropy)\n",
    "res = evaluate!(knn; opts...)\n",
    "round(res.measurement[1], sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Now we do the same with a MultinomialClassifier"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mc = machine(MultinomialClassifier(), Xtrain, ytrain)\n",
    "res = evaluate!(mc; opts...)\n",
    "round(res.measurement[1], sigdigits=3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Both methods seem to offer comparable levels of performance.\n",
    "Let's check the misclassification over the full training set:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mcr_k = misclassification_rate(predict_mode(knn, Xtrain), ytrain)\n",
    "mcr_m = misclassification_rate(predict_mode(mc, Xtrain), ytrain)\n",
    "println(rpad(\"KNN mcr:\", 10), round(mcr_k, sigdigits=3))\n",
    "println(rpad(\"MNC mcr:\", 10), round(mcr_m, sigdigits=3))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "So here we have done no hyperparameter training and already have a misclassification rate below 5%.\n",
    "Clearly the problem is not very difficult.\n",
    "\n",
    "## Visualising the classes\n",
    "\n",
    "One way to get intuition for why the dataset is so easy to classify is to project it onto a 2D space using the PCA and display the two classes to see if they are well separated."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load PCA\n",
    "pca = machine(PCA(maxoutdim=2), Xtrain)\n",
    "fit!(pca)\n",
    "Wtrain = transform(pca, Xtrain);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's now display this using different colours for the different classes:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x1 = Wtrain.x1\n",
    "x2 = Wtrain.x2\n",
    "\n",
    "mask_1 = ytrain .== 1\n",
    "mask_2 = ytrain .== 2\n",
    "mask_3 = ytrain .== 3\n",
    "\n",
    "figure(figsize=(8, 6))\n",
    "plot(x1[mask_1], x2[mask_1], linestyle=\"none\", marker=\"o\", color=\"red\")\n",
    "plot(x1[mask_2], x2[mask_2], linestyle=\"none\", marker=\"o\", color=\"blue\")\n",
    "plot(x1[mask_3], x2[mask_3], linestyle=\"none\", marker=\"o\", color=\"magenta\")\n",
    "\n",
    "xlabel(\"PCA dimension 1\", fontsize=14)\n",
    "ylabel(\"PCA dimension 2\", fontsize=14)\n",
    "legend([\"Class 1\", \"Class 2\", \"Class 3\"], fontsize=12)\n",
    "xticks(fontsize=12)\n",
    "yticks(fontsize=12)\n",
    "\n",
    "savefig(\"assets/EX-wine-pca.svg\") # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "![](/assets/EX-wine-pca.svg)\n",
    "\n",
    "On that figure it now becomes quite clear why we managed to achieve such high scores with very simple classifiers.\n",
    "At this point it's a bit pointless to dig much deaper into parameter tuning etc.\n",
    "\n",
    "As a last step, we can report performances of the models on the test set which we set aside earlier:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "perf_k = misclassification_rate(predict_mode(knn, Xtest), ytest)\n",
    "perf_m = misclassification_rate(predict_mode(mc, Xtest), ytest)\n",
    "println(rpad(\"KNN mcr:\", 10), round(perf_k, sigdigits=3))\n",
    "println(rpad(\"MNC mcr:\", 10), round(perf_m, sigdigits=3))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Pretty good for so little work!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0-DEV.390"
  },
  "kernelspec": {
   "name": "julia-1.4",
   "display_name": "Julia 1.4.0-DEV.390",
   "language": "julia"
  }
 },
 "nbformat": 4
}
