{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"MLJTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Getting started"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ, RDatasets, ScientificTypes, PrettyPrinting\n",
    "\n",
    "@load LinearRegressor pkg=MLJLinearModels\n",
    "@load RidgeRegressor pkg=MLJLinearModels\n",
    "@load LassoRegressor pkg=MLJLinearModels\n",
    "\n",
    "hitters = dataset(\"ISLR\", \"Hitters\")\n",
    "@show size(hitters)\n",
    "names(hitters) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The target is `Salary`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y, X = unpack(hitters, ==(:Salary), col->true);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "It has missing values which we will just ignore:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "no_miss = .!ismissing.(y)\n",
    "y = collect(skipmissing(y))\n",
    "X = X[no_miss, :]\n",
    "train, test = partition(eachindex(y), 0.5, shuffle=true, rng=424);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Most features are currently encoded as integers but we will consider them as continuous"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xc = coerce(X, autotype(X, rules=(:discrete_to_continuous,)))\n",
    "scitype(Xc)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "There're a few features that are categorical which we'll one-hot-encode."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Ridge pipeline\n",
    "### Baseline\n",
    "\n",
    "Let's first fit a simple pipeline with a one-hot-encoder and a basic linear regression:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@pipeline HotReg(hot = OneHotEncoder(),\n",
    "                 reg = LinearRegressor())\n",
    "\n",
    "model = HotReg()\n",
    "pipe1 = machine(model, Xc, y)\n",
    "fit!(pipe1, rows=train)\n",
    "ŷ = predict(pipe1, rows=test)\n",
    "round(rms(ŷ, y[test]), sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Basic ridge\n",
    "\n",
    "Let's now swap the linear regressor for a ridge one without specifying the penalty (`1` by default):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model.reg = RidgeRegressor()\n",
    "pipe2 = machine(model, Xc, y)\n",
    "fit!(pipe2, rows=train)\n",
    "ŷ = predict(pipe2, rows=test)\n",
    "round(rms(ŷ, y[test]), sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Ok that's a bit better but not really by a wide margin."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Cross validating"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "What penalty should you use? Let's do a simple CV to try  to find out:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r  = range(model, :(reg.lambda), lower=1e-2, upper=1e9, scale=:log10)\n",
    "tm = TunedModel(model=model, ranges=r, tuning=Grid(resolution=50),\n",
    "                measure=rms)\n",
    "mtm = machine(tm, Xc, y)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "@show round(best_mdl.reg.lambda, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "right, and  with that we get:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(mtm, rows=test)\n",
    "round(rms(ŷ, y[test]), sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "It's a bit of a case of bad data (and tuning) though, let's remove the categorical features:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xc2 = select(Xc, Not([:League, :Division, :NewLeague]))\n",
    "pipe2 = machine(model, Xc2, y)\n",
    "fit!(pipe2, rows=train)\n",
    "ŷ = predict(pipe2, rows=test)\n",
    "round(rms(ŷ, y[test]), sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "So here we've done no hyperparameter tuning and already get comparable results, let's re-tune and use proper cross-validation as well"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tm.resampling = CV(nfolds=5)\n",
    "mtm = machine(tm, Xc2, y)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "ŷ = predict(mtm, rows=test)\n",
    "round(rms(ŷ, y[test]), sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Ok that's better!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "_ongoing completion_"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0-DEV.435"
  },
  "kernelspec": {
   "name": "julia-1.4",
   "display_name": "Julia 1.4.0-DEV.435",
   "language": "julia"
  }
 },
 "nbformat": 4
}
