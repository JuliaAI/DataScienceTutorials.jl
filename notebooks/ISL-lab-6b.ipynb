{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"MLJTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Getting started"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ, RDatasets, ScientificTypes, PrettyPrinting\n",
    "\n",
    "@load LinearRegressor pkg=MLJLinearModels\n",
    "@load RidgeRegressor pkg=MLJLinearModels\n",
    "@load LassoRegressor pkg=MLJLinearModels\n",
    "\n",
    "hitters = dataset(\"ISLR\", \"Hitters\")\n",
    "@show size(hitters)\n",
    "names(hitters) |> pprint"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The target is `Salary`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "y, X = unpack(hitters, ==(:Salary), col->true);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "It has missing values which we will just ignore:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "no_miss = .!ismissing.(y)\n",
    "y = collect(skipmissing(y))\n",
    "X = X[no_miss, :]\n",
    "train, test = partition(eachindex(y), 0.5, shuffle=true, rng=424);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Most features are currently encoded as integers but we will consider them as continuous"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xc = coerce(X, autotype(X, rules=(:discrete_to_continuous,)))\n",
    "scitype(Xc)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "There're a few features that are categorical which we'll one-hot-encode."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Ridge pipeline\n",
    "### Baseline\n",
    "\n",
    "Let's first fit a simple pipeline with a standardizer, a one-hot-encoder and a basic linear regression:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@pipeline RegPipe(std = Standardizer(),\n",
    "                  hot = OneHotEncoder(),\n",
    "                  reg = LinearRegressor())\n",
    "\n",
    "model = RegPipe()\n",
    "pipe  = machine(model, Xc, y)\n",
    "fit!(pipe, rows=train)\n",
    "ŷ = predict(pipe, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Basic ridge\n",
    "\n",
    "Let's now swap the linear regressor for a ridge one without specifying the penalty (`1` by default):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pipe.model.reg = RidgeRegressor()\n",
    "fit!(pipe, rows=train)\n",
    "ŷ = predict(pipe, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Ok that's a bit better but surely we can do better with an appropriate selection of the hyperparameter."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Cross validating"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "What penalty should you use? Let's do a simple CV to try  to find out:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r  = range(model, :(reg.lambda), lower=1e-2, upper=100_000, scale=:log10)\n",
    "tm = TunedModel(model=model, ranges=r, tuning=Grid(resolution=50),\n",
    "                resampling=CV(nfolds=3, rng=4141), measure=rms)\n",
    "mtm = machine(tm, Xc, y)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "round(best_mdl.reg.lambda, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "right, and  with that we get:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(mtm, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Lasso pipeline\n",
    "\n",
    "Let's do the same as above but using a Lasso model and adjusting the range a bit:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mtm.model.model.reg = LassoRegressor()\n",
    "mtm.model.ranges = range(model, :(reg.lambda), lower=500, upper=100_000, scale=:log10)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "round(best_mdl.reg.lambda, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Ok and let's see how that does:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(mtm, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Pretty good! and the parameters are reasonably sparse as expected:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "coefs = mtm.fitresult.fitresult.machine.fitresult\n",
    "round.(coefs, sigdigits=2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "with around 50% sparsity:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "sum(coefs .≈ 0) / length(coefs)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Elastic net pipeline"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load ElasticNetRegressor pkg=MLJLinearModels\n",
    "\n",
    "mtm.model.model.reg = ElasticNetRegressor()\n",
    "mtm.model.ranges = [range(model, :(reg.lambda), lower=0.1, upper=100, scale=:log10),\n",
    "                    range(model, :(reg.gamma),  lower=500, upper=10_000, scale=:log10)]\n",
    "mtm.model.tuning = Grid(resolution=10)\n",
    "fit!(mtm, rows=train)\n",
    "\n",
    "best_mdl = fitted_params(mtm).best_model\n",
    "@show round(best_mdl.reg.lambda, sigdigits=4)\n",
    "@show round(best_mdl.reg.gamma, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "And it's not too bad in terms of accuracy either"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict(mtm, rows=test)\n",
    "round(rms(ŷ, y[test])^2, sigdigits=4)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "But the simple ridge regression seems to work best here."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0-DEV.435"
  },
  "kernelspec": {
   "name": "julia-1.4",
   "display_name": "Julia 1.4.0-DEV.435",
   "language": "julia"
  }
 },
 "nbformat": 4
}
