{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Before running this, please make sure to activate and instantiate the environment\n",
    "corresponding to [this `Project.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Project.toml) and [this `Manifest.toml`](https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/Manifest.toml)\n",
    "so that you get an environment which matches the one used to generate the tutorials:\n",
    "\n",
    "```julia\n",
    "cd(\"MLJTutorials\") # cd to folder with the *.toml\n",
    "using Pkg; Pkg.activate(\".\"); Pkg.instantiate()\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Getting started"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ, RDatasets\n",
    "\n",
    "auto = dataset(\"ISLR\", \"Auto\")\n",
    "y, X = unpack(auto, ==(:MPG), col->true)\n",
    "train, test = partition(eachindex(y), 0.5, shuffle=true, rng=444);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note the use of `rng=` to seed the shuffling of indices so that the results are reproducible."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Polynomial regression"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@load LinearRegressor pkg=MLJLinearModels"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "In this part we only build models with the Horsepower feature.\n",
    "Let's get a baseline:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lm = LinearRegressor()\n",
    "mlm = machine(lm, select(X, :Horsepower), y)\n",
    "fit!(mlm, rows=train)\n",
    "rms(predict(mlm, rows=test), y[test])^2"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note that we square the measure to  match the results obtained in the ISL labs where the mean squared error (here we use the `rms` which is the square root of that).\n",
    "\n",
    "We now want to build three polynomial models of degree 1, 2 and 3 respectively; we start by forming the corresponding feature matrix:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "hp = X.Horsepower\n",
    "Xhp = DataFrame(hp1=hp, hp2=hp.^2, hp3=hp.^3);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Now we  can write a simple pipeline where the first step selects the features we want (and with it the degree of the polynomial) and the second is the linear regressor:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@pipeline LinMod(fs = FeatureSelector(features=[:hp1]),\n",
    "                 lr = LinearRegressor());"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Then we can  instantiate and fit 3 models where we specify the features each time:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lrm = LinMod()\n",
    "lr1 = machine(lrm, Xhp, y) # poly of degree 1 (line)\n",
    "fit!(lr1, rows=train)\n",
    "\n",
    "lrm.fs.features = [:hp1, :hp2] # poly of degree 2\n",
    "lr2 = machine(lrm, Xhp, y)\n",
    "fit!(lr2, rows=train)\n",
    "\n",
    "lrm.fs.features = [:hp1, :hp2, :hp3] # poly of degree 3\n",
    "lr3 = machine(lrm, Xhp, y)\n",
    "fit!(lr3, rows=train)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Let's check the performances on the test set"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "get_mse(lr) = rms(predict(lr, rows=test), y[test])^2\n",
    "\n",
    "@show get_mse(lr1)\n",
    "@show get_mse(lr2)\n",
    "@show get_mse(lr3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## K-Folds Cross Validation\n",
    "\n",
    "Let's crossvalidate over the degree of the  polynomial.\n",
    "\n",
    "**Note**: there's a  bit of gymnastics here because MLJ doesn't directly support a polynomial regression; see our tutorial on [tuning models](/pub/getting-started/model-tuning.html) for a gentler introduction to model tuning."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xhp = DataFrame([hp.^i for i in 1:10])\n",
    "\n",
    "cases = [[Symbol(\"x$j\") for j in 1:i] for i in 1:10]\n",
    "r = range(lrm, :(fs.features), values=cases)\n",
    "\n",
    "tm = TunedModel(model=lrm, ranges=r, resampling=CV(nfolds=10), measure=rms)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Now we're left with fitting the tuned model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mtm = machine(tm, Xhp, y)\n",
    "fit!(mtm)\n",
    "rep = report(mtm)\n",
    "@show round.(rep.measurements.^2, digits=2)\n",
    "@show argmin(rep.measurements)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "So the conclusion here is that the 5th order polynomial does quite well.\n",
    "\n",
    "In ISL they use a different seed so the results are a bit different but comparable."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## The Bootstrap\n",
    "\n",
    "_Bootstrapping is not currently supported in MLJ._"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0-DEV.495"
  },
  "kernelspec": {
   "name": "julia-1.4",
   "display_name": "Julia 1.4.0-DEV.495",
   "language": "julia"
  }
 },
 "nbformat": 4
}
