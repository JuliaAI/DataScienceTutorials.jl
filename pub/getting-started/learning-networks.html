<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/MLJTutorials/libs/highlight/github.min.css"> <link rel=stylesheet  href="/MLJTutorials/css/judoc.css"> <link rel=stylesheet  href="/MLJTutorials/css/pure.css"> <link rel=stylesheet  href="/MLJTutorials/css/side-menu.css"> <link rel=stylesheet  href="/MLJTutorials/css/extra.css"> <title>Learning networks</title> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/MLJTutorials/" id=menu-logo-link > <div class=menu-logo > <img id=menu-logo  alt="MLJ Logo" src="/MLJTutorials/assets/infra/MLJLogo2.svg" /> <p><strong>MLJ Tutorials</strong></p> </div> </a> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/MLJTutorials/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Data basics</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/MLJTutorials/pub/data/loading.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Loading data</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/data/dataframe.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> DataFrame</a> </ul> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/choosing-a-model.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/fit-and-predict.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/model-tuning.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/ensembles.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/ensembles-2.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/composing-models.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item pure-menu-selected"><a href="/MLJTutorials/pub/getting-started/learning-networks.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/learning-networks-2.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a> </ul> <li class=pure-menu-sublist-title ><strong>Intro to Stats Learning</strong> <ul class=pure-menu-sublist  id=isl> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-2.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 2</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-3.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 3</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-4.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 4</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-5.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 5</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-6b.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 6b</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-8.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 8</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-9.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 9</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-10.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 10</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/AMES.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/wine.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/horse.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Horse</a> </ul> </ul> </div> </div> <div id=main > <div class=jd-content > <h1 id=learning_networks ><a href="/MLJTutorials/pub/getting-started/learning-networks.html#learning_networks">Learning networks</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/notebooks/A-learning-networks.ipynb" target=_blank ><em>notebook</em></a>, <em>the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/scripts/A-learning-networks-raw.jl" target=_blank ><em>raw script</em></a>, <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/scripts/A-learning-networks.jl" target=_blank ><em>annoted script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=jd-toc ><ol><li><a href="/MLJTutorials/pub/getting-started/learning-networks.html#preliminary_steps">Preliminary steps</a><li><a href="/MLJTutorials/pub/getting-started/learning-networks.html#defining_a_learning_network">Defining a learning network</a><ol><li><a href="/MLJTutorials/pub/getting-started/learning-networks.html#sources_and_nodes">Sources and nodes</a><li><a href="/MLJTutorials/pub/getting-started/learning-networks.html#modifying_hyperparameters">Modifying hyperparameters</a></ol><li><a href="/MLJTutorials/pub/getting-started/learning-networks.html#quotarrowquot_syntax">&quot;Arrow&quot; syntax</a></ol></div><h2 id=preliminary_steps ><a href="/MLJTutorials/pub/getting-started/learning-networks.html#preliminary_steps">Preliminary steps</a></h2> <p>Let&#39;s generate a <code>DataFrame</code> with some dummy regression data, let&#39;s also load the good old ridge regressor.</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLJ, DataFrames, Random
<span class=hljs-meta >@load</span> RidgeRegressor pkg=MultivariateStats

Random.seed!(<span class=hljs-number >5</span>) <span class=hljs-comment ># for reproducibility</span>
x1 = rand(<span class=hljs-number >300</span>)
x2 = rand(<span class=hljs-number >300</span>)
x3 = rand(<span class=hljs-number >300</span>)
y = exp.(x1 - x2 -<span class=hljs-number >2</span>x3 + <span class=hljs-number >0.1</span>*rand(<span class=hljs-number >300</span>))
X = DataFrame(x1=x1, x2=x2, x3=x3)
first(X, <span class=hljs-number >3</span>) |&gt; pretty</code></pre><div class=code_output ><pre><code class="plaintext hljs">┌────────────────────┬─────────────────────┬────────────────────┐
│ x1                 │ x2                  │ x3                 │
│ Float64            │ Float64             │ Float64            │
│ Continuous         │ Continuous          │ Continuous         │
├────────────────────┼─────────────────────┼────────────────────┤
│ 0.5303646605536807 │ 0.09410638348155964 │ 0.2999012171698161 │
│ 0.777009169833947  │ 0.4111301767644082  │ 0.4069405144799101 │
│ 0.3013103535440822 │ 0.5499402666491009  │ 0.8495024908781252 │
└────────────────────┴─────────────────────┴────────────────────┘
</code></pre></div> <p>Let&#39;s also prepare the train and test split which will be useful later on.</p> <pre><code class="julia hljs">test, train = partition(eachindex(y), <span class=hljs-number >0.8</span>);</code></pre>
<h2 id=defining_a_learning_network ><a href="/MLJTutorials/pub/getting-started/learning-networks.html#defining_a_learning_network">Defining a learning network</a></h2>
<p>In MLJ, a <em>learning network</em> is a directed acyclic graph &#40;DAG&#41; whose <em>nodes</em> apply trained or untrained operations such as a <code>predict</code> or <code>transform</code> &#40;trained&#41; or <code>&#43;</code>, <code>vcat</code> etc. &#40;untrained&#41;. Learning networks can be seen as pipelines on steroids.</p>
<p>Let&#39;s consider the following simple DAG:</p>
<p><img src="/MLJTutorials/assets/diagrams/composite1.svg" alt="Operation DAG" /></p>
<p>It corresponds to a fairly standard regression workflow: the data is standardized, the target is transformed using a Box-Cox transformation, a ridge regression is applied and the result is converted back by inverting the transform.</p>
<p><strong>Note</strong>: actually  this DAG is simple enough that it could also have been done with a pipeline.</p>
<h3 id=sources_and_nodes ><a href="/MLJTutorials/pub/getting-started/learning-networks.html#sources_and_nodes">Sources and nodes</a></h3>
<p>In MLJ a learning network starts at <strong>source</strong> nodes and flows through nodes &#40;<code>X</code> and <code>y</code>&#41; defining operations/transformations &#40;<code>W</code>, <code>z</code>, <code>ẑ</code>, <code>ŷ</code>&#41;. To define the source nodes, use the <code>source</code> function, you should specify whether it&#39;s a target:</p>
<pre><code class="julia hljs">Xs = source(X)
ys = source(y, kind=:target)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Source{:target} @ 1…66
</code></pre></div>
<p>To define an &quot;trained-operation&quot; node, you must simply create a machine wrapping a model and another node &#40;the data&#41; and indicate which operation should be performed &#40;e.g. <code>transform</code>&#41;:</p>
<pre><code class="julia hljs">stand = machine(Standardizer(), Xs)
W = transform(stand, Xs)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Node @ 5…07 = transform(1…58, 1…66)</code></pre></div>
<p>You can <code>fit&#33;</code> a trained-operation node at any point, MLJ will fit whatever it needs that is upstream of that node. In this case, there is just a source node upstream of <code>W</code> so fitting <code>W</code> will just fit the standardizer:</p>
<pre><code class="julia hljs">fit!(W, rows=train);</code></pre>
<p>If you want to get the transformed data, you can then call the node speciying on which part of the data the operation should be performed:</p>
<pre><code class="julia hljs">W()             <span class=hljs-comment ># transforms all data</span>
W(rows=test, )  <span class=hljs-comment ># transforms only test data</span>
W(X[<span class=hljs-number >3</span>:<span class=hljs-number >4</span>, :])    <span class=hljs-comment ># transforms specific data</span></code></pre><div class=code_output ><pre><code class="plaintext hljs">2×3 DataFrame
│ Row │ x1        │ x2       │ x3        │
│     │ Float64   │ Float64  │ Float64   │
├─────┼───────────┼──────────┼───────────┤
│ 1   │ -0.690031 │ 0.191073 │ 1.05874   │
│ 2   │ 1.35423   │ -1.06167 │ -0.854028 │</code></pre></div>
<p>Let&#39;s now define the other nodes:</p>
<pre><code class="julia hljs">box_model = UnivariateBoxCoxTransformer()
box = machine(box_model, ys)
z = transform(box, ys)

ridge_model = RidgeRegressor(lambda=<span class=hljs-number >0.1</span>)
ridge = machine(ridge_model, W, z)
ẑ = predict(ridge, W)

ŷ = inverse_transform(box, ẑ)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Node @ 8…74 = inverse_transform(1…95, predict(3…22, transform(1…58, 1…66)))</code></pre></div>
<p>Note that we have not yet done any training, but if we now call <code>fit&#33;</code> on <code>ŷ</code>, it will fit all nodes upstream of <code>ŷ</code> that need to be re-trained:</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train);</code></pre>
<p>Now that <code>ŷ</code> has been fitted, you can apply the full graph on test data &#40;or any compatible data&#41;. For instance, let&#39;s get the <code>rms</code> between the ground truth and the predicted values:</p>
<pre><code class="julia hljs">rms(y[test], ŷ(rows=test))</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.019910338054336767</code></pre></div>
<h3 id=modifying_hyperparameters ><a href="/MLJTutorials/pub/getting-started/learning-networks.html#modifying_hyperparameters">Modifying hyperparameters</a></h3>
<p>Hyperparameters can be accessed using the dot syntax as usual. Let&#39;s modify the regularisation parameter of the ridge regression:</p>
<pre><code class="julia hljs">ridge_model.lambda = <span class=hljs-number >5.0</span>;</code></pre>
<p>Since the node <code>ẑ</code> corresponds to a machine that wraps <code>ridge_model</code>, that node has effectively changed and will be retrained:</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train)
rms(y[test], ŷ(rows=test))</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.05425574766979042</code></pre></div>
<h2 id=quotarrowquot_syntax ><a href="/MLJTutorials/pub/getting-started/learning-networks.html#quotarrowquot_syntax">&quot;Arrow&quot; syntax</a></h2>  <strong>Important</strong>: for this to work, you need to be using <strong>Julia ≥ 1.3</strong>:</p>
<p>The syntax to define nodes etc. is a bit verbose. MLJ supports a shorter syntax which abstracts away some of the steps. We will refer to it as the &quot;arrow&quot; syntax as it makes use of the <code>|&gt;</code> operator which can be interpreted as &quot;data flow&quot;.</p>
<p>Let&#39;s start with <code>W</code> and <code>z</code> &#40;the &quot;first layer&quot;&#41;:</p>
<pre><code class="julia hljs">W = X |&gt; Standardizer()
z = y |&gt; UnivariateBoxCoxTransformer()</code></pre><div class=code_output ><pre><code class="plaintext hljs">Node @ 1…83 = transform(1…18, 6…23)</code></pre></div>
<p>Note that we feed <code>X</code> and <code>y</code> directly into models. In the background, MLJ will create source nodes and assumes that the operation is a <code>transform</code> given the models are unsupervised.</p>
<p>For a node that corresponds to a supervised model, you can feed a tuple where the first element corresponds to the input &#40;here <code>W</code>&#41; and the second corresponds to the target &#40;here <code>z</code>&#41;, MLJ will assume the operation is a <code>predict</code>:</p>
<pre><code class="julia hljs">ẑ = (W, z) |&gt; RidgeRegressor(lambda=<span class=hljs-number >0.1</span>);</code></pre>
<p>Finally we need to apply the inverse of the transform encapsulated in the node <code>z</code>, for this:</p>
<pre><code class="julia hljs">ŷ = ẑ |&gt; inverse_transform(z);</code></pre>
<p>That&#39;s it&#33; You can now fit the network as before:</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train)
rms(y[test], ŷ(rows=test))</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.019910338054336767</code></pre></div>
<p>To <em>manually</em> modify hyperparameters on a node, you can access them like so:</p>
<pre><code class="julia hljs">ẑ[:lambda] = <span class=hljs-number >5.0</span>;</code></pre>
<p>Here remember that <code>ẑ</code> is a node with a machine that wraps around a ridge regression with a parameter <code>lambda</code> so the syntax above is equivalent to</p>
<pre><code class="julia hljs">ẑ.machine.model.lambda = <span class=hljs-number >5.0</span>;</code></pre>
<p>which is relevant if you want to tune the hyperparameter using a <code>TunedModel</code>.</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train)
rms(y[test], ŷ(rows=test))</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.05425574766979042</code></pre></div>
<div class=page-foot >
  <div class=copyright >
    &copy; Anthony Blaom, Thibaut Lienart and collaborators. Last modified: October 21, 2019. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>.
  </div>
</div>

</div>

      </div> 
  </div> 
  <script src="/MLJTutorials/libs/pure/ui.min.js"></script>