<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/MLJTutorials/libs/highlight/github.min.css"> <link rel=stylesheet  href="/MLJTutorials/css/judoc.css"> <link rel=stylesheet  href="/MLJTutorials/css/pure.css"> <link rel=stylesheet  href="/MLJTutorials/css/side-menu.css"> <link rel=stylesheet  href="/MLJTutorials/css/extra.css"> <title></title> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/MLJTutorials/" id=menu-logo-link > <div class=menu-logo > <img id=menu-logo  alt="MLJ Logo" src="/MLJTutorials/assets/infra/MLJLogo2.svg" /> <p><strong>MLJ Tutorials</strong></p> </div> </a> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/MLJTutorials/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/choosing-a-model.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/fit-and-predict.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/model-tuning.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item pure-menu-selected"><a href="/MLJTutorials/pub/getting-started/ensembles.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/ensembles-2.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/composing-models.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/learning-networks.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/learning-networks-2.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/AMES.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/wine.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/horse.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Horse</a> </ul> <li class=pure-menu-sublist-title ><strong>Intro to Stats Learning</strong> <ul class=pure-menu-sublist  id=isl> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-2.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 2</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-3.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 3</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-4.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 4</a> </ul> </ul> </div> </div> <div id=main > <div class=jd-content > <h1 id=ensemble_models ><a href="/MLJTutorials/pub/getting-started/ensembles.html#ensemble_models">Ensemble models</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/notebooks/A-ensembles.ipynb" target=_blank ><em>notebook</em></a>, <em>the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/scripts/A-ensembles.jl" target=_blank ><em>raw script</em></a>, <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/scripts/A-ensembles.jl" target=_blank ><em>annoted script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=jd-toc ><ol><li><a href="/MLJTutorials/pub/getting-started/ensembles.html#preliminary_steps">Preliminary steps</a><li><a href="/MLJTutorials/pub/getting-started/ensembles.html#homogenous_ensembles">Homogenous ensembles</a><ol><li><a href="/MLJTutorials/pub/getting-started/ensembles.html#training_and_testing_an_ensemble">Training and testing an ensemble</a><li><a href="/MLJTutorials/pub/getting-started/ensembles.html#systematic_tuning">Systematic tuning</a><li><a href="/MLJTutorials/pub/getting-started/ensembles.html#reporting_results">Reporting results</a></ol></ol></div><h2 id=preliminary_steps ><a href="/MLJTutorials/pub/getting-started/ensembles.html#preliminary_steps">Preliminary steps</a></h2> <p>Let&#39;s start by loading the relevant packages and generating some dummy data.</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLJ, DataFrames, Statistics, PrettyPrinting

Xraw = rand(<span class=hljs-number >300</span>, <span class=hljs-number >3</span>)
y = exp.(Xraw[:,<span class=hljs-number >1</span>] - Xraw[:,<span class=hljs-number >2</span>] - <span class=hljs-number >2</span>Xraw[:,<span class=hljs-number >3</span>] + <span class=hljs-number >0.1</span>*rand(<span class=hljs-number >300</span>))
X = DataFrame(Xraw)

train, test = partition(eachindex(y), <span class=hljs-number >0.7</span>);</code></pre> <p>Let&#39;s also load a simple model:</p> <pre><code class="julia hljs"><span class=hljs-meta >@load</span> KNNRegressor
knn_model = KNNRegressor(K=<span class=hljs-number >10</span>)</code></pre><div class=code_output ><pre><code class="plaintext hljs">MLJModels.NearestNeighbors_.KNNRegressor(K = 10,
                                         algorithm = :kdtree,
                                         metric = Distances.Euclidean(0.0),
                                         leafsize = 10,
                                         reorder = true,
                                         weights = :uniform,) @ 9…43</code></pre></div> <p>As before, let&#39;s instantiate a machine that wraps the model and data:</p> <pre><code class="julia hljs">knn = machine(knn_model, X, y)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Machine{KNNRegressor} @ 1…84
</code></pre></div> <p>and fit it</p> <pre><code class="julia hljs">fit!(knn, rows=train)
ŷ = predict(knn, X[test, :]) <span class=hljs-comment ># or use rows=test</span>
rms(ŷ, y[test])</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.07075932585979508</code></pre></div>
<p>The few steps above are equivalent to just calling <code>evaluate&#33;</code>:</p>
<pre><code class="julia hljs">evaluate!(knn, resampling=Holdout(fraction_train=<span class=hljs-number >0.7</span>), measure=rms) |&gt; pprint</code></pre><div class=code_output ><pre><code class="plaintext hljs">(measure = [rms],
 measurement = [0.07075932585979508],
 per_fold = [[0.07075932585979508]],
 per_observation = [missing])
</code></pre></div>
<h2 id=homogenous_ensembles ><a href="/MLJTutorials/pub/getting-started/ensembles.html#homogenous_ensembles">Homogenous ensembles</a></h2>
<p>MLJ offers basic support for ensembling such as <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating"><em>bagging</em></a>. Defining such an ensemble of simple &quot;atomic&quot; models is done via the <code>EnsembleModel</code> constructor:</p>
<pre><code class="julia hljs">ensemble_model = EnsembleModel(atom=knn_model, n=<span class=hljs-number >20</span>);</code></pre>
<p>where the <code>n&#61;20</code> indicates how many models are present in the ensemble.</p>
<h3 id=training_and_testing_an_ensemble ><a href="/MLJTutorials/pub/getting-started/ensembles.html#training_and_testing_an_ensemble">Training and testing an ensemble</a></h3>
<p>Now that we&#39;ve instantiated an ensemble, it can be trained and tested the same as any other model:</p>
<pre><code class="julia hljs">ensemble = machine(ensemble_model, X, y)
estimates = evaluate!(ensemble, resampling=CV())
estimates |&gt; pprint</code></pre><div class=code_output ><pre><code class="plaintext hljs">(measure = [rms],
 measurement = [0.07388838342857286],
 per_fold = [[0.06566024243320519,
              0.05335613918760356,
              0.07935311013590386,
              0.10590255886679735,
              0.0718891785241601,
              0.06716907142376717]],
 per_observation = [missing])
</code></pre></div>
<p>here the implicit measure is the <code>rms</code> &#40;default for regressions&#41;. The <code>measurement</code> is the mean taken over the folds:</p>
<pre><code class="julia hljs"><span class=hljs-meta >@show</span> estimates.measurement[<span class=hljs-number >1</span>]
<span class=hljs-meta >@show</span> mean(estimates.per_fold[<span class=hljs-number >1</span>])</code></pre><div class=code_output ><pre><code class="plaintext hljs">estimates.measurement[1] = 0.07388838342857286
mean(estimates.per_fold[1]) = 0.07388838342857286
</code></pre></div>
<p>Note that multiple measurements can be specified jointly. Here only on measurement is &#40;implicitly&#41; specified but we still have to select the corresponding results &#40;whence the <code>&#91;1&#93;</code> for both  the <code>measurement</code> and <code>per_fold</code>&#41;.</p>
<h3 id=systematic_tuning ><a href="/MLJTutorials/pub/getting-started/ensembles.html#systematic_tuning">Systematic tuning</a></h3>
<p>Let&#39;s simultaneously tune the ensemble&#39;s <code>bagging_fraction</code> and the K-Nearest neighbour hyperparameter <code>K</code>. Since one of our models is  a field of the  other, we have nested hyperparameters:</p>
<pre><code class="julia hljs">params(ensemble_model) |&gt; pprint</code></pre><div class=code_output ><pre><code class="plaintext hljs">(atom = (K = 10,
         algorithm = :kdtree,
         metric = Distances.Euclidean(0.0),
         leafsize = 10,
         reorder = true,
         weights = :uniform),
 weights = [],
 bagging_fraction = 0.8,
 rng = Random._GLOBAL_RNG(),
 n = 20,
 parallel = true,
 out_of_bag_measure = [])
</code></pre></div>
<p>To define a tuning grid, we construct ranges for the two parameters and collate these ranges:</p>
<pre><code class="julia hljs">B_range = range(ensemble_model, :bagging_fraction, lower=<span class=hljs-number >0.5</span>, upper=<span class=hljs-number >1.0</span>)
K_range = range(ensemble_model, :(atom.K), lower=<span class=hljs-number >1</span>, upper=<span class=hljs-number >20</span>);</code></pre>
<p>the scale for a tuning grid is linear by default but can be specified to <code>:log10</code> for logarithmic ranges. Now we have to define a <code>TunedModel</code> and fit it:</p>
<pre><code class="julia hljs">tm = TunedModel(model=ensemble_model,
                tuning=Grid(resolution=<span class=hljs-number >10</span>), <span class=hljs-comment ># 10x10 grid</span>
                resampling=Holdout(fraction_train=<span class=hljs-number >0.8</span>, rng=<span class=hljs-number >42</span>),
                ranges=[B_range, K_range])

tuned_ensemble = machine(tm, X, y)
fit!(tuned_ensemble, rows=train);</code></pre>
<p>Note the <code>rng&#61;42</code> seeds the random number generator for reproducibility of this example.</p>
<h3 id=reporting_results ><a href="/MLJTutorials/pub/getting-started/ensembles.html#reporting_results">Reporting results</a></h3>
<p>The best model can be accessed like so:</p>
<pre><code class="julia hljs">best_ensemble = fitted_params(tuned_ensemble).best_model
<span class=hljs-meta >@show</span> best_ensemble.atom.K
<span class=hljs-meta >@show</span> best_ensemble.bagging_fraction</code></pre><div class=code_output ><pre><code class="plaintext hljs">best_ensemble.atom.K = 1
best_ensemble.bagging_fraction = 0.5555555555555556
</code></pre></div>
<p>The <code>report</code> method gives more detailed information on the tuning process:</p>
<pre><code class="julia hljs">r = report(tuned_ensemble);</code></pre>
<p>For instance, <code>r.measurements</code> are the measurements for all pairs of hyperparameters which you could visualise nicely:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> PyPlot

figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))

vals_b = r.parameter_values[:, <span class=hljs-number >1</span>]
vals_k = r.parameter_values[:, <span class=hljs-number >2</span>]

tricontourf(vals_b, vals_k, r.measurements)
xticks(<span class=hljs-number >0.5</span>:<span class=hljs-number >0.1</span>:<span class=hljs-number >1</span>, fontsize=<span class=hljs-number >12</span>)
xlabel(<span class=hljs-string >"Bagging fraction"</span>, fontsize=<span class=hljs-number >14</span>)
yticks([<span class=hljs-number >1</span>, <span class=hljs-number >5</span>, <span class=hljs-number >10</span>, <span class=hljs-number >15</span>, <span class=hljs-number >20</span>], fontsize=<span class=hljs-number >12</span>)
ylabel(<span class=hljs-string >"Number of neighbors - K"</span>, fontsize=<span class=hljs-number >14</span>)
</code></pre>
<p><img src="/MLJTutorials/assets/literate/A-ensembles-heatmap.svg" alt="" /></p>
<p>Finally you can always just evaluate the model by reporting <code>rms</code> on the test set:</p>
<pre><code class="julia hljs">ŷ = predict(tuned_ensemble, rows=test)
rms(ŷ, y[test])</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.07760723789977601</code></pre></div>
<div class=page-foot >
  <div class=copyright >
    &copy; Anthony Blaom, Thibaut Lienart and collaborators. Last modified: October 21, 2019. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>.
  </div>
</div>

</div>

      </div> 
  </div> 
  <script src="/MLJTutorials/libs/pure/ui.min.js"></script>