<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/MLJTutorials/libs/highlight/github.min.css"> <link rel=stylesheet  href="/MLJTutorials/css/judoc.css"> <link rel=stylesheet  href="/MLJTutorials/css/pure.css"> <link rel=stylesheet  href="/MLJTutorials/css/side-menu.css"> <link rel=stylesheet  href="/MLJTutorials/css/extra.css"> <title></title> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/MLJTutorials/" id=menu-logo-link > <div class=menu-logo > <img id=menu-logo  alt="MLJ Logo" src="/MLJTutorials/assets/infra/MLJLogo2.svg" /> <p><strong>MLJ Tutorials</strong></p> </div> </a> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/MLJTutorials/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/choosing-a-model.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/fit-and-predict.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/model-tuning.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/ensembles.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/ensembles-2.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/composing-models.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/learning-networks.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/AMES.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/wine.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item pure-menu-selected"><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/horse.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Horse</a> </ul> </ul> </div> </div> <div id=main > <div class=jd-content > <h1 id=crabs_with_xgboost ><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html#crabs_with_xgboost">Crabs with XGBoost</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/notebooks/EX-crabs-xgb.ipynb" target=_blank ><em>notebook</em></a> <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/scripts/EX-crabs-xgb.jl" target=_blank ><em>raw script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=jd-toc ><ol><li><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html#first_steps">First steps</a><li><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html#xgboost_machine">XGBoost machine</a><ol><li><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html#more_tuning_1">More tuning &#40;1&#41;</a><li><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html#more_tuning_2">More tuning &#40;2&#41;</a><li><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html#more_tuning_3">More tuning &#40;3&#41;</a></ol></ol></div>This example is inspired from <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">this post</a> showing how to use XGBoost.</p> <h2 id=first_steps ><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html#first_steps">First steps</a></h2> <p>Again, the crabs dataset is so common that there is a simple load function for it:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLJ, StatsBase, Random, PyPlot, CategoricalArrays, PrettyPrinting
X, y = <span class=hljs-meta >@load_crabs</span>
<span class=hljs-meta >@show</span> size(X)
<span class=hljs-meta >@show</span> y[<span class=hljs-number >1</span>:<span class=hljs-number >3</span>]
first(X, <span class=hljs-number >3</span>) |&gt; pretty</code></pre><div class=code_output ><pre><code class="plaintext hljs">size(X) = (200, 5)
y[1:3] = CategoricalArrays.CategoricalString{UInt32}["B", "B", "B"]
┌────────────┬────────────┬────────────┬────────────┬────────────┐
│ FL         │ RW         │ CL         │ CW         │ BD         │
│ Float64    │ Float64    │ Float64    │ Float64    │ Float64    │
│ Continuous │ Continuous │ Continuous │ Continuous │ Continuous │
├────────────┼────────────┼────────────┼────────────┼────────────┤
│ 8.1        │ 6.7        │ 16.1       │ 19.0       │ 7.0        │
│ 8.8        │ 7.7        │ 18.1       │ 20.8       │ 7.4        │
│ 9.2        │ 7.8        │ 19.0       │ 22.4       │ 7.7        │
└────────────┴────────────┴────────────┴────────────┴────────────┘
</code></pre></div> <p>It&#39;s a classification problem with the following classes:</p> <pre><code class="julia hljs">levels(y) |&gt; pprint</code></pre><div class=code_output ><pre><code class="plaintext hljs">["B", "O"]
</code></pre></div> <p>It&#39;s not a very big dataset so we will likely overfit it badly using something as sophisticated as XGBoost but it will do for a demonstration.</p> <pre><code class="julia hljs">train, test = partition(eachindex(y), <span class=hljs-number >0.70</span>, shuffle=<span class=hljs-literal >true</span>, rng=<span class=hljs-number >52</span>)
<span class=hljs-meta >@load</span> XGBoostClassifier
xgb_model = XGBoostClassifier()</code></pre><div class=code_output ><pre><code class="plaintext hljs">MLJModels.XGBoost_.XGBoostClassifier(num_round = 1,
                                     booster = "gbtree",
                                     disable_default_eval_metric = 0,
                                     eta = 0.3,
                                     gamma = 0.0,
                                     max_depth = 6,
                                     min_child_weight = 1.0,
                                     max_delta_step = 0.0,
                                     subsample = 1.0,
                                     colsample_bytree = 1.0,
                                     colsample_bylevel = 1.0,
                                     lambda = 1.0,
                                     alpha = 0.0,
                                     tree_method = "auto",
                                     sketch_eps = 0.03,
                                     scale_pos_weight = 1.0,
                                     updater = "grow_colmaker",
                                     refresh_leaf = 1,
                                     process_type = "default",
                                     grow_policy = "depthwise",
                                     max_leaves = 0,
                                     max_bin = 256,
                                     predictor = "cpu_predictor",
                                     sample_type = "uniform",
                                     normalize_type = "tree",
                                     rate_drop = 0.0,
                                     one_drop = 0,
                                     skip_drop = 0.0,
                                     feature_selector = "cyclic",
                                     top_k = 0,
                                     tweedie_variance_power = 1.5,
                                     objective = "automatic",
                                     base_score = 0.5,
                                     eval_metric = "mlogloss",
                                     seed = 0,) @ 6…29</code></pre></div> <p>Let&#39;s check whether the training and is balanced, <code>StatsBase.countmap</code> is useful for that:</p> <pre><code class="julia hljs">countmap(y[train]) |&gt; pprint</code></pre><div class=code_output ><pre><code class="plaintext hljs">Dict("B" =&gt; 73, "O" =&gt; 67)
</code></pre></div> <p>which is pretty balanced. You could check the same on the test set and full set and the same comment would still hold.</p> <h2 id=xgboost_machine ><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html#xgboost_machine">XGBoost machine</a></h2> <p>Wrap a machine around an XGBoost model &#40;XGB&#41; and the data:</p> <pre><code class="julia hljs">xgb  = XGBoostClassifier()
xgbm = machine(xgb, X, y)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Machine{XGBoostClassifier} @ 3…93
</code></pre></div> <p>We will tune it varying the number of rounds used and generate a learning curve</p> <pre><code class="julia hljs">r = range(xgb, :num_round, lower=<span class=hljs-number >10</span>, upper=<span class=hljs-number >500</span>)
curve = learning_curve!(xgbm, resampling=CV(),
                        range=r, resolution=<span class=hljs-number >25</span>,
                        measure=cross_entropy)</code></pre><div class=code_output ><pre><code class="plaintext hljs">(parameter_name = "num_round",
 parameter_scale = :linear,
 parameter_values = [10, 30, 51, 71, 92, 112, 132, 153, 173, 194, 214, 235, 255, 275, 296, 316, 337, 357, 378, 398, 418, 439, 459, 480, 500],
 measurements = [0.8189709782600403, 0.9566784501075745, 0.9794661998748779, 0.9755905270576477, 0.973228931427002, 0.9782517552375793, 0.9744570851325989, 0.9760178923606873, 0.9730918407440186, 0.9792575836181641, 0.9787470698356628, 0.9826120734214783, 0.9854166507720947, 0.9833042621612549, 0.9805116057395935, 0.9808792471885681, 0.9800875186920166, 0.9776668548583984, 0.979369580745697, 0.9778077602386475, 0.9787402749061584, 0.9779520630836487, 0.9782482981681824, 0.9770736694335938, 0.9777476787567139],)</code></pre></div> <p>Let&#39;s have a look</p> <pre><code class="julia hljs">figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
plot(curve.parameter_values, curve.measurements)
xlabel(<span class=hljs-string >"Number of rounds"</span>, fontsize=<span class=hljs-number >14</span>)
ylabel(<span class=hljs-string >"Cross entropy"</span>, fontsize=<span class=hljs-number >14</span>)
xticks([<span class=hljs-number >10</span>, <span class=hljs-number >100</span>, <span class=hljs-number >250</span>, <span class=hljs-number >500</span>], fontsize=<span class=hljs-number >12</span>)
yticks(<span class=hljs-number >0.8</span>:<span class=hljs-number >0.05</span>:<span class=hljs-number >1</span>, fontsize=<span class=hljs-number >12</span>)
</code></pre> <p><img src="/MLJTutorials/assets/literate/EX-crabs-xgb-curve1.svg" alt="" /></p> <p>So we&#39;re doing quite a good job with 100 rounds. Let&#39;s fix that:</p> <pre><code class="julia hljs">xgb.num_round = <span class=hljs-number >100</span>;</code></pre>
<h3 id=more_tuning_1 ><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html#more_tuning_1">More tuning &#40;1&#41;</a></h3>
<p>Let&#39;s now tune the maximum depth of each tree and the minimum child weight in the boosting.</p>
<pre><code class="julia hljs">r1 = range(xgb, :max_depth, lower=<span class=hljs-number >3</span>, upper=<span class=hljs-number >10</span>)
r2 = range(xgb, :min_child_weight, lower=<span class=hljs-number >0</span>, upper=<span class=hljs-number >5</span>)

tm = TunedModel(model=xgb, tuning=Grid(resolution=<span class=hljs-number >8</span>),
                resampling=CV(rng=<span class=hljs-number >11</span>), ranges=[r1,r2],
                measure=cross_entropy)
mtm = machine(tm, X, y)
fit!(mtm, rows=train)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Machine{ProbabilisticTunedModel} @ 1…42
</code></pre></div>
<p>Great, as always we can investigate the tuning by using <code>report</code> and can, for instance, plot a heatmap of the measurements:</p>
<pre><code class="julia hljs">r = report(mtm)

md = r.parameter_values[:,<span class=hljs-number >1</span>]
mcw = r.parameter_values[:,<span class=hljs-number >2</span>]

figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
tricontourf(md, mcw, r.measurements)

xlabel(<span class=hljs-string >"Maximum tree depth"</span>, fontsize=<span class=hljs-number >14</span>)
ylabel(<span class=hljs-string >"Minimum child weight"</span>, fontsize=<span class=hljs-number >14</span>)
xticks(<span class=hljs-number >3</span>:<span class=hljs-number >2</span>:<span class=hljs-number >10</span>, fontsize=<span class=hljs-number >12</span>)
yticks(fontsize=<span class=hljs-number >12</span>)
</code></pre>
<p><img src="/MLJTutorials/assets/literate/EX-crabs-xgb-heatmap.svg" alt="" /></p>
<p>Let&#39;s extract the optimal model and inspect its parameters:</p>
<pre><code class="julia hljs">xgb = fitted_params(mtm).best_model
<span class=hljs-meta >@show</span> xgb.max_depth
<span class=hljs-meta >@show</span> xgb.min_child_weight</code></pre><div class=code_output ><pre><code class="plaintext hljs">xgb.max_depth = 5
xgb.min_child_weight = 1.4285714285714286
</code></pre></div>
<h3 id=more_tuning_2 ><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html#more_tuning_2">More tuning &#40;2&#41;</a></h3>
<p>Let&#39;s examine the effect of <code>gamma</code>:</p>
<pre><code class="julia hljs">xgbm = machine(xgb, X, y)
r = range(xgb, :gamma, lower=<span class=hljs-number >0</span>, upper=<span class=hljs-number >10</span>)
curve = learning_curve!(xgbm, resampling=CV(),
                        range=r, resolution=<span class=hljs-number >30</span>,
                        measure=cross_entropy);</code></pre>
<p>actually it doesn&#39;t look like it&#39;s changing anything:</p>
<pre><code class="julia hljs"><span class=hljs-meta >@show</span> round(minimum(curve.measurements), sigdigits=<span class=hljs-number >3</span>)
<span class=hljs-meta >@show</span> round(maximum(curve.measurements), sigdigits=<span class=hljs-number >3</span>)</code></pre><div class=code_output ><pre><code class="plaintext hljs">round(minimum(curve.measurements), sigdigits=3) = 0.902
round(maximum(curve.measurements), sigdigits=3) = 0.902
</code></pre></div>
<h3 id=more_tuning_3 ><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html#more_tuning_3">More tuning &#40;3&#41;</a></h3>
<p>Let&#39;s examine the effect of <code>subsample</code> and <code>colsample_bytree</code>:</p>
<pre><code class="julia hljs">r1 = range(xgb, :subsample, lower=<span class=hljs-number >0.6</span>, upper=<span class=hljs-number >1.0</span>)
r2 = range(xgb, :colsample_bytree, lower=<span class=hljs-number >0.6</span>, upper=<span class=hljs-number >1.0</span>)
tm = TunedModel(model=xgb, tuning=Grid(resolution=<span class=hljs-number >8</span>),
                resampling=CV(rng=<span class=hljs-number >234</span>), ranges=[r1,r2],
                measure=cross_entropy)
mtm = machine(tm, X, y)
fit!(mtm, rows=train)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Machine{ProbabilisticTunedModel} @ 1…79
</code></pre></div>
<p>and the usual procedure to visualise it:</p>
<pre><code class="julia hljs">r = report(mtm)
ss = r.parameter_values[:,<span class=hljs-number >1</span>]
cbt = r.parameter_values[:,<span class=hljs-number >2</span>]

figure(figsize=(<span class=hljs-number >8</span>,<span class=hljs-number >6</span>))
tricontourf(ss, cbt, r.measurements)

xlabel(<span class=hljs-string >"Sub sample"</span>, fontsize=<span class=hljs-number >14</span>)
ylabel(<span class=hljs-string >"Col sample by tree"</span>, fontsize=<span class=hljs-number >14</span>)
xticks(fontsize=<span class=hljs-number >12</span>)
yticks(fontsize=<span class=hljs-number >12</span>)
</code></pre>
<p><img src="/MLJTutorials/assets/literate/EX-crabs-xgb-heatmap2.svg" alt="" /></p>
<p>Let&#39;s retrieve the best models:</p>
<pre><code class="julia hljs">xgb = fitted_params(mtm).best_model
<span class=hljs-meta >@show</span> xgb.subsample
<span class=hljs-meta >@show</span> xgb.colsample_bytree</code></pre><div class=code_output ><pre><code class="plaintext hljs">xgb.subsample = 0.6571428571428571
xgb.colsample_bytree = 0.8285714285714286
</code></pre></div>
<p>We could continue with more fine tuning but given how small the dataset is, it doesn&#39;t make much sense. How does it fare on the test set?</p>
<pre><code class="julia hljs">ŷ = predict_mode(mtm, rows=test)
round(misclassification_rate(ŷ, y[test]), sigdigits=<span class=hljs-number >3</span>)</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.0667</code></pre></div>
<div class=page-foot >
  <div class=copyright >
    &copy; Anthony Blaom, Thibaut Lienart and collaborators. Last modified: October 22, 2019. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>.
  </div>
</div>

</div>

      </div> 
  </div> 
  <script src="/MLJTutorials/libs/pure/ui.min.js"></script>