<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/MLJTutorials/libs/highlight/github.min.css"> <link rel=stylesheet  href="/MLJTutorials/css/judoc.css"> <link rel=stylesheet  href="/MLJTutorials/css/pure.css"> <link rel=stylesheet  href="/MLJTutorials/css/side-menu.css"> <link rel=stylesheet  href="/MLJTutorials/css/extra.css"> <title></title> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/MLJTutorials/" id=menu-logo-link > <div class=menu-logo > <img id=menu-logo  alt="MLJ Logo" src="/MLJTutorials/assets/infra/MLJLogo2.svg" /> <p><strong>MLJ Tutorials</strong></p> </div> </a> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/MLJTutorials/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/choosing-a-model.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/fit-and-predict.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/model-tuning.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/ensembles.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/ensembles-2.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/composing-models.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/learning-networks.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/learning-networks-2.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks (2)</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item pure-menu-selected"><a href="/MLJTutorials/pub/end-to-end/AMES.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/wine.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/horse.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Horse</a> </ul> <li class=pure-menu-sublist-title ><strong>Intro to Stats Learning</strong> <ul class=pure-menu-sublist  id=isl> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-2.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 2</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-3.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 3</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-4.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 4</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-5.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 5</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/isl/lab-6b.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Lab 6b</a> </ul> </ul> </div> </div> <div id=main > <div class=jd-content > <h1 id=ames ><a href="/MLJTutorials/pub/end-to-end/AMES.html#ames">AMES</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/notebooks/EX-AMES.ipynb" target=_blank ><em>notebook</em></a>, <em>the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/scripts/EX-AMES.jl" target=_blank ><em>raw script</em></a>, <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/master/scripts/EX-AMES.jl" target=_blank ><em>annoted script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=jd-toc ><ol><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#baby_steps">Baby steps</a><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#dummy_model">Dummy model</a><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#knn-ridge_blend">KNN-Ridge blend</a><ol><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#using_the_expanded_syntax">Using the expanded syntax</a><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#using_the_quotarrowquot_syntax">Using the &quot;arrow&quot; syntax</a><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#tuning_the_model">Tuning the model</a></ol></ol></div><h2 id=baby_steps ><a href="/MLJTutorials/pub/end-to-end/AMES.html#baby_steps">Baby steps</a></h2> <p>Let&#39;s load a reduced version of the well-known Ames House Price data set &#40;containing six of the more important categorical features and six of the more important numerical features&#41;. As &quot;iris&quot; the dataset is so common that you can load it directly with <code>@load_ames</code> and the reduced version via <code>@load_reduced_ames</code> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLJ, PrettyPrinting, DataFrames, Statistics

X, y = <span class=hljs-meta >@load_reduced_ames</span>
X = DataFrame(X)
<span class=hljs-meta >@show</span> size(X)
first(X, <span class=hljs-number >3</span>) |&gt; pretty</code></pre><div class=code_output ><pre><code class="plaintext hljs">size(X) = (1456, 12)
┌──────────────────────────┬────────────┬──────────────────────────┬────────────┬─────────────┬────────────┬────────────┬────────────┬──────────────────────────┬────────────┬──────────────┬────────────┐
│ OverallQual              │ GrLivArea  │ Neighborhood             │ x1stFlrSF  │ TotalBsmtSF │ BsmtFinSF1 │ LotArea    │ GarageCars │ MSSubClass               │ GarageArea │ YearRemodAdd │ YearBuilt  │
│ CategoricalString{UInt8} │ Float64    │ CategoricalString{UInt8} │ Float64    │ Float64     │ Float64    │ Float64    │ Int64      │ CategoricalString{UInt8} │ Int64      │ Float64      │ Float64    │
│ Multiclass{10}           │ Continuous │ Multiclass{25}           │ Continuous │ Continuous  │ Continuous │ Continuous │ Count      │ Multiclass{15}           │ Count      │ Continuous   │ Continuous │
├──────────────────────────┼────────────┼──────────────────────────┼────────────┼─────────────┼────────────┼────────────┼────────────┼──────────────────────────┼────────────┼──────────────┼────────────┤
│ 5                        │ 816.0      │ Mitchel                  │ 816.0      │ 816.0       │ 816.0      │ 6600.0     │ 2          │ _20                      │ 816        │ 2003.0       │ 1982.0     │
│ 8                        │ 2028.0     │ Timber                   │ 2028.0     │ 1868.0      │ 1460.0     │ 11443.0    │ 3          │ _20                      │ 880        │ 2006.0       │ 2005.0     │
│ 7                        │ 1509.0     │ Gilbert                  │ 807.0      │ 783.0       │ 0.0        │ 7875.0     │ 2          │ _60                      │ 393        │ 2003.0       │ 2003.0     │
└──────────────────────────┴────────────┴──────────────────────────┴────────────┴─────────────┴────────────┴────────────┴────────────┴──────────────────────────┴────────────┴──────────────┴────────────┘
</code></pre></div> <p>and the target is a continuous vector:</p> <pre><code class="julia hljs"><span class=hljs-meta >@show</span> y[<span class=hljs-number >1</span>:<span class=hljs-number >3</span>]
scitype(y)</code></pre><div class=code_output ><pre><code class="plaintext hljs">y[1:3] = [138000.0, 369900.0, 180000.0]
AbstractArray{Continuous,1}</code></pre></div> <p>so this is a standard regression problem with a mix of categorical and continuous input.</p> <h2 id=dummy_model ><a href="/MLJTutorials/pub/end-to-end/AMES.html#dummy_model">Dummy model</a></h2> <p>Remember that a model is just a container for hyperparameters; let&#39;s take a particularly simple one: the constant regression.</p> <pre><code class="julia hljs">creg = ConstantRegressor()</code></pre><div class=code_output ><pre><code class="plaintext hljs">ConstantRegressor(distribution_type = Distributions.Normal,) @ 5…95</code></pre></div>
<p>Wrapping the model in data creates a <em>machine</em> which will store training outcomes &#40;<em>fit-results</em>&#41;</p>
<pre><code class="julia hljs">cmach = machine(creg, X, y)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Machine{ConstantRegressor} @ 1…61
</code></pre></div>
<p>You can now train the machine specifying the data it should be trained on &#40;if unspecified, all the data will be used&#41;;</p>
<pre><code class="julia hljs">train, test = partition(eachindex(y), <span class=hljs-number >0.70</span>, shuffle=<span class=hljs-literal >true</span>); <span class=hljs-comment ># 70:30 split</span>
fit!(cmach, rows=train)
ŷ = predict(cmach, rows=test)
ŷ[<span class=hljs-number >1</span>:<span class=hljs-number >3</span>] |&gt; pprint</code></pre><div class=code_output ><pre><code class="plaintext hljs">[Distributions.Normal{Float64}(μ=181489.71736997055, σ=77912.05830349971),
 Distributions.Normal{Float64}(μ=181489.71736997055, σ=77912.05830349971),
 Distributions.Normal{Float64}(μ=181489.71736997055, σ=77912.05830349971)]
</code></pre></div>
<p>Observe that the output is probabilistic, each element is a univariate normal distribution &#40;with the same mean and variance as it&#39;s a constant model&#41;.</p>
<p>You can recover deterministic output by either computing the mean of predictions or using <code>predict_mean</code> directly &#40;the <code>mean</code> function can  bve applied to any distribution from <a href="https://github.com/JuliaStats/Distributions.jl"><code>Distributions.jl</code></a>&#41;:</p>
<pre><code class="julia hljs">ŷ = predict_mean(cmach, rows=test)
ŷ[<span class=hljs-number >1</span>:<span class=hljs-number >3</span>]</code></pre><div class=code_output ><pre><code class="plaintext hljs">3-element Array{Float64,1}:
 181489.71736997055
 181489.71736997055
 181489.71736997055</code></pre></div>
<p>You can then call one of the loss functions to assess the quality of the model by comparing the performances on the test set:</p>
<pre><code class="julia hljs">rmsl(ŷ, y[test])</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.4050170626046206</code></pre></div>
<h2 id=knn-ridge_blend ><a href="/MLJTutorials/pub/end-to-end/AMES.html#knn-ridge_blend">KNN-Ridge blend</a></h2>
<p>Let&#39;s try something a bit fancier than a constant regressor.</p>
<ul>
<li><p>one-hot-encode categorical inputs</p>

<li><p>log-transform the target</p>

<li><p>fit both a KNN regression and a Ridge regression on the data</p>

<li><p>Compute a weighted average of individual model predictions</p>

<li><p>inverse transform &#40;exponentiate&#41; the blended prediction</p>

</ul>
<p>You will first define a fixed model where all hyperparameters are specified or set to default. Then you will see how to create a model around a learning network that can be tuned.</p>
<pre><code class="julia hljs"><span class=hljs-meta >@load</span> RidgeRegressor pkg=<span class=hljs-string >"MultivariateStats"</span>
<span class=hljs-meta >@load</span> KNNRegressor</code></pre><div class=code_output ><pre><code class="plaintext hljs">KNNRegressor(K = 5,
             algorithm = :kdtree,
             metric = Distances.Euclidean(0.0),
             leafsize = 10,
             reorder = true,
             weights = :uniform,) @ 1…25</code></pre></div>
<h3 id=using_the_expanded_syntax ><a href="/MLJTutorials/pub/end-to-end/AMES.html#using_the_expanded_syntax">Using the expanded syntax</a></h3>
<p>Let&#39;s start by defining the source nodes:</p>
<pre><code class="julia hljs">Xs = source(X)
ys = source(y, kind=:target)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Source{:target} @ 1…20
</code></pre></div>
<p>On the &quot;first layer&quot;, there&#39;s one hot encoder and a log transform, these will respectively lead to node <code>W</code> and node <code>z</code>:</p>
<pre><code class="julia hljs">hot = machine(OneHotEncoder(), Xs)

W = transform(hot, Xs)
z = log(ys);</code></pre>
<p>On the &quot;second layer&quot;, there&#39;s a KNN regressor and a ridge regressor, these lead to node <code>ẑ₁</code> and <code>ẑ₂</code>
<pre><code class="julia hljs">knn   = machine(KNNRegressor(K=<span class=hljs-number >5</span>), W, z)
ridge = machine(RidgeRegressor(lambda=<span class=hljs-number >2.5</span>), W, z)

ẑ₁ = predict(ridge, W)
ẑ₂ = predict(knn, W)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Node @ 1…27 = predict(1…71, transform(8…18, 1…23))</code></pre></div>
<p>On the &quot;third layer&quot;, there&#39;s a weighted combination of the two regression models:</p>
<pre><code class="julia hljs">ẑ = <span class=hljs-number >0.3</span>ẑ₁ + <span class=hljs-number >0.7</span>ẑ₂;</code></pre>
<p>And finally we need to invert the initial transformation of the target &#40;which was a log&#41;:</p>
<pre><code class="julia hljs">ŷ = exp(ẑ);</code></pre>
<p>You&#39;ve now defined a full learning network which you can fit and use for prediction:</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train)
ypreds = ŷ(rows=test)
rmsl(y[test], ypreds)</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.18432329273233017</code></pre></div>
<h3 id=using_the_quotarrowquot_syntax ><a href="/MLJTutorials/pub/end-to-end/AMES.html#using_the_quotarrowquot_syntax">Using the &quot;arrow&quot; syntax</a></h3>
<p>If you&#39;re using Julia 1.3, you can use the following syntax to do the same thing.</p>
<p><em>First layer</em>: one hot encoding and log transform:</p>
<pre><code class="julia hljs">W = Xs |&gt; OneHotEncoder()
z = ys |&gt; log;</code></pre>
<p><em>Second layer</em>: KNN Regression and Ridge regression</p>
<pre><code class="julia hljs">ẑ₁ = (W, z) |&gt; KNNRegressor(K=<span class=hljs-number >5</span>)
ẑ₂ = (W, z) |&gt; RidgeRegressor(lambda=<span class=hljs-number >2.5</span>);</code></pre>
<p><em>Third layer</em>: weighted sum of the two models:</p>
<pre><code class="julia hljs">ẑ = <span class=hljs-number >0.3</span>ẑ₁ + <span class=hljs-number >0.7</span>ẑ₂;</code></pre>
<p>then the inverse transform</p>
<pre><code class="julia hljs">ŷ = exp(ẑ);</code></pre>
<p>You can then fit and evaluate the model as usual:</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train)
rmsl(y[test], ŷ(rows=test))</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.1419255153173174</code></pre></div>
<h3 id=tuning_the_model ><a href="/MLJTutorials/pub/end-to-end/AMES.html#tuning_the_model">Tuning the model</a></h3>
<p>So far the hyperparameters were explicitly given but it makes more sense to learn them. For this, we define a model around the learning network which can then be trained and tuned as any model:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >mutable struct</span> KNNRidgeBlend &lt;: DeterministicNetwork
    knn_model::KNNRegressor
    ridge_model::RidgeRegressor
    knn_weight::<span class=hljs-built_in >Float64</span>
<span class=hljs-keyword >end</span></code></pre>
<p>We must specify how such a model should be fit, which is effectively just the learning network we had defined before except that now the parameters are contained in the struct:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> MLJ.fit(model::KNNRidgeBlend, verbosity::<span class=hljs-built_in >Int</span>, X, y)
    Xs = source(X)
    ys = source(y, kind=:target)
    hot = machine(OneHotEncoder(), Xs)
    W = transform(hot, Xs)
    z = log(ys)
    ridge_model = model.ridge_model
    knn_model = model.knn_model
    ridge = machine(ridge_model, W, z)
    knn = machine(knn_model, W, z)
    <span class=hljs-comment ># and finally</span>
    ẑ = model.knn_weight * predict(knn, W) + (<span class=hljs-number >1.0</span> - model.knn_weight) * predict(ridge, W)
    ŷ = exp(ẑ)
    fit!(ŷ, verbosity=<span class=hljs-number >0</span>)
    <span class=hljs-keyword >return</span> fitresults(ŷ)
<span class=hljs-keyword >end</span></code></pre>
<p><strong>Note</strong>: you really  want to set <code>verbosity&#61;0</code> here otherwise in the tuning you will get a lot of verbose output&#33;</p>
<p>You can now instantiate and fit such a model:</p>
<pre><code class="julia hljs">krb = KNNRidgeBlend(KNNRegressor(K=<span class=hljs-number >5</span>), RidgeRegressor(lambda=<span class=hljs-number >2.5</span>), <span class=hljs-number >0.3</span>)
mach = machine(krb, X, y)
fit!(mach, rows=train)

preds = predict(mach, rows=test)
rmsl(y[test], preds)</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.1419255153173174</code></pre></div>
<p>But more interestingly, the hyperparameters of the model can be tuned.</p>
<p>Before we get started, it&#39;s important to note that the hyperparameters of the model have different levels of <em>nesting</em>. This becomes explicit when trying to access elements:</p>
<pre><code class="julia hljs"><span class=hljs-meta >@show</span> krb.knn_weight
<span class=hljs-meta >@show</span> krb.knn_model.K
<span class=hljs-meta >@show</span> krb.ridge_model.lambda</code></pre><div class=code_output ><pre><code class="plaintext hljs">krb.knn_weight = 0.3
krb.knn_model.K = 5
krb.ridge_model.lambda = 2.5
</code></pre></div>
<p>You can also see all the hyperparameters using the <code>params</code> function:</p>
<pre><code class="julia hljs">params(krb) |&gt; pprint</code></pre><div class=code_output ><pre><code class="plaintext hljs">(knn_model = (K = 5,
              algorithm = :kdtree,
              metric = Distances.Euclidean(0.0),
              leafsize = 10,
              reorder = true,
              weights = :uniform),
 ridge_model = (lambda = 2.5,),
 knn_weight = 0.3)
</code></pre></div>
<p>The range of values to do your hyperparameter tuning over should follow the nesting structure reflected by <code>params</code>:</p>
<pre><code class="julia hljs">k_range = range(krb, :(knn_model.K), lower=<span class=hljs-number >2</span>, upper=<span class=hljs-number >100</span>, scale=:log10)
l_range = range(krb, :(ridge_model.lambda), lower=<span class=hljs-number >1e-4</span>, upper=<span class=hljs-number >10</span>, scale=:log10)
w_range = range(krb, :(knn_weight), lower=<span class=hljs-number >0.1</span>, upper=<span class=hljs-number >0.9</span>)

ranges = [k_range, l_range, w_range]</code></pre><div class=code_output ><pre><code class="plaintext hljs">3-element Array{MLJ.NumericRange{T,Symbol} where T,1}:
 NumericRange @ 5…04
 NumericRange @ 9…57
 NumericRange @ 5…40</code></pre></div>
<p>Now there remains to define how the tuning should be done, let&#39;s just specify a very coarse grid tuning with cross validation and instantiate a tuned model:</p>
<pre><code class="julia hljs">tuning = Grid(resolution=<span class=hljs-number >3</span>)
resampling = CV(nfolds=<span class=hljs-number >6</span>)

tm = TunedModel(model=krb, tuning=tuning, resampling=resampling,
                ranges=ranges, measure=rmsl)</code></pre><div class=code_output ><pre><code class="plaintext hljs">MLJ.DeterministicTunedModel(model = JuDoc.KNNRidgeBlend(knn_model = KNNRegressor @ 1…63,
                                                        ridge_model = RidgeRegressor @ 3…89,
                                                        knn_weight = 0.3,),
                            tuning = Grid(resolution = 3,
                                          acceleration = ComputationalResources.CPU1{Nothing}(nothing),),
                            resampling = CV(nfolds = 6,
                                            shuffle = false,
                                            rng = Random._GLOBAL_RNG(),),
                            measure = MLJBase.RMSL(),
                            weights = nothing,
                            operation = StatsBase.predict,
                            ranges = MLJ.NumericRange{T,Symbol} where T[NumericRange @ 5…04, NumericRange @ 9…57, NumericRange @ 5…40],
                            full_report = true,
                            train_best = true,) @ 4…32</code></pre></div>
<p>which we can now finally fit...</p>
<pre><code class="julia hljs">mtm = machine(tm, X, y)
fit!(mtm, rows=train);</code></pre>
<p>To retrieve the best model, you can use:</p>
<pre><code class="julia hljs">krb_best = fitted_params(mtm).best_model
<span class=hljs-meta >@show</span> krb_best.knn_model.K
<span class=hljs-meta >@show</span> krb_best.ridge_model.lambda
<span class=hljs-meta >@show</span> krb_best.knn_weight</code></pre><div class=code_output ><pre><code class="plaintext hljs">krb_best.knn_model.K = 2
krb_best.ridge_model.lambda = 0.03162277660168379
krb_best.knn_weight = 0.1
</code></pre></div>
<p>you can also use <code>mtm</code> to make predictions &#40;which will be done using the best model&#41;</p>
<pre><code class="julia hljs">preds = predict(mtm, rows=test)
rmsl(y[test], preds)</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.13141571479175176</code></pre></div>
<div class=page-foot >
  <div class=copyright >
    &copy; Anthony Blaom, Thibaut Lienart and collaborators. Last modified: October 24, 2019. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>.
  </div>
</div>

</div>

      </div> 
  </div> 
  <script src="/MLJTutorials/libs/pure/ui.min.js"></script>