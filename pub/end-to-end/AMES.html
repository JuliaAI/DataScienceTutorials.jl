<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/MLJTutorials/libs/highlight/github.min.css"> <link rel=stylesheet  href="/MLJTutorials/css/judoc.css"> <link rel=stylesheet  href="/MLJTutorials/css/pure.css"> <link rel=stylesheet  href="/MLJTutorials/css/side-menu.css"> <link rel=stylesheet  href="/MLJTutorials/css/extra.css"> <title></title> <div id=layout > <a href="#menu" id=menuLink  class=menu-link ><span></span></a> <div id=menu > <div class=pure-menu > <a href="/MLJTutorials/" id=menu-logo-link > <div class=menu-logo > <img id=menu-logo  alt="MLJ Logo" src="/MLJTutorials/assets/infra/MLJLogo2.svg" /> <p><strong>MLJ Tutorials</strong></p> </div> </a> <ul class=pure-menu-list > <li class="pure-menu-item pure-menu-top-item "><a href="/MLJTutorials/" class=pure-menu-link ><strong>Home</strong></a> <li class=pure-menu-sublist-title ><strong>Getting started</strong> <ul class=pure-menu-sublist > <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/choosing-a-model.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Choosing a model</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/fit-and-predict.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Fit, predict, transform</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/model-tuning.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Model tuning</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/ensembles.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/ensembles-2.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Ensembles (2)</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/composing-models.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Composing models</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/getting-started/learning-networks.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Learning networks</a> </ul> <li class=pure-menu-sublist-title ><strong>End to end examples</strong> <ul class=pure-menu-sublist  id=e2e> <li class="pure-menu-item pure-menu-selected"><a href="/MLJTutorials/pub/end-to-end/AMES.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> AMES</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/wine.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Wine</a> <li class="pure-menu-item "><a href="/MLJTutorials/pub/end-to-end/crabs-xgb.html" class=pure-menu-link ><span style="padding-right:0.5rem;">•</span> Crabs (XGB)</a> </ul> </ul> </div> </div> <div id=main > <div class=jd-content > <h1 id=ames ><a href="/MLJTutorials/pub/end-to-end/AMES.html#ames">AMES</a></h1> <em>Download the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/notebooks/EX-AMES.ipynb" target=_blank ><em>notebook</em></a> <em>or the</em> <a href="https://raw.githubusercontent.com/alan-turing-institute/MLJTutorials/gh-pages/scripts/EX-AMES.jl" target=_blank ><em>raw script</em></a> <em>for this tutorial &#40;right-click on the link and save&#41;.</em> <div class=jd-toc ><ol><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#baby_steps">Baby steps</a><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#dummy_model">Dummy model</a><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#knn-ridge_blend">KNN-Ridge blend</a><ol><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#using_the_expanded_syntax">Using the expanded syntax</a><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#using_the_quotarrowquot_syntax">Using the &quot;arrow&quot; syntax</a><li><a href="/MLJTutorials/pub/end-to-end/AMES.html#tuning_the_model">Tuning the model</a></ol></ol></div><h2 id=baby_steps ><a href="/MLJTutorials/pub/end-to-end/AMES.html#baby_steps">Baby steps</a></h2> <p>Let&#39;s load a reduced version of the well-known Ames House Price data set &#40;containing six of the more important categorical features and six of the more important numerical features&#41;. As &quot;iris&quot; the dataset is so common that you can load it directly with <code>@load_ames</code> and the reduced version via <code>@load_reduced_ames</code> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLJ, MLJBase, PrettyPrinting, DataFrames, Statistics

X, y = <span class=hljs-meta >@load_reduced_ames</span>
<span class=hljs-meta >@show</span> size(X)
first(X, <span class=hljs-number >3</span>) |&gt; pretty</code></pre><div class=code_output ><pre><code class="plaintext hljs">size(X) = (1456, 12)
┌─────────────────────────────────────────────────┬────────────┬─────────────────────────────────────────────┬────────────┬─────────────┬────────────┬────────────┬────────────┬─────────────────────────────────────────────┬────────────┬──────────────┬────────────┐
│ OverallQual                                     │ GrLivArea  │ Neighborhood                                │ x1stFlrSF  │ TotalBsmtSF │ BsmtFinSF1 │ LotArea    │ GarageCars │ MSSubClass                                  │ GarageArea │ YearRemodAdd │ YearBuilt  │
│ CategoricalArrays.CategoricalValue{Int64,UInt8} │ Float64    │ CategoricalArrays.CategoricalString{UInt32} │ Float64    │ Float64     │ Float64    │ Float64    │ Int64      │ CategoricalArrays.CategoricalString{UInt32} │ Int64      │ Float64      │ Float64    │
│ OrderedFactor{10}                               │ Continuous │ Multiclass{25}                              │ Continuous │ Continuous  │ Continuous │ Continuous │ Count      │ Multiclass{15}                              │ Count      │ Continuous   │ Continuous │
├─────────────────────────────────────────────────┼────────────┼─────────────────────────────────────────────┼────────────┼─────────────┼────────────┼────────────┼────────────┼─────────────────────────────────────────────┼────────────┼──────────────┼────────────┤
│ 5                                               │ 816.0      │ Mitchel                                     │ 816.0      │ 816.0       │ 816.0      │ 6600.0     │ 2          │ _20                                         │ 816        │ 2003.0       │ 1982.0     │
│ 8                                               │ 2028.0     │ Timber                                      │ 2028.0     │ 1868.0      │ 1460.0     │ 11443.0    │ 3          │ _20                                         │ 880        │ 2006.0       │ 2005.0     │
│ 7                                               │ 1509.0     │ Gilbert                                     │ 807.0      │ 783.0       │ 0.0        │ 7875.0     │ 2          │ _60                                         │ 393        │ 2003.0       │ 2003.0     │
└─────────────────────────────────────────────────┴────────────┴─────────────────────────────────────────────┴────────────┴─────────────┴────────────┴────────────┴────────────┴─────────────────────────────────────────────┴────────────┴──────────────┴────────────┘
</code></pre></div> <p>and the target is a continuous vector:</p> <pre><code class="julia hljs"><span class=hljs-meta >@show</span> y[<span class=hljs-number >1</span>:<span class=hljs-number >3</span>]
scitype_union(y)</code></pre><div class=code_output ><pre><code class="plaintext hljs">y[1:3] = [138000.0, 369900.0, 180000.0]
Continuous</code></pre></div> <p>so this is a standard regression problem with a mix of categorical and continuous input.</p> <h2 id=dummy_model ><a href="/MLJTutorials/pub/end-to-end/AMES.html#dummy_model">Dummy model</a></h2> <p>Remember that a model is just a container for hyperparameters; let&#39;s take a particularly simple one: the constant regression.</p> <pre><code class="julia hljs">creg = ConstantRegressor()</code></pre><div class=code_output ><pre><code class="plaintext hljs">ConstantRegressor(distribution_type = Distributions.Normal,) @ 1…51</code></pre></div>
<p>Wrapping the model in data creates a <em>machine</em> which will store training outcomes &#40;<em>fit-results</em>&#41;</p>
<pre><code class="julia hljs">cmach = machine(creg, X, y)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Machine{ConstantRegressor} @ 1…13
</code></pre></div>
<p>You can now train the machine specifying the data it should be trained on &#40;if unspecified, all the data will be used&#41;;</p>
<pre><code class="julia hljs">train, test = partition(eachindex(y), <span class=hljs-number >0.70</span>, shuffle=<span class=hljs-literal >true</span>); <span class=hljs-comment ># 70:30 split</span>
fit!(cmach, rows=train)
ŷ = predict(cmach, rows=test)
ŷ[<span class=hljs-number >1</span>:<span class=hljs-number >3</span>] |&gt; pprint</code></pre><div class=code_output ><pre><code class="plaintext hljs">[Distributions.Normal{Float64}(μ=181022.65848871443, σ=74975.21062525548),
 Distributions.Normal{Float64}(μ=181022.65848871443, σ=74975.21062525548),
 Distributions.Normal{Float64}(μ=181022.65848871443, σ=74975.21062525548)]
</code></pre></div>
<p>Observe that the output is probabilistic, each element is a univariate normal distribution &#40;with the same mean and variance as it&#39;s a constant model&#41;.</p>
<p>You can recover deterministic output by either computing the mean of predictions or using <code>predict_mean</code> directly &#40;the <code>mean</code> function can  bve applied to any distribution from <a href="https://github.com/JuliaStats/Distributions.jl"><code>Distributions.jl</code></a>&#41;:</p>
<pre><code class="julia hljs">ŷ = predict_mean(cmach, rows=test)
ŷ[<span class=hljs-number >1</span>:<span class=hljs-number >3</span>]</code></pre><div class=code_output ><pre><code class="plaintext hljs">3-element Array{Float64,1}:
 181022.65848871443
 181022.65848871443
 181022.65848871443</code></pre></div>
<p>You can then call one of the loss functions to assess the quality of the model by comparing the performances on the test set:</p>
<pre><code class="julia hljs">rmsl(ŷ, y[test])</code></pre><div class=code_output ><pre><code class="plaintext hljs">0.42407113929256157</code></pre></div>
<h2 id=knn-ridge_blend ><a href="/MLJTutorials/pub/end-to-end/AMES.html#knn-ridge_blend">KNN-Ridge blend</a></h2>
<p>Let&#39;s try something a bit fancier than a constant regressor.</p>
<ul>
<li><p>one-hot-encode categorical inputs</p>

<li><p>log-transform the target</p>

<li><p>fit both a KNN regression and a Ridge regression on the data</p>

<li><p>Compute a weighted average of individual model predictions</p>

<li><p>inverse transform &#40;exponentiate&#41; the blended prediction</p>

</ul>
<p>You will first define a fixed model where all hyperparameters are specified or set to default. Then you will see how to create a model around a learning network that can be tuned.</p>
<pre><code class="julia hljs"><span class=hljs-meta >@load</span> RidgeRegressor pkg=<span class=hljs-string >"MultivariateStats"</span>
<span class=hljs-meta >@load</span> KNNRegressor</code></pre><div class=code_output ><pre><code class="plaintext hljs">MLJModels.NearestNeighbors_.KNNRegressor(K = 5,
                                         algorithm = :kdtree,
                                         metric = Distances.Euclidean(0.0),
                                         leafsize = 10,
                                         reorder = true,
                                         weights = :uniform,) @ 4…11</code></pre></div>
<h3 id=using_the_expanded_syntax ><a href="/MLJTutorials/pub/end-to-end/AMES.html#using_the_expanded_syntax">Using the expanded syntax</a></h3>
<p>Let&#39;s start by defining the source nodes:</p>
<pre><code class="julia hljs">Xs = source(X)
ys = source(ys, kind=:target)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Source{:target} @ 9…65
</code></pre></div>
<p>On the &quot;first layer&quot;, there&#39;s one hot encoder and a log transform, these will respectively lead to node <code>W</code> and node <code>z</code>:</p>
<pre><code class="julia hljs">hot = machine(OneHotEncoder(), Xs)

W = transform(hot, Xs)
z = log(ys);</code></pre>
<p>On the &quot;second layer&quot;, there&#39;s a KNN regressor and a ridge regressor, these lead to node <code>ẑ₁</code> and <code>ẑ₂</code>
<pre><code class="julia hljs">knn   = machine(KNNRegressor(K=<span class=hljs-number >5</span>), W, z)
ridge = machine(RidgeRegressor(lambda=<span class=hljs-number >2.5</span>), W, z)

ẑ₁ = predict(ridge, W)
ẑ₂ = predict(knn, W)</code></pre><div class=code_output ><pre><code class="plaintext hljs">Node @ 8…20 = predict(1…96, transform(1…64, 4…13))</code></pre></div>
<p>On the &quot;third layer&quot;, there&#39;s a weighted combination of the two regression models:</p>
<pre><code class="julia hljs">ẑ = <span class=hljs-number >0.3</span>ẑ₁ + <span class=hljs-number >0.7</span>ẑ₂;</code></pre>
<p>And finally we need to invert the initial transformation of the target &#40;which was a log&#41;:</p>
<pre><code class="julia hljs">ŷ = exp(ẑ);</code></pre>
<p>You&#39;ve now defined a full learning network which you can fit and use for prediction:</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train)
ypreds = ŷ(rows=test)
rmsl(y[test], ypreds)</code></pre><div class=code_output ><pre><code class="plaintext hljs">There was an error running the code:
BoundsError([0.8824201966716334, 0.68929788928311, 0.1512831591929075, 1.3423900203590722, 0.16757765382339337, 0.26042025727256435, 0.25432570415676203, 0.8788061137833973, 0.14542136901906572, 0.09611136004739343, 0.7925766142287616, 0.4729944373676558, 0.3946364265791899, 0.1741652106845784, 0.7023921755998833, 0.3965740887075485, 0.24038200525209405, 0.67985972922416, 0.8339518895663548, 0.18535936274504805, 1.0498043623153044, 1.084715014389438, 0.8986568756813592, 0.3045737282410472, 0.24434333529200208, 0.15923811609195698, 0.2533332021436981, 0.46355936641949164, 0.3190814340068368, 0.36753162631576086, 0.41901666858852055, 0.21632126470863855, 0.27652760276772465, 0.3655033964862564, 0.4474756155630158, 0.22600384961264705, 0.15804102383390173, 0.22281112392626068, 0.21638384097337018, 1.294190348229172, 0.8232146439330219, 0.4293786340775122, 1.9045859653820487, 0.9520976554103757, 0.8935985257317205, 0.613504454306241, 0.544876435175521, 0.12231824281552442, 0.19117412355861493, 0.6920131176352526, 0.25339742169029683, 0.5373907346407155, 0.43829363964830675, 0.47041924443686767, 0.768028669868627, 0.8428750055880934, 0.4068229585003875, 0.505511300514387, 0.10475598164480679, 0.15182801044634764, 0.988640162265772, 1.324488473038386, 1.2223897714964775, 0.19450029414129455, 0.23625956450719385, 0.5275911602823729, 0.3404739710701936, 0.24669809193257483, 1.1311497722254635, 0.31163874612051884, 0.4221777077414989, 0.6952766900829044, 0.18722190288115498, 0.3722414412785445, 0.49733314720882843, 1.0175222314035222, 0.5103979159013786, 0.27918647112932854, 0.3619601257584575, 0.47307682469809603, 1.7278172402737, 0.2849482688289057, 0.13022701837933343, 0.43961011141046247, 1.5294454875332666, 0.40533481146744177, 0.7405417966451596, 0.24955857109390914, 0.7167796590847649, 0.5348807754096259, 0.29646201389820664, 0.22047726203703105, 0.6543587320667872, 0.2451964912800568, 0.8399787280075045, 0.43356161508535995, 0.46439749518346085, 0.5016857635672913, 0.18201736110420808, 0.7446829212374304, 0.7587490963160295, 0.1592520788945278, 0.7624692459630795, 0.2808107599592921, 0.5118702884640618, 0.4040889181565197, 0.10084003022455365, 0.4117484262608757, 0.23966047328520262, 0.08034233185889436, 0.6208031397560285, 0.8903688904816113, 0.6787289098406235, 0.7106486207296403, 0.5890051707416946, 0.12308765777660587, 1.50980102659133, 0.2996478799508778, 0.4248158134949732, 0.7419837791955565, 0.42208051464858876, 0.3504915123889207, 0.742779654051853, 1.6930176951169842, 0.2118639027225586, 0.8357115916880172, 0.4395585701527553, 0.18079517335559347, 0.26910061945502756, 0.26453757125929733, 0.17725745776945348, 0.2270229134062552, 0.8830783851172905, 0.647263761015731, 0.11998483658293492, 0.7881410862915491, 0.27984734491564534, 0.8858987038329044, 1.2675986717289036, 0.4834869647078448, 1.6000559596909665, 0.3100419281555279, 0.5107284589753598, 0.5214447196865878, 1.0861289557326201, 0.6317786290440153, 0.33871023217395724, 0.4452852330766957, 0.49769897598866886, 0.9264217016747218, 0.22461849689599472, 0.5916541008348366, 0.3881841242106094, 0.2023404936763391, 1.0761267391627594, 0.10033384841740917, 1.5079968645898694, 0.2589939507099133, 0.6822469667474843, 0.10346350459793882, 0.34599764871860805, 0.8395963882548432, 0.48238333502276476, 0.36275119233088604, 0.8032271232820847, 0.7590450814598337, 0.8874210216168881, 0.3425661578353543, 0.5795051061164107, 0.6770428335512602, 0.1847317571656531, 0.2077258911454392, 0.7529427946769002, 0.4501487524065644, 0.21232498320609183, 0.7951221894466809, 0.19151033184878496, 0.9457347789721045, 0.13695289523087997, 0.35506891245132816, 0.29465509671467105, 0.20821668215012615, 0.497496314412091, 0.2179873339019616, 0.37957379748404846, 0.12865561480590412, 0.22794015237998558, 0.4916265130299221, 0.6517002553668381, 0.7920931794052433, 0.24859798845721778, 0.3317317794554786, 0.5146495886137887, 0.25492941800399727, 0.13439754996492354, 0.47481627797321907, 1.090182031070921, 0.41773091689429415, 0.5879561605968213, 0.09067868772186022, 0.4178191452631903, 0.42192262970609484, 0.2959324434189643, 0.20488179345341206, 0.31331544450585713, 0.2655259927626632, 1.3986986495772982, 0.3812028836449365, 0.2011504019626275, 0.8959017338429036, 0.17352865223052963, 0.33076990922188, 0.10479905631890102, 0.178188838633881, 0.7596313856127547, 0.12451409951212415, 0.635178538404609, 0.2033077353559966, 0.4748779013327535, 0.16454762892057825, 1.1764707940176022, 0.15268158206328208, 0.5454513719683087, 0.1599229041143835, 0.3680575274525032, 0.35173443254167697, 0.7042529189244076, 0.18045399556206848, 0.19852605998471584, 1.0177825937433451, 0.30342432107209844, 0.62590467741741, 0.1914147635478226, 0.5091901298565314, 1.7906490259740349, 0.4390693488531654, 0.26311783764341307, 1.1328468584371516, 0.6441660308613508, 0.3349484609106515, 0.28605272698804846, 0.5916733239284493, 2.1691032880636776, 0.12677615684874236, 0.35656685011818123, 0.654515447277089, 0.29802069722142255, 0.22273995346893127, 0.37019588357751787, 0.690438703135472, 0.8157017124829535, 0.17822447686026976, 0.3312254018793879, 0.2625869529733951, 0.3529633814854357, 0.39117936343280163, 0.33234694611257165, 0.22624650989429135, 0.3119985267667304, 0.13015354935735607, 0.7428854897340736, 1.2217218739199491, 0.3273118050189631, 0.5855668286250185, 0.2949512932012813, 0.5918162798314907, 0.1125109679094201, 0.7303058521828234, 0.6582985468062281, 0.7342164147534327, 0.13240369901418844, 0.4792364158888682, 1.515427630391452, 0.2578704993076123, 0.8684106712723041, 0.1452256842320393, 0.5208582383021536, 0.5047008503141255, 0.46814722932899705, 0.42947027073997035, 0.14922889991468216, 0.16997165239651957, 0.3636560825533641, 1.0123209610690345, 0.14056405828066415, 0.23313807546068108, 0.3750944382676325, 0.19994954416763788, 0.3822669679394559, 0.29746534409764425, 0.31245726470534946, 0.41476554978916486, 1.0268959643798958, 0.8746547998745581, 0.16356444029702139, 0.5443821318736206, 0.47717142292773, 0.17819603305882203, 0.0958872965530087, 0.6882877254615462], ([1297, 721, 1179, 264, 497, 1221, 838, 1294, 733, 488, 777, 959, 267, 203, 615, 1071, 71, 1231, 707, 1188, 372, 36, 141, 33, 121, 534, 618, 1440, 313, 843, 527, 502, 1122, 831, 974, 1041, 651, 893, 697, 517, 1109, 947, 595, 224, 563, 684, 907, 865, 842, 659, 1031, 924, 1003, 762, 1171, 942, 118, 906, 1396, 652, 914, 245, 576, 443, 282, 1142, 356, 132, 494, 719, 756, 620, 1015, 1134, 900, 582, 1002, 556, 1418, 738, 278, 922, 1291, 484, 193, 265, 250, 1302, 713, 133, 1166, 44, 1055, 1205, 573, 281, 1259, 1243, 80, 647, 896, 931, 1144, 42, 510, 990, 151, 916, 331, 940, 473, 1335, 237, 242, 1023, 233, 1113, 1423, 889, 966, 1114, 665, 817, 298, 259, 712, 1116, 63, 378, 148, 391, 230, 91, 531, 693, 359, 1364, 696, 1177, 50, 880, 147, 694, 640, 675, 1379, 1052, 1094, 375, 720, 1169, 125, 1282, 18, 128, 130, 994, 998, 143, 170, 351, 27, 1381, 180, 586, 525, 418, 565, 274, 11, 1357, 486, 1320, 735, 787, 244, 1254, 479, 503, 34, 1195, 530, 112, 26, 521, 6, 60, 149, 49, 324, 480, 708, 594, 946, 1009, 1248, 671, 110, 548, 782, 457, 403, 432, 1163, 1267, 1018, 660, 1421, 1017, 382, 1154, 544, 78, 229, 330, 804, 934, 901, 86, 77, 1087, 1384, 1387, 339, 622, 631, 254, 646, 28, 178, 345, 1447, 1420, 755, 159, 398, 863, 500, 355, 346, 423, 795, 7, 908, 943, 1235, 460, 859, 235, 475, 308, 1312, 961, 19, 167, 412, 803, 1308, 452, 852, 909, 780, 158, 22, 1363, 249, 283, 1180, 234, 275, 124, 62, 511, 699, 1182, 221, 288, 326, 965, 1187, 214, 1369, 134, 501, 1097, 634, 1304, 729, 1019, 637, 825, 1194, 788, 971, 246, 606, 1281, 841, 140, 218, 1213, 145, 644, 641, 1075, 836, 752, 363, 1197, 997, 656, 37, 1361, 455, 1377, 945, 587, 1293, 792, 839, 352, 75, 1157, 815, 39, 1220, 540, 970, 766, 1455, 92, 253, 1039, 1165, 683, 14, 748, 832, 115, 165, 296, 505, 400, 962, 154, 583, 912, 1199, 887, 185, 1362, 1146, 401, 600, 558, 142, 920, 741, 763, 523, 439, 334, 150, 585, 890, 821, 870, 1034, 818, 1295, 894, 367, 1203, 89, 104, 371, 1208, 746, 1082, 1033, 1095, 82, 1124, 695, 506, 636, 113, 881, 47, 1353, 390, 884, 704, 877, 1299, 175, 532, 903, 72, 1209, 581, 1140, 241, 1183, 978, 1125, 122, 1325, 1339, 742, 182, 952, 807, 407, 68, 1274, 666, 765, 434, 295, 357, 873, 676, 374, 718, 1241, 1319, 215, 437, 1108, 20, 951, 1127, 127, 1286, 10, 1223, 854, 1410, 1053, 543, 772, 3, 208, 674, 592, 187, 1406, 388, 1101, 813, 261, 597, 285, 217, 958, 1394, 725, 465, 1025, 1026, 200, 1012, 1086, 667, 661, 1202, 850, 1233, 1347, 513, 864, 469, 1409, 984, 524, 348, 1303, 1439, 1441, 905, 66, 744, 192, 205, 983, 904, 949, 321, 1141, 384, 638, 1193, 329, 672, 43, 157, 1365, 1120, 727, 632, 702, 621, 1344, 377, 231, 1307, 944, 1178, 248, 415, 1456, 438, 617, 584, 1279, 84, 301, 1288, 54, 1284, 1327, 856, 1305, 1272, 991, 392, 1332, 266, 85, 557, 956, 1445, 1435, 999, 989, 449, 1155, 126, 1043, 402, 81, 601, 613, 1300, 99, 753, 515, 679, 1264, 522, 271, 1317, 1204, 1102, 1091, 1159, 76, 292, 347, 358, 212, 860, 761, 1443, 1411, 1349, 161, 1192, 194, 219, 1356, 1270, 450, 169, 1354, 1261, 309, 174, 797, 928, 939, 911, 430, 1008, 272, 1398, 546, 768, 1176, 1185, 1454, 591, 393, 1150, 304, 533, 650, 186, 953, 589, 258, 1242, 117, 472, 1271, 1334, 1277, 627, 1313, 847, 824, 969, 431, 833, 1345, 603, 1448, 745, 993, 690, 1057, 1032, 1268, 1422, 98, 1323, 848, 1285, 898, 1331, 982, 406, 410, 891, 790, 1343, 24, 1397, 1367, 1200, 1278, 294, 1265, 462, 414, 1326, 477, 1428, 1136, 976, 1181, 599, 791, 629, 869, 1139, 114, 751, 774, 624, 785, 717, 1078, 163, 102, 837, 739, 1240, 929, 875, 1239, 806, 1219, 1133, 162, 56, 387, 577, 226, 74, 1380, 299, 476, 111, 1405, 979, 1050, 106, 88, 568, 987, 1174, 1442, 1329, 353, 119, 687, 826, 228, 858, 360, 607, 1338, 293, 625, 1123, 1092, 1047, 692, 1292, 550, 79, 417, 1065, 1416, 1370, 349, 251, 155, 323, 988, 173, 1045, 1400, 487, 861, 1238, 1436, 1378, 571, 1314, 575, 1059, 1404, 1246, 1010, 1372, 1424, 1164, 868, 950, 783, 495, 396, 662, 1162, 771, 204, 714, 1016, 268, 610, 1066, 1316, 1260, 197, 732, 608, 853, 1391, 381, 995, 184, 120, 701, 658, 1044, 1037, 277, 1322, 364, 1427, 1189, 776, 519, 1161, 440, 394, 336, 236, 370, 365, 1315, 649, 350, 897, 1206, 628, 338, 810, 1168, 948, 1309, 225, 682, 554, 1186, 1257, 1417, 485, 918, 64, 516, 923, 619, 1022, 773, 58, 463, 1224, 698, 433, 981, 526, 518, 1426, 731, 343, 1079, 216, 1040, 1342, 376, 136, 448, 198, 373, 1063, 799, 760, 1368, 883, 1216, 968, 1430, 493, 1096, 1280, 1310, 107, 1253, 677, 48, 1112, 70, 213, 542, 317, 379, 767, 866, 689, 206, 447, 1215, 1230, 737, 1160, 172, 680, 195, 1211, 1232, 300, 673, 41, 131, 492, 257, 1027, 784, 688, 1062, 210, 137, 320, 1004, 1099, 1414, 17, 823, 1269, 967, 1449, 567, 1001, 52, 754, 874, 1251, 25, 325, 436, 1392, 123, 1236, 307, 1105, 1190, 97, 711, 604, 51, 252, 902, 1145, 315, 1119, 144, 1042, 885, 555, 871, 291, 1046, 1088, 4, 90, 917, 1006, 749, 1263, 937, 1212, 1438, 1173, 474, 750, 1184, 135, 812, 1403, 1222, 508, 354, 31, 520, 1287, 116, 1084, 1333, 1256, 361, 1118, 723, 303, 975, 399, 1128, 536, 188, 483, 1311, 404, 964, 1030, 23, 421, 156, 559, 913, 709, 1401, 562, 1336, 1111, 1444, 316, 648, 706, 1056, 1390, 260, 504, 29, 425, 380, 1385, 775, 239, 537, 466, 1276, 1013, 482, 1324, 862, 1048, 954, 757, 973, 38, 413, 986, 419, 1412, 1374, 1115, 925, 1074, 758, 846, 1340, 424, 152, 422, 1214, 1098, 645, 996, 1090, 1069, 1083, 886, 94, 882, 327, 103],))
</code></pre></div>
<h3 id=using_the_quotarrowquot_syntax ><a href="/MLJTutorials/pub/end-to-end/AMES.html#using_the_quotarrowquot_syntax">Using the &quot;arrow&quot; syntax</a></h3>
<p>If you&#39;re using Julia 1.3, you can use the following syntax to do the same thing.</p>
<p><em>First layer</em>: one hot encoding and log transform:</p>
<pre><code class="julia hljs">W = Xs |&gt; OneHotEncoder()
z = ys |&gt; log;</code></pre>
<p><em>Second layer</em>: KNN Regression and Ridge regression</p>
<pre><code class="julia hljs">ẑ₁ = (W, z) |&gt; KNNRegressor(K=<span class=hljs-number >5</span>)
ẑ₂ = (W, z) |&gt; RidgeRegressor(lambda=<span class=hljs-number >2.5</span>);</code></pre>
<p><em>Third layer</em>: weighted sum of the two models:</p>
<pre><code class="julia hljs">ẑ = <span class=hljs-number >0.3</span>ẑ₁ + <span class=hljs-number >0.7</span>ẑ₂;</code></pre>
<p>then the inverse transform</p>
<pre><code class="julia hljs">ŷ = exp(ẑ);</code></pre>
<p>You can then fit and evaluate the model as usual:</p>
<pre><code class="julia hljs">fit!(ŷ, rows=train)
rmsl(y[test], ŷ(rows=test))</code></pre><div class=code_output ><pre><code class="plaintext hljs">There was an error running the code:
BoundsError([0.8824201966716334, 0.68929788928311, 0.1512831591929075, 1.3423900203590722, 0.16757765382339337, 0.26042025727256435, 0.25432570415676203, 0.8788061137833973, 0.14542136901906572, 0.09611136004739343, 0.7925766142287616, 0.4729944373676558, 0.3946364265791899, 0.1741652106845784, 0.7023921755998833, 0.3965740887075485, 0.24038200525209405, 0.67985972922416, 0.8339518895663548, 0.18535936274504805, 1.0498043623153044, 1.084715014389438, 0.8986568756813592, 0.3045737282410472, 0.24434333529200208, 0.15923811609195698, 0.2533332021436981, 0.46355936641949164, 0.3190814340068368, 0.36753162631576086, 0.41901666858852055, 0.21632126470863855, 0.27652760276772465, 0.3655033964862564, 0.4474756155630158, 0.22600384961264705, 0.15804102383390173, 0.22281112392626068, 0.21638384097337018, 1.294190348229172, 0.8232146439330219, 0.4293786340775122, 1.9045859653820487, 0.9520976554103757, 0.8935985257317205, 0.613504454306241, 0.544876435175521, 0.12231824281552442, 0.19117412355861493, 0.6920131176352526, 0.25339742169029683, 0.5373907346407155, 0.43829363964830675, 0.47041924443686767, 0.768028669868627, 0.8428750055880934, 0.4068229585003875, 0.505511300514387, 0.10475598164480679, 0.15182801044634764, 0.988640162265772, 1.324488473038386, 1.2223897714964775, 0.19450029414129455, 0.23625956450719385, 0.5275911602823729, 0.3404739710701936, 0.24669809193257483, 1.1311497722254635, 0.31163874612051884, 0.4221777077414989, 0.6952766900829044, 0.18722190288115498, 0.3722414412785445, 0.49733314720882843, 1.0175222314035222, 0.5103979159013786, 0.27918647112932854, 0.3619601257584575, 0.47307682469809603, 1.7278172402737, 0.2849482688289057, 0.13022701837933343, 0.43961011141046247, 1.5294454875332666, 0.40533481146744177, 0.7405417966451596, 0.24955857109390914, 0.7167796590847649, 0.5348807754096259, 0.29646201389820664, 0.22047726203703105, 0.6543587320667872, 0.2451964912800568, 0.8399787280075045, 0.43356161508535995, 0.46439749518346085, 0.5016857635672913, 0.18201736110420808, 0.7446829212374304, 0.7587490963160295, 0.1592520788945278, 0.7624692459630795, 0.2808107599592921, 0.5118702884640618, 0.4040889181565197, 0.10084003022455365, 0.4117484262608757, 0.23966047328520262, 0.08034233185889436, 0.6208031397560285, 0.8903688904816113, 0.6787289098406235, 0.7106486207296403, 0.5890051707416946, 0.12308765777660587, 1.50980102659133, 0.2996478799508778, 0.4248158134949732, 0.7419837791955565, 0.42208051464858876, 0.3504915123889207, 0.742779654051853, 1.6930176951169842, 0.2118639027225586, 0.8357115916880172, 0.4395585701527553, 0.18079517335559347, 0.26910061945502756, 0.26453757125929733, 0.17725745776945348, 0.2270229134062552, 0.8830783851172905, 0.647263761015731, 0.11998483658293492, 0.7881410862915491, 0.27984734491564534, 0.8858987038329044, 1.2675986717289036, 0.4834869647078448, 1.6000559596909665, 0.3100419281555279, 0.5107284589753598, 0.5214447196865878, 1.0861289557326201, 0.6317786290440153, 0.33871023217395724, 0.4452852330766957, 0.49769897598866886, 0.9264217016747218, 0.22461849689599472, 0.5916541008348366, 0.3881841242106094, 0.2023404936763391, 1.0761267391627594, 0.10033384841740917, 1.5079968645898694, 0.2589939507099133, 0.6822469667474843, 0.10346350459793882, 0.34599764871860805, 0.8395963882548432, 0.48238333502276476, 0.36275119233088604, 0.8032271232820847, 0.7590450814598337, 0.8874210216168881, 0.3425661578353543, 0.5795051061164107, 0.6770428335512602, 0.1847317571656531, 0.2077258911454392, 0.7529427946769002, 0.4501487524065644, 0.21232498320609183, 0.7951221894466809, 0.19151033184878496, 0.9457347789721045, 0.13695289523087997, 0.35506891245132816, 0.29465509671467105, 0.20821668215012615, 0.497496314412091, 0.2179873339019616, 0.37957379748404846, 0.12865561480590412, 0.22794015237998558, 0.4916265130299221, 0.6517002553668381, 0.7920931794052433, 0.24859798845721778, 0.3317317794554786, 0.5146495886137887, 0.25492941800399727, 0.13439754996492354, 0.47481627797321907, 1.090182031070921, 0.41773091689429415, 0.5879561605968213, 0.09067868772186022, 0.4178191452631903, 0.42192262970609484, 0.2959324434189643, 0.20488179345341206, 0.31331544450585713, 0.2655259927626632, 1.3986986495772982, 0.3812028836449365, 0.2011504019626275, 0.8959017338429036, 0.17352865223052963, 0.33076990922188, 0.10479905631890102, 0.178188838633881, 0.7596313856127547, 0.12451409951212415, 0.635178538404609, 0.2033077353559966, 0.4748779013327535, 0.16454762892057825, 1.1764707940176022, 0.15268158206328208, 0.5454513719683087, 0.1599229041143835, 0.3680575274525032, 0.35173443254167697, 0.7042529189244076, 0.18045399556206848, 0.19852605998471584, 1.0177825937433451, 0.30342432107209844, 0.62590467741741, 0.1914147635478226, 0.5091901298565314, 1.7906490259740349, 0.4390693488531654, 0.26311783764341307, 1.1328468584371516, 0.6441660308613508, 0.3349484609106515, 0.28605272698804846, 0.5916733239284493, 2.1691032880636776, 0.12677615684874236, 0.35656685011818123, 0.654515447277089, 0.29802069722142255, 0.22273995346893127, 0.37019588357751787, 0.690438703135472, 0.8157017124829535, 0.17822447686026976, 0.3312254018793879, 0.2625869529733951, 0.3529633814854357, 0.39117936343280163, 0.33234694611257165, 0.22624650989429135, 0.3119985267667304, 0.13015354935735607, 0.7428854897340736, 1.2217218739199491, 0.3273118050189631, 0.5855668286250185, 0.2949512932012813, 0.5918162798314907, 0.1125109679094201, 0.7303058521828234, 0.6582985468062281, 0.7342164147534327, 0.13240369901418844, 0.4792364158888682, 1.515427630391452, 0.2578704993076123, 0.8684106712723041, 0.1452256842320393, 0.5208582383021536, 0.5047008503141255, 0.46814722932899705, 0.42947027073997035, 0.14922889991468216, 0.16997165239651957, 0.3636560825533641, 1.0123209610690345, 0.14056405828066415, 0.23313807546068108, 0.3750944382676325, 0.19994954416763788, 0.3822669679394559, 0.29746534409764425, 0.31245726470534946, 0.41476554978916486, 1.0268959643798958, 0.8746547998745581, 0.16356444029702139, 0.5443821318736206, 0.47717142292773, 0.17819603305882203, 0.0958872965530087, 0.6882877254615462], ([1297, 721, 1179, 264, 497, 1221, 838, 1294, 733, 488, 777, 959, 267, 203, 615, 1071, 71, 1231, 707, 1188, 372, 36, 141, 33, 121, 534, 618, 1440, 313, 843, 527, 502, 1122, 831, 974, 1041, 651, 893, 697, 517, 1109, 947, 595, 224, 563, 684, 907, 865, 842, 659, 1031, 924, 1003, 762, 1171, 942, 118, 906, 1396, 652, 914, 245, 576, 443, 282, 1142, 356, 132, 494, 719, 756, 620, 1015, 1134, 900, 582, 1002, 556, 1418, 738, 278, 922, 1291, 484, 193, 265, 250, 1302, 713, 133, 1166, 44, 1055, 1205, 573, 281, 1259, 1243, 80, 647, 896, 931, 1144, 42, 510, 990, 151, 916, 331, 940, 473, 1335, 237, 242, 1023, 233, 1113, 1423, 889, 966, 1114, 665, 817, 298, 259, 712, 1116, 63, 378, 148, 391, 230, 91, 531, 693, 359, 1364, 696, 1177, 50, 880, 147, 694, 640, 675, 1379, 1052, 1094, 375, 720, 1169, 125, 1282, 18, 128, 130, 994, 998, 143, 170, 351, 27, 1381, 180, 586, 525, 418, 565, 274, 11, 1357, 486, 1320, 735, 787, 244, 1254, 479, 503, 34, 1195, 530, 112, 26, 521, 6, 60, 149, 49, 324, 480, 708, 594, 946, 1009, 1248, 671, 110, 548, 782, 457, 403, 432, 1163, 1267, 1018, 660, 1421, 1017, 382, 1154, 544, 78, 229, 330, 804, 934, 901, 86, 77, 1087, 1384, 1387, 339, 622, 631, 254, 646, 28, 178, 345, 1447, 1420, 755, 159, 398, 863, 500, 355, 346, 423, 795, 7, 908, 943, 1235, 460, 859, 235, 475, 308, 1312, 961, 19, 167, 412, 803, 1308, 452, 852, 909, 780, 158, 22, 1363, 249, 283, 1180, 234, 275, 124, 62, 511, 699, 1182, 221, 288, 326, 965, 1187, 214, 1369, 134, 501, 1097, 634, 1304, 729, 1019, 637, 825, 1194, 788, 971, 246, 606, 1281, 841, 140, 218, 1213, 145, 644, 641, 1075, 836, 752, 363, 1197, 997, 656, 37, 1361, 455, 1377, 945, 587, 1293, 792, 839, 352, 75, 1157, 815, 39, 1220, 540, 970, 766, 1455, 92, 253, 1039, 1165, 683, 14, 748, 832, 115, 165, 296, 505, 400, 962, 154, 583, 912, 1199, 887, 185, 1362, 1146, 401, 600, 558, 142, 920, 741, 763, 523, 439, 334, 150, 585, 890, 821, 870, 1034, 818, 1295, 894, 367, 1203, 89, 104, 371, 1208, 746, 1082, 1033, 1095, 82, 1124, 695, 506, 636, 113, 881, 47, 1353, 390, 884, 704, 877, 1299, 175, 532, 903, 72, 1209, 581, 1140, 241, 1183, 978, 1125, 122, 1325, 1339, 742, 182, 952, 807, 407, 68, 1274, 666, 765, 434, 295, 357, 873, 676, 374, 718, 1241, 1319, 215, 437, 1108, 20, 951, 1127, 127, 1286, 10, 1223, 854, 1410, 1053, 543, 772, 3, 208, 674, 592, 187, 1406, 388, 1101, 813, 261, 597, 285, 217, 958, 1394, 725, 465, 1025, 1026, 200, 1012, 1086, 667, 661, 1202, 850, 1233, 1347, 513, 864, 469, 1409, 984, 524, 348, 1303, 1439, 1441, 905, 66, 744, 192, 205, 983, 904, 949, 321, 1141, 384, 638, 1193, 329, 672, 43, 157, 1365, 1120, 727, 632, 702, 621, 1344, 377, 231, 1307, 944, 1178, 248, 415, 1456, 438, 617, 584, 1279, 84, 301, 1288, 54, 1284, 1327, 856, 1305, 1272, 991, 392, 1332, 266, 85, 557, 956, 1445, 1435, 999, 989, 449, 1155, 126, 1043, 402, 81, 601, 613, 1300, 99, 753, 515, 679, 1264, 522, 271, 1317, 1204, 1102, 1091, 1159, 76, 292, 347, 358, 212, 860, 761, 1443, 1411, 1349, 161, 1192, 194, 219, 1356, 1270, 450, 169, 1354, 1261, 309, 174, 797, 928, 939, 911, 430, 1008, 272, 1398, 546, 768, 1176, 1185, 1454, 591, 393, 1150, 304, 533, 650, 186, 953, 589, 258, 1242, 117, 472, 1271, 1334, 1277, 627, 1313, 847, 824, 969, 431, 833, 1345, 603, 1448, 745, 993, 690, 1057, 1032, 1268, 1422, 98, 1323, 848, 1285, 898, 1331, 982, 406, 410, 891, 790, 1343, 24, 1397, 1367, 1200, 1278, 294, 1265, 462, 414, 1326, 477, 1428, 1136, 976, 1181, 599, 791, 629, 869, 1139, 114, 751, 774, 624, 785, 717, 1078, 163, 102, 837, 739, 1240, 929, 875, 1239, 806, 1219, 1133, 162, 56, 387, 577, 226, 74, 1380, 299, 476, 111, 1405, 979, 1050, 106, 88, 568, 987, 1174, 1442, 1329, 353, 119, 687, 826, 228, 858, 360, 607, 1338, 293, 625, 1123, 1092, 1047, 692, 1292, 550, 79, 417, 1065, 1416, 1370, 349, 251, 155, 323, 988, 173, 1045, 1400, 487, 861, 1238, 1436, 1378, 571, 1314, 575, 1059, 1404, 1246, 1010, 1372, 1424, 1164, 868, 950, 783, 495, 396, 662, 1162, 771, 204, 714, 1016, 268, 610, 1066, 1316, 1260, 197, 732, 608, 853, 1391, 381, 995, 184, 120, 701, 658, 1044, 1037, 277, 1322, 364, 1427, 1189, 776, 519, 1161, 440, 394, 336, 236, 370, 365, 1315, 649, 350, 897, 1206, 628, 338, 810, 1168, 948, 1309, 225, 682, 554, 1186, 1257, 1417, 485, 918, 64, 516, 923, 619, 1022, 773, 58, 463, 1224, 698, 433, 981, 526, 518, 1426, 731, 343, 1079, 216, 1040, 1342, 376, 136, 448, 198, 373, 1063, 799, 760, 1368, 883, 1216, 968, 1430, 493, 1096, 1280, 1310, 107, 1253, 677, 48, 1112, 70, 213, 542, 317, 379, 767, 866, 689, 206, 447, 1215, 1230, 737, 1160, 172, 680, 195, 1211, 1232, 300, 673, 41, 131, 492, 257, 1027, 784, 688, 1062, 210, 137, 320, 1004, 1099, 1414, 17, 823, 1269, 967, 1449, 567, 1001, 52, 754, 874, 1251, 25, 325, 436, 1392, 123, 1236, 307, 1105, 1190, 97, 711, 604, 51, 252, 902, 1145, 315, 1119, 144, 1042, 885, 555, 871, 291, 1046, 1088, 4, 90, 917, 1006, 749, 1263, 937, 1212, 1438, 1173, 474, 750, 1184, 135, 812, 1403, 1222, 508, 354, 31, 520, 1287, 116, 1084, 1333, 1256, 361, 1118, 723, 303, 975, 399, 1128, 536, 188, 483, 1311, 404, 964, 1030, 23, 421, 156, 559, 913, 709, 1401, 562, 1336, 1111, 1444, 316, 648, 706, 1056, 1390, 260, 504, 29, 425, 380, 1385, 775, 239, 537, 466, 1276, 1013, 482, 1324, 862, 1048, 954, 757, 973, 38, 413, 986, 419, 1412, 1374, 1115, 925, 1074, 758, 846, 1340, 424, 152, 422, 1214, 1098, 645, 996, 1090, 1069, 1083, 886, 94, 882, 327, 103],))
</code></pre></div>
<h3 id=tuning_the_model ><a href="/MLJTutorials/pub/end-to-end/AMES.html#tuning_the_model">Tuning the model</a></h3>
<p>So far the hyperparameters were explicitly given but it makes more sense to learn them. For this, we define a model around the learning network which can then be trained and tuned as any model:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >mutable struct</span> KNNRidgeBlend &lt;: DeterministicNetwork
    knn_model::KNNRegressor
    ridge_model::RidgeRegressor
    knn_weight::<span class=hljs-built_in >Float64</span>
<span class=hljs-keyword >end</span></code></pre>
<p>We must specify how such a model should be fit, which is effectively just the learning network we had defined before except that now the parameters are contained in the struct:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> MLJ.fit(model::KNNRidgeBlend, verbosity::<span class=hljs-built_in >Int</span>, X, y)
    Xs, ys = source.((X, y))
    hot = machine(OneHotEncoder(), Xs)
    W = transform(hot, Xs)
    z = log(ys)
    ridge_model = model.ridge_model
    knn_model = model.knn_model
    ridge = machine(ridge_model, W, z)
    knn = machine(knn_model, W, z)
    <span class=hljs-comment ># and finally</span>
    ẑ = model.knn_weight * predict(knn, W) + (<span class=hljs-number >1.0</span> - model.knn_weight) * predict(ridge, W)
    ŷ = exp(ẑ)
    fit!(ŷ, verbosity=<span class=hljs-number >0</span>)
    <span class=hljs-keyword >return</span> fitresults(ŷ)
<span class=hljs-keyword >end</span></code></pre>
<p><strong>Note</strong>: you really  want to set <code>verbosity&#61;0</code> here otherwise in the tuning you will get a lot of verbose output&#33;</p>
<p>You can now instantiate and fit such a model:</p>
<pre><code class="julia hljs">krb = KNNRidgeBlend(KNNRegressor(K=<span class=hljs-number >5</span>), RidgeRegressor(lambda=<span class=hljs-number >2.5</span>), <span class=hljs-number >0.3</span>)
mach = machine(krb, X, y)
fit!(mach, rows=train)

preds = predict(mach, rows=test)
rmsl(y[test], preds)</code></pre><div class=code_output ><pre><code class="plaintext hljs">There was an error running the code:
ErrorException("Improperly exported supervised network does not have a unique input source. ")
</code></pre></div>
<p>But more interestingly, the hyperparameters of the model can be tuned.</p>
<p>Before we get started, it&#39;s important to note that the hyperparameters of the model have different levels of <em>nesting</em>. This becomes explicit when trying to access elements:</p>
<pre><code class="julia hljs"><span class=hljs-meta >@show</span> krb.knn_weight
<span class=hljs-meta >@show</span> krb.knn_model.K
<span class=hljs-meta >@show</span> krb.ridge_model.lambda</code></pre><div class=code_output ><pre><code class="plaintext hljs">krb.knn_weight = 0.3
krb.knn_model.K = 5
krb.ridge_model.lambda = 2.5
</code></pre></div>
<p>You can also see all the hyperparameters using the <code>params</code> function:</p>
<pre><code class="julia hljs">params(krb) |&gt; pprint</code></pre><div class=code_output ><pre><code class="plaintext hljs">(knn_model = (K = 5,
              algorithm = :kdtree,
              metric = Distances.Euclidean(0.0),
              leafsize = 10,
              reorder = true,
              weights = :uniform),
 ridge_model = (lambda = 2.5,),
 knn_weight = 0.3)
</code></pre></div>
<p>The range of values to do your hyperparameter tuning over should follow the nesting structure reflected by <code>params</code>:</p>
<pre><code class="julia hljs">k_range = range(krb, :(knn_model.K), lower=<span class=hljs-number >2</span>, upper=<span class=hljs-number >100</span>, scale=:log10)
l_range = range(krb, :(ridge_model.lambda), lower=<span class=hljs-number >1e-4</span>, upper=<span class=hljs-number >10</span>, scale=:log10)
w_range = range(krb, :(knn_weight), lower=<span class=hljs-number >0.1</span>, upper=<span class=hljs-number >0.9</span>)

ranges = [k_range, l_range, w_range]</code></pre><div class=code_output ><pre><code class="plaintext hljs">3-element Array{MLJ.NumericRange{T,Symbol} where T,1}:
 NumericRange @ 1…43
 NumericRange @ 1…94
 NumericRange @ 2…43</code></pre></div>
<p>Now there remains to define how the tuning should be done, let&#39;s just specify a very coarse grid tuning with cross validation and instantiate a tuned model:</p>
<pre><code class="julia hljs">tuning = Grid(resolution=<span class=hljs-number >3</span>)
resampling = CV(nfolds=<span class=hljs-number >6</span>)

tm = TunedModel(model=krb, tuning=tuning, resampling=resampling,
                ranges=ranges, measure=rmsl)</code></pre><div class=code_output ><pre><code class="plaintext hljs">MLJ.DeterministicTunedModel(model = JuDoc.KNNRidgeBlend(knn_model = KNNRegressor @ 3…35,
                                                        ridge_model = RidgeRegressor @ 8…37,
                                                        knn_weight = 0.3,),
                            tuning = Grid(resolution = 3,
                                          parallel = true,),
                            resampling = CV(nfolds = 6,
                                            shuffle = false,
                                            rng = Random._GLOBAL_RNG(),),
                            measure = MLJBase.RMSL(),
                            weights = nothing,
                            operation = StatsBase.predict,
                            ranges = MLJ.NumericRange{T,Symbol} where T[NumericRange @ 1…43, NumericRange @ 1…94, NumericRange @ 2…43],
                            full_report = true,
                            train_best = true,) @ 4…71</code></pre></div>
<p>which we can now finally fit...</p>
<pre><code class="julia hljs">mtm = machine(tm, X, y)
fit!(mtm, rows=train);</code></pre><div class=code_output ><pre><code class="plaintext hljs">There was an error running the code:
ErrorException("Improperly exported supervised network does not have a unique input source. ")
</code></pre></div>
<p>To retrieve the best model, you can use:</p>
<pre><code class="julia hljs">krb_best = fitted_params(mtm).best_model
<span class=hljs-meta >@show</span> krb_best.knn_model.K
<span class=hljs-meta >@show</span> krb_best.ridge_model.lambda
<span class=hljs-meta >@show</span> krb_best.knn_weight</code></pre><div class=code_output ><pre><code class="plaintext hljs">There was an error running the code:
ErrorException("Machine{DeterministicTunedModel} @ 1…61 has not been trained.")
</code></pre></div>
<p>you can also use <code>mtm</code> to make predictions &#40;which will be done using the best model&#41;</p>
<pre><code class="julia hljs">preds = predict(mtm, rows=test)
rmsl(y[test], preds)</code></pre><div class=code_output ><pre><code class="plaintext hljs">There was an error running the code:
ErrorException("Machine{DeterministicTunedModel} @ 1…61 has not been trained.")
</code></pre></div>
<div class=page-foot >
  <div class=copyright >
    &copy; Anthony Blaom, Thibaut Lienart and collaborators. Last modified: October 24, 2019. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>.
  </div>
</div>

</div>

      </div> 
  </div> 
  <script src="/MLJTutorials/libs/pure/ui.min.js"></script>